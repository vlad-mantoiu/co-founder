---
phase: 41-autonomous-runner-core-taor-loop
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - backend/app/agent/loop/system_prompt.py
  - backend/tests/agent/test_system_prompt.py
autonomous: true
requirements: [AGNT-02]

must_haves:
  truths:
    - "System prompt contains the founder's Idea Brief content verbatim — not summarized or paraphrased"
    - "System prompt contains the Understanding Interview QnA verbatim — every question and answer appears"
    - "System prompt includes the co-founder persona identity with we/us collaborative voice"
    - "System prompt includes the build plan for the agent to execute"
    - "System prompt includes critical guardrails — no data deletion, no external prod API calls"
  artifacts:
    - path: "backend/app/agent/loop/system_prompt.py"
      provides: "build_system_prompt() function assembling persona + idea brief + QnA + build plan"
      exports: ["build_system_prompt"]
    - path: "backend/tests/agent/test_system_prompt.py"
      provides: "Unit tests for system prompt assembly with verbatim injection verification"
      min_lines: 50
  key_links:
    - from: "backend/app/agent/loop/system_prompt.py"
      to: "backend/app/agent/runner_autonomous.py"
      via: "build_system_prompt() called at loop start to assemble system param (Plan 03)"
      pattern: "build_system_prompt"
---

<objective>
TDD the system prompt builder that assembles the agent's identity, founder context (Idea Brief + Understanding QnA), and build plan into a single system prompt string.

Purpose: The agent's behavior is entirely shaped by its system prompt. Verbatim injection of founder context (per AGNT-02) ensures the agent makes decisions referencing the founder's actual words, not generic defaults. This is a pure function with defined I/O — ideal for TDD.

Output: Tested `build_system_prompt()` function that the TAOR loop calls once at session start.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/41-autonomous-runner-core-taor-loop/41-CONTEXT.md
@.planning/phases/41-autonomous-runner-core-taor-loop/41-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED — Write failing tests for build_system_prompt()</name>
  <files>backend/tests/agent/test_system_prompt.py</files>
  <action>
Create test file importing `build_system_prompt` from `app.agent.loop.system_prompt`.

Test cases:

1. `test_idea_brief_in_prompt` — Pass `idea_brief={"problem": "Founders waste time on boilerplate", "target_user": "Non-technical founders"}`. Assert both "Founders waste time on boilerplate" and "Non-technical founders" appear verbatim in the returned string.

2. `test_qna_in_prompt` — Pass `understanding_qna=[{"question": "What is your revenue model?", "answer": "SaaS subscriptions"}, {"question": "Who is your competition?", "answer": "Bubble, Webflow"}]`. Assert all 4 strings (both questions and both answers) appear verbatim in the returned string.

3. `test_empty_qna_no_crash` — Pass `understanding_qna=[]`. Assert the function returns a string without raising.

4. `test_build_plan_in_prompt` — Pass `build_plan={"phases": [{"name": "auth"}, {"name": "dashboard"}]}`. Assert "auth" and "dashboard" appear in the returned string.

5. `test_persona_identity_present` — Call with minimal args. Assert the returned string contains "co-founder" (case-insensitive) — confirming the persona section is included.

6. `test_guardrails_present` — Call with minimal args. Assert the returned string contains "Do not delete" or "do not delete" (case-insensitive) — confirming critical guardrails are present.

7. `test_collaborative_voice` — Call with minimal args. Assert the returned string contains "we" or "us" — confirming collaborative voice instruction.

8. `test_narration_instructions_present` — Call with minimal args. Assert the returned string contains "Narrate" — confirming narration behavior instructions are in the prompt.

Function signature: `build_system_prompt(idea_brief: dict, understanding_qna: list[dict], build_plan: dict) -> str`

Run tests — they must FAIL (ImportError).
  </action>
  <verify>
    <automated>cd /Users/vladcortex/co-founder/backend && .venv/bin/pytest tests/agent/test_system_prompt.py -x -q 2>&1 | tail -5</automated>
    <manual>Tests fail with ImportError (module doesn't exist yet)</manual>
  </verify>
  <done>8 test cases written, all fail with ImportError because app.agent.loop.system_prompt does not exist yet</done>
</task>

<task type="auto">
  <name>Task 2: GREEN — Implement build_system_prompt()</name>
  <files>backend/app/agent/loop/system_prompt.py</files>
  <action>
Create `backend/app/agent/loop/system_prompt.py` implementing the prompt builder per research Pattern 4 and CONTEXT.md locked decisions.

```python
def build_system_prompt(
    idea_brief: dict,
    understanding_qna: list[dict],
    build_plan: dict,
) -> str:
```

**Persona section** (per CONTEXT.md locked decisions):
- Co-founder identity: "You are the founder's AI co-founder — a senior technical partner building their product together."
- Voice: Use "we/us" for shared decisions, "I" for internal reasoning
- Narration instructions: Narrate before every tool call ("I'm creating...") and after ("Auth module created.")
- Light markdown: **Phase Name** for phases, `file/path.py` for files
- Error tone: "Hit an issue with X. Trying a different approach..." — never panicked
- Critical guardrails: Do not delete data. Do not make external API calls to production services.
- Execute the provided build plan in sequence. Do not deviate from the plan order.

**Idea Brief section**: `## Founder's Idea Brief` followed by `json.dumps(idea_brief, indent=2)` — full verbatim injection per CONTEXT.md locked decision.

**Understanding QnA section**: `## Understanding Interview (Founder's Answers)` — iterate over qna list, format each as `Q: {question}\nA: {answer}\n\n`. If list is empty, include the header with "(No interview responses provided)".

**Build Plan section**: `## Build Plan (Execute in Order)` followed by `json.dumps(build_plan, indent=2)`.

Join all sections with `\n\n`.

Run all tests — they must PASS.
  </action>
  <verify>
    <automated>cd /Users/vladcortex/co-founder/backend && .venv/bin/pytest tests/agent/test_system_prompt.py -x -q</automated>
    <manual>All 8 tests pass, no import errors</manual>
  </verify>
  <done>build_system_prompt() passes all 8 tests — idea brief injected verbatim, QnA injected verbatim, persona/guardrails/voice present</done>
</task>

</tasks>

<verification>
- `cd backend && .venv/bin/pytest tests/agent/test_system_prompt.py -v` — all 8 pass
- `cd backend && .venv/bin/pytest tests/ -x -q --ignore=tests/e2e` — full suite green (no regressions)
- `python -c "from app.agent.loop.system_prompt import build_system_prompt; print('OK')"` — importable
</verification>

<success_criteria>
- build_system_prompt() includes idea_brief content verbatim in output string
- build_system_prompt() includes all QnA questions and answers verbatim in output string
- build_system_prompt() includes co-founder persona, collaborative voice, narration instructions, and guardrails
- build_system_prompt() includes build plan content in output string
- Empty QnA list handled gracefully without crash
- Full test suite green (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/41-autonomous-runner-core-taor-loop/41-02-SUMMARY.md`
</output>
