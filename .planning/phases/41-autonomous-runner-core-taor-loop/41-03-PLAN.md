---
phase: 41-autonomous-runner-core-taor-loop
plan: 03
type: execute
wave: 2
depends_on: ["41-01", "41-02"]
files_modified:
  - backend/app/agent/runner_autonomous.py
  - backend/tests/agent/test_taor_loop.py
autonomous: true
requirements: [AGNT-01, AGNT-02, AGNT-06]

must_haves:
  truths:
    - "A job routed through AUTONOMOUS_AGENT=true completes the TAOR loop end-to-end — agent reasons, calls tools, observes results, reaches end_turn"
    - "The agent system prompt includes the founder's Idea Brief and Understanding Interview QnA verbatim"
    - "With MAX_TOOL_CALLS=5, exceeding the cap terminates with a structured iteration-limit-reached escalation"
    - "Repeating the same tool call 3 times in 10-call window triggers repetition detection and loop halt"
    - "Tool results exceeding 1000 words are middle-truncated before appending to message history"
    - "Narration text from the agent's text stream is written to LogStreamer on sentence boundaries"
  artifacts:
    - path: "backend/app/agent/runner_autonomous.py"
      provides: "AutonomousRunner with working run_agent_loop() TAOR implementation"
      exports: ["AutonomousRunner"]
      min_lines: 100
    - path: "backend/tests/agent/test_taor_loop.py"
      provides: "Unit tests for TAOR loop with mocked Anthropic stream"
      min_lines: 100
  key_links:
    - from: "backend/app/agent/runner_autonomous.py"
      to: "app.agent.loop.safety"
      via: "IterationGuard injected and called per tool dispatch"
      pattern: "guard\\.check_iteration_cap|guard\\.check_repetition|guard\\.truncate"
    - from: "backend/app/agent/runner_autonomous.py"
      to: "app.agent.tools.dispatcher"
      via: "ToolDispatcher.dispatch() called for each tool_use block"
      pattern: "dispatcher\\.dispatch"
    - from: "backend/app/agent/runner_autonomous.py"
      to: "app.agent.loop.system_prompt"
      via: "build_system_prompt() called once at loop start"
      pattern: "build_system_prompt"
    - from: "backend/app/agent/runner_autonomous.py"
      to: "app.services.log_streamer"
      via: "LogStreamer.write_event() for narration streaming"
      pattern: "streamer\\.write_event"
    - from: "backend/app/agent/runner_autonomous.py"
      to: "anthropic.AsyncAnthropic"
      via: "client.messages.stream() for streaming tool-use API"
      pattern: "client\\.messages\\.stream"
---

<objective>
Implement the core TAOR (Think-Act-Observe-Repeat) loop in AutonomousRunner.run_agent_loop(), wiring together all foundational modules (IterationGuard, ToolDispatcher, system_prompt builder, LogStreamer) with the Anthropic streaming tool-use API.

Purpose: This is the heart of the autonomous agent — the loop that replaces the old LangGraph pipeline. After this plan, a job routed through `AUTONOMOUS_AGENT=true` will execute a complete autonomous build cycle (with stubbed tools).

Output: Working AutonomousRunner.run_agent_loop() with streaming narration, safety guards, and comprehensive tests using mocked Anthropic responses.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/41-autonomous-runner-core-taor-loop/41-RESEARCH.md
@.planning/phases/41-autonomous-runner-core-taor-loop/41-01-SUMMARY.md
@.planning/phases/41-autonomous-runner-core-taor-loop/41-02-SUMMARY.md
@backend/app/agent/runner_autonomous.py
@backend/app/agent/runner.py
@backend/app/services/log_streamer.py
@backend/app/core/llm_config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement AutonomousRunner.run_agent_loop() with TAOR loop</name>
  <files>backend/app/agent/runner_autonomous.py</files>
  <action>
Rewrite `runner_autonomous.py` to implement the full TAOR loop in `run_agent_loop()`. Keep all other 13 methods as NotImplementedError stubs (they are for the pre-existing onboarding/brief/artifact pipeline and remain unused by the autonomous agent path).

**Constructor `__init__`:**
- Accept `model: str` parameter (default `"claude-sonnet-4-20250514"`) — Phase 43 will set this from tier config
- Accept `api_key: str | None = None` parameter — falls back to `Settings.anthropic_api_key`
- Create `anthropic.AsyncAnthropic(api_key=api_key or settings.anthropic_api_key)` as `self._client`
- Store `self._model = model`

**`run_agent_loop(self, context: dict) -> dict`:**

Expected `context` keys: `project_id`, `user_id`, `job_id`, `idea_brief` (dict), `understanding_qna` (list[dict]), `build_plan` (dict), `redis` (Redis connection), `max_tool_calls` (optional int, default 150).

Implementation per research Pattern 1:

1. **Setup:**
   - Import and call `build_system_prompt(context["idea_brief"], context["understanding_qna"], context["build_plan"])` to get `system` string
   - Create `IterationGuard(max_tool_calls=context.get("max_tool_calls", 150))`
   - Create `InMemoryToolDispatcher()` as the default dispatcher (Phase 42 will swap to E2B)
   - If `context.get("dispatcher")` is provided, use that instead (for testing)
   - Create `LogStreamer(redis=context["redis"], job_id=context["job_id"], phase="agent")` if redis is provided; otherwise skip streaming (for unit tests without Redis)
   - Import `AGENT_TOOLS` from `app.agent.tools.definitions`
   - Initialize `messages = [{"role": "user", "content": "Begin building the project per the build plan."}]`

2. **TAOR Loop:**
   ```python
   while True:
       async with self._client.messages.stream(
           model=self._model,
           system=system,
           messages=messages,
           tools=AGENT_TOOLS,
           max_tokens=4096,
       ) as stream:
           # Stream narration text — accumulate and flush on sentence boundaries
           accumulated_text = ""
           async for chunk in stream.text_stream:
               accumulated_text += chunk
               if accumulated_text.rstrip().endswith((".", "!", "?", "\n")):
                   line = accumulated_text.strip()
                   if line and streamer:
                       await streamer.write_event(line, source="agent")
                   accumulated_text = ""

           # Flush remaining text
           if accumulated_text.strip() and streamer:
               await streamer.write_event(accumulated_text.strip(), source="agent")

           response = await stream.get_final_message()

       # Track usage for future budget daemon (Phase 43)
       _input_tokens = response.usage.input_tokens
       _output_tokens = response.usage.output_tokens

       # Check stop condition
       if response.stop_reason == "end_turn":
           # Extract final text from response
           final_text = ""
           for block in response.content:
               if hasattr(block, "text"):
                   final_text += block.text
           return {
               "status": "completed",
               "project_id": context.get("project_id"),
               "phases_completed": [],
               "result": final_text,
           }

       # Find tool_use blocks
       tool_use_blocks = [b for b in response.content if b.type == "tool_use"]
       if not tool_use_blocks:
           # No tool calls and not end_turn — treat as completion
           final_text = ""
           for block in response.content:
               if hasattr(block, "text"):
                   final_text += block.text
           return {
               "status": "completed",
               "project_id": context.get("project_id"),
               "phases_completed": [],
               "result": final_text,
           }

       # Append assistant turn to history
       messages.append({"role": "assistant", "content": response.content})

       # Dispatch each tool call
       tool_results = []
       for tool_block in tool_use_blocks:
           try:
               guard.check_iteration_cap()
               guard.check_repetition(tool_block.name, tool_block.input)
           except IterationCapError:
               # Narrate graceful stop
               stop_msg = f"I've reached my action limit of {guard._max} tool calls. Here's what I completed and what's remaining..."
               if streamer:
                   await streamer.write_event(stop_msg, source="agent")
               return {
                   "status": "iteration_limit_reached",
                   "project_id": context.get("project_id"),
                   "phases_completed": [],
                   "result": stop_msg,
               }
           except RepetitionError as e:
               # Narrate repetition stop
               stop_msg = f"Hit a repeated action pattern. {str(e)}. Stopping to avoid a loop."
               if streamer:
                   await streamer.write_event(stop_msg, source="agent")
               return {
                   "status": "repetition_detected",
                   "project_id": context.get("project_id"),
                   "phases_completed": [],
                   "result": stop_msg,
               }

           # Narrate before tool call (per CONTEXT.md: narrate before AND after)
           if streamer:
               await streamer.write_event(
                   f"Running `{tool_block.name}` ...", source="agent"
               )

           try:
               result_text = await dispatcher.dispatch(tool_block.name, tool_block.input)
           except Exception as exc:
               result_text = f"Error: {type(exc).__name__}: {str(exc)}"

           # Middle-truncate large results
           result_text = guard.truncate_tool_result(result_text)

           tool_results.append({
               "type": "tool_result",
               "tool_use_id": tool_block.id,
               "content": result_text,
           })

       # Append tool results as user turn
       messages.append({"role": "user", "content": tool_results})
   ```

3. **Error handling:** Wrap the entire loop in try/except for `anthropic.APIError` — log and return `{"status": "api_error", ...}`.

**CRITICAL anti-patterns to avoid (from RESEARCH.md):**
- Do NOT use `TrackedAnthropicClient` — it does not support streaming. Use raw `AsyncAnthropic`.
- Do NOT write every text delta token to Redis — accumulate on sentence boundaries.
- Always call `stream.get_final_message()` AFTER consuming `stream.text_stream`.
- Always append assistant turn (with tool_use blocks) BEFORE user turn (with tool_results). Order matters.
- Always use `tool_block.id` for `tool_use_id` in results.

Use `structlog.get_logger(__name__)` for structured logging with `job_id`, `iteration`, `tool_name` bindings per iteration.
  </action>
  <verify>
    <automated>cd /Users/vladcortex/co-founder/backend && .venv/bin/python -c "from app.agent.runner_autonomous import AutonomousRunner; print('importable')"</automated>
    <manual>AutonomousRunner imports cleanly, run_agent_loop method exists, isinstance check passes</manual>
  </verify>
  <done>AutonomousRunner.run_agent_loop() implemented with TAOR loop, streaming narration, safety guard integration, and proper Anthropic API message ordering</done>
</task>

<task type="auto">
  <name>Task 2: Write comprehensive TAOR loop tests with mocked Anthropic stream</name>
  <files>backend/tests/agent/test_taor_loop.py</files>
  <action>
Create comprehensive test file for the TAOR loop. Since we cannot make real Anthropic API calls in tests, mock the `client.messages.stream()` context manager to return controlled responses.

**Test helper — mock stream factory:**

Create a helper class `MockStreamManager` that simulates `AsyncMessageStreamManager`:
- `text_stream`: async generator yielding text chunks
- `get_final_message()`: returns a `Mock` with `.stop_reason`, `.content` (list of content blocks), `.usage` (with `.input_tokens`, `.output_tokens`)
- Content blocks: use `Mock` objects with `.type = "text"` and `.text = "..."` for text, or `.type = "tool_use"` with `.name`, `.input`, `.id` for tool calls

Create `make_mock_response(stop_reason, text_blocks, tool_use_blocks, input_tokens=100, output_tokens=50)` helper.

**Test cases:**

1. `test_loop_reaches_end_turn` — Mock returns one response with `stop_reason="end_turn"` and text "Build complete". Assert `run_agent_loop()` returns `{"status": "completed", ...}` with result containing "Build complete".

2. `test_loop_dispatches_tools` — Mock returns first response with `stop_reason="tool_use"` containing a `write_file` tool call, then second response with `stop_reason="end_turn"`. Assert the InMemoryToolDispatcher's `_fs` dict contains the written file after the loop completes. Assert status is "completed".

3. `test_loop_iteration_cap` — Set `max_tool_calls=2` in context. Mock returns tool_use responses indefinitely (at least 5 iterations). Assert `run_agent_loop()` returns `{"status": "iteration_limit_reached", ...}`.

4. `test_loop_repetition_detection` — Mock returns the same `bash` tool call with identical args 3 times. Assert `run_agent_loop()` returns `{"status": "repetition_detected", ...}`.

5. `test_loop_tool_result_truncation` — Mock returns one `read_file` tool call. Configure InMemoryToolDispatcher to have a file with 2000 words. After loop completes, inspect the messages list — the tool_result content should contain `[1000 words omitted]`.

6. `test_system_prompt_contains_idea_brief` — Pass `idea_brief={"problem": "Too many meetings"}` in context. Mock the `client.messages.stream` call and capture the `system` kwarg. Assert "Too many meetings" appears in the system arg.

7. `test_system_prompt_contains_qna` — Pass `understanding_qna=[{"question": "Revenue?", "answer": "SaaS"}]` in context. Capture `system` kwarg. Assert "Revenue?" and "SaaS" appear.

8. `test_narration_written_to_streamer` — Mock returns one text response with `stop_reason="end_turn"`. Provide a mock `LogStreamer` (or mock redis). Assert `write_event` was called with the narration text.

9. `test_tool_error_returns_error_string` — Configure InMemoryToolDispatcher with failure injection for "bash" call 0. Mock returns a bash tool call. Assert the loop does NOT crash — instead the error is captured as a tool_result string and the loop continues.

10. `test_runner_still_satisfies_protocol` — `isinstance(AutonomousRunner(), Runner)` returns True.

**Mocking strategy:**
- Patch `anthropic.AsyncAnthropic` on the `AutonomousRunner` instance (or pass a mock client)
- Use `unittest.mock.AsyncMock` for async context manager
- For `context["redis"]`, use `AsyncMock()` — LogStreamer wraps it but tests don't need real Redis
- Pass `dispatcher=InMemoryToolDispatcher(...)` in context to control tool behavior
- Pass `max_tool_calls=N` in context to control iteration cap

All tests are async (`pytest.mark.asyncio` or auto mode).
  </action>
  <verify>
    <automated>cd /Users/vladcortex/co-founder/backend && .venv/bin/pytest tests/agent/test_taor_loop.py -x -q</automated>
    <manual>All 10 TAOR loop tests pass with mocked Anthropic stream</manual>
  </verify>
  <done>10 TAOR loop tests pass — end_turn completion, tool dispatch, iteration cap, repetition detection, truncation, system prompt verification, narration streaming, error handling, and protocol compliance all verified</done>
</task>

</tasks>

<verification>
- `cd backend && .venv/bin/pytest tests/agent/test_taor_loop.py -v` — all 10 pass
- `cd backend && .venv/bin/pytest tests/agent/ -v` — all new + existing agent tests pass
- `cd backend && .venv/bin/pytest tests/ -x -q --ignore=tests/e2e` — full suite green (no regressions)
- `python -c "from app.agent.runner_autonomous import AutonomousRunner; from app.agent.runner import Runner; assert isinstance(AutonomousRunner(), Runner); print('Protocol OK')"` — protocol compliance
</verification>

<success_criteria>
- run_agent_loop() executes TAOR cycle: stream → check stop → dispatch tools → append results → repeat
- end_turn stop_reason returns status "completed" with final text
- MAX_TOOL_CALLS exceeded returns status "iteration_limit_reached"
- 3x identical tool call in 10-call window returns status "repetition_detected"
- Tool results >1000 words are truncated before appending to messages
- System prompt includes verbatim idea_brief and understanding_qna
- Narration text written to LogStreamer on sentence boundaries
- Tool dispatch errors captured as error string, loop continues
- AutonomousRunner still satisfies Runner protocol
- Full test suite green (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/41-autonomous-runner-core-taor-loop/41-03-SUMMARY.md`
</output>
