---
phase: 05-capacity-queue-worker-model
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - backend/app/queue/semaphore.py
  - backend/app/queue/estimator.py
  - backend/tests/domain/test_semaphore.py
  - backend/tests/domain/test_estimator.py
autonomous: true

must_haves:
  truths:
    - "Per-user concurrency limit enforced: Bootstrapper 2, Partner 3, CTO 10"
    - "Per-project concurrency limit enforced: Bootstrapper 2, Partner 3, CTO 5"
    - "Semaphore slots auto-release after TTL expires (prevents deadlock on crash)"
    - "Wait time estimate uses EMA with tier-aware defaults (Bootstrapper 480s, Partner 600s, CTO 900s)"
    - "Wait time estimate returns seconds, formatted string, and confidence interval"
  artifacts:
    - path: "backend/app/queue/semaphore.py"
      provides: "RedisSemaphore for distributed concurrency control"
      contains: "class RedisSemaphore"
    - path: "backend/app/queue/estimator.py"
      provides: "WaitTimeEstimator with EMA and tier-aware defaults"
      contains: "class WaitTimeEstimator"
  key_links:
    - from: "backend/app/queue/semaphore.py"
      to: "redis"
      via: "Redis SADD/SREM/SCARD for slot management, SETEX for TTL"
      pattern: "redis\\.sadd|redis\\.srem"
    - from: "backend/app/queue/estimator.py"
      to: "redis"
      via: "Redis GET/SET for EMA average storage"
      pattern: "redis\\.get|redis\\.set"
---

<objective>
Build the distributed concurrency semaphore and wait time estimator — two independent Redis-based primitives that enforce capacity limits and provide user-facing wait estimates.

Purpose: The semaphore prevents cost explosion by enforcing per-user and per-project concurrency limits. The estimator provides transparency to users about expected wait times, improving UX and reducing support burden.
Output: Working RedisSemaphore and WaitTimeEstimator with full TDD coverage.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-capacity-queue-worker-model/05-RESEARCH.md

Reference existing patterns:
@backend/app/db/redis.py (Redis client access pattern: get_redis())
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create distributed concurrency semaphore with TDD</name>
  <files>
    backend/app/queue/semaphore.py
    backend/tests/domain/test_semaphore.py
  </files>
  <action>
    **RED phase first — write failing tests:**

    Create `backend/tests/domain/test_semaphore.py` using fakeredis async:
    - Test acquire succeeds when under limit (max_concurrent=2, first acquire returns True)
    - Test acquire succeeds up to limit (2 acquires succeed for max=2)
    - Test acquire fails at limit (3rd acquire for max=2 returns False)
    - Test release frees slot (acquire 2, release 1, acquire 1 more succeeds)
    - Test count returns accurate slot count
    - Test TTL auto-release: acquire with TTL=1, wait 2s, verify slot freed (use asyncio.sleep in test)
    - Test idempotent release (releasing non-existent slot doesn't error)
    - Test separate keys for user vs project semaphores don't interfere

    **GREEN phase — implement:**

    Create `backend/app/queue/semaphore.py`:
    ```python
    class RedisSemaphore:
        """Distributed semaphore for concurrency control using Redis sets + TTL."""

        def __init__(self, redis: Redis, key: str, max_concurrent: int, ttl: int = 3600):
            self.redis = redis
            self.key = key
            self.max_concurrent = max_concurrent
            self.ttl = ttl  # Lease timeout (prevents deadlock on crash)

        async def acquire(self, job_id: str) -> bool:
            """Try to acquire a slot. Returns True if acquired, False if at limit."""
            slot_set_key = f"{self.key}:slots"
            current = await self.redis.scard(slot_set_key)

            if current >= self.max_concurrent:
                return False

            added = await self.redis.sadd(slot_set_key, job_id)
            if added:
                # Set individual TTL key for auto-release on crash
                await self.redis.setex(f"{self.key}:slot:{job_id}", self.ttl, "1")
                return True
            return False

        async def release(self, job_id: str) -> None:
            """Release a slot back to the semaphore."""
            slot_set_key = f"{self.key}:slots"
            await self.redis.srem(slot_set_key, job_id)
            await self.redis.delete(f"{self.key}:slot:{job_id}")

        async def heartbeat(self, job_id: str) -> None:
            """Extend TTL for long-running job (prevents premature release)."""
            await self.redis.expire(f"{self.key}:slot:{job_id}", self.ttl)

        async def count(self) -> int:
            """Return current number of acquired slots."""
            return await self.redis.scard(f"{self.key}:slots")

        async def cleanup_stale(self) -> int:
            """Remove stale slots where TTL key has expired.

            Returns: Number of slots cleaned up.
            """
            slot_set_key = f"{self.key}:slots"
            members = await self.redis.smembers(slot_set_key)
            cleaned = 0

            for job_id in members:
                ttl_key = f"{self.key}:slot:{job_id}"
                exists = await self.redis.exists(ttl_key)
                if not exists:
                    await self.redis.srem(slot_set_key, job_id)
                    cleaned += 1

            return cleaned
    ```

    Helper functions for creating per-user and per-project semaphores:
    ```python
    def user_semaphore(redis: Redis, user_id: str, tier: str) -> RedisSemaphore:
        from app.queue.schemas import TIER_CONCURRENT_USER
        max_concurrent = TIER_CONCURRENT_USER.get(tier, 2)
        return RedisSemaphore(redis, f"concurrency:user:{user_id}", max_concurrent)

    def project_semaphore(redis: Redis, project_id: str, tier: str) -> RedisSemaphore:
        from app.queue.schemas import TIER_CONCURRENT_PROJECT
        max_concurrent = TIER_CONCURRENT_PROJECT.get(tier, 2)
        return RedisSemaphore(redis, f"concurrency:project:{project_id}", max_concurrent)
    ```
  </action>
  <verify>
    Run: `cd /Users/vladcortex/co-founder && python -m pytest backend/tests/domain/test_semaphore.py -v`
    All 8+ tests pass.
  </verify>
  <done>
    RedisSemaphore acquires/releases slots atomically. Per-user and per-project limits enforced via separate Redis keys. TTL prevents deadlock. Cleanup removes stale slots. All tests green.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create wait time estimator with EMA and tier-aware defaults (TDD)</name>
  <files>
    backend/app/queue/estimator.py
    backend/tests/domain/test_estimator.py
  </files>
  <action>
    **RED phase first — write failing tests:**

    Create `backend/tests/domain/test_estimator.py` using fakeredis async:
    - Test default duration for bootstrapper is 480s (8min)
    - Test default duration for partner is 600s (10min)
    - Test default duration for cto_scale is 900s (15min)
    - Test record_completion updates EMA: record 600s, then record 300s, new avg = 0.3*300 + 0.7*600 = 510
    - Test estimate_wait_time: position 3, default bootstrapper (480s), 1 worker = 1440s
    - Test estimate_with_confidence returns dict with estimate_seconds, lower_bound, upper_bound, message, confidence
    - Test format_wait_time: <60s shows seconds, 60-3599 shows minutes, 3600+ shows hours+minutes
    - Test estimate adapts after multiple recordings (EMA converges)

    **GREEN phase — implement:**

    Create `backend/app/queue/estimator.py`:
    ```python
    class WaitTimeEstimator:
        """Estimates wait time using Exponential Moving Average per tier."""

        DEFAULTS = {
            "bootstrapper": 480,   # 8min
            "partner": 600,        # 10min
            "cto_scale": 900,      # 15min
        }

        def __init__(self, redis: Redis, alpha: float = 0.3):
            self.redis = redis
            self.alpha = alpha  # EMA weight (0.3 = 30% new, 70% historical)

        async def record_completion(self, tier: str, duration_seconds: float) -> None:
            key = f"queue:avg_duration:{tier}"
            current_avg = float(await self.redis.get(key) or self.DEFAULTS.get(tier, 600))
            new_avg = self.alpha * duration_seconds + (1 - self.alpha) * current_avg
            await self.redis.set(key, str(new_avg))

        async def estimate_wait_time(self, tier: str, position: int, active_workers: int = 1) -> int:
            key = f"queue:avg_duration:{tier}"
            avg = float(await self.redis.get(key) or self.DEFAULTS.get(tier, 600))
            workers = max(active_workers, 1)
            return int((avg * position) / workers)

        async def estimate_with_confidence(self, tier: str, position: int, active_workers: int = 1) -> dict:
            key = f"queue:avg_duration:{tier}"
            avg = float(await self.redis.get(key) or self.DEFAULTS.get(tier, 600))
            workers = max(active_workers, 1)

            estimate = (avg * position) / workers
            lower = estimate * 0.7
            upper = estimate * 1.3

            return {
                "estimate_seconds": int(estimate),
                "lower_bound": int(lower),
                "upper_bound": int(upper),
                "message": f"{self.format_wait_time(int(lower))}-{self.format_wait_time(int(upper))}",
                "confidence": "medium" if position < 10 else "low",
            }

        @staticmethod
        def format_wait_time(seconds: int) -> str:
            if seconds < 60:
                return f"{seconds} seconds"
            elif seconds < 3600:
                minutes = seconds // 60
                return f"{minutes} minute{'s' if minutes != 1 else ''}"
            else:
                hours = seconds // 3600
                minutes = (seconds % 3600) // 60
                if minutes > 0:
                    return f"{hours}h {minutes}m"
                return f"{hours}h"
    ```
  </action>
  <verify>
    Run: `cd /Users/vladcortex/co-founder && python -m pytest backend/tests/domain/test_estimator.py -v`
    All 8+ tests pass.
  </verify>
  <done>
    WaitTimeEstimator uses EMA with alpha=0.3. Tier-aware defaults (480/600/900). Confidence intervals at +/-30%. Format produces human-readable strings. All tests green.
  </done>
</task>

</tasks>

<verification>
- `python -m pytest backend/tests/domain/test_semaphore.py backend/tests/domain/test_estimator.py -v` — all tests pass
- Verify TIER_CONCURRENT_USER matches locked decisions: {bootstrapper: 2, partner: 3, cto_scale: 10}
- Verify TIER_CONCURRENT_PROJECT matches: {bootstrapper: 2, partner: 3, cto_scale: 5}
- Verify estimator defaults match research: {bootstrapper: 480, partner: 600, cto_scale: 900}
</verification>

<success_criteria>
- RedisSemaphore enforces per-user and per-project concurrency limits
- TTL prevents deadlock on worker crash
- WaitTimeEstimator adapts to recent job durations via EMA
- Tier-specific defaults provide realistic initial estimates
- Human-readable format for all time ranges
- All tests green with fakeredis
</success_criteria>

<output>
After completion, create `.planning/phases/05-capacity-queue-worker-model/05-02-SUMMARY.md`
</output>
