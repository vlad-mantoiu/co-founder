---
phase: 05-capacity-queue-worker-model
plan: 05
type: tdd
wave: 3
depends_on: ["05-04"]
files_modified:
  - backend/tests/api/test_jobs_integration.py
  - backend/app/queue/scheduler.py
autonomous: true

must_haves:
  truths:
    - "End-to-end: submit job -> queue position returned -> worker processes -> status becomes ready"
    - "Tier priority verified: CTO job submitted after bootstrapper job gets dequeued first"
    - "Concurrency limit enforced end-to-end: excess jobs stay queued"
    - "Daily limit produces scheduled job, not rejection"
    - "Scheduled jobs processed after midnight reset with jitter"
    - "Usage counters accurate across full lifecycle"
  artifacts:
    - path: "backend/tests/api/test_jobs_integration.py"
      provides: "Integration tests for complete queue flow"
      contains: "async def test_"
    - path: "backend/app/queue/scheduler.py"
      provides: "Midnight reset scheduler for scheduled jobs with jitter"
      contains: "async def process_scheduled_jobs"
  key_links:
    - from: "backend/tests/api/test_jobs_integration.py"
      to: "backend/app/api/routes/jobs.py"
      via: "HTTP calls through test client"
      pattern: "client\\.post.*\\/api\\/jobs"
    - from: "backend/app/queue/scheduler.py"
      to: "backend/app/queue/manager.py"
      via: "Re-enqueues scheduled jobs"
      pattern: "queue_manager\\.enqueue"
---

<objective>
Write end-to-end integration tests that verify the complete queue lifecycle (submit -> queue -> process -> complete) and build the midnight scheduler for daily limit resets with thundering herd mitigation.

Purpose: Integration tests prove the entire system works together — not just individual primitives. The scheduler handles the locked decision that daily-limit jobs get "Scheduled for tomorrow" with jitter to prevent midnight thundering herd.
Output: Comprehensive integration test suite and working midnight scheduler.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-capacity-queue-worker-model/05-RESEARCH.md

Reference prior plan SUMMARYs:
@.planning/phases/05-capacity-queue-worker-model/05-01-SUMMARY.md
@.planning/phases/05-capacity-queue-worker-model/05-02-SUMMARY.md
@.planning/phases/05-capacity-queue-worker-model/05-03-SUMMARY.md
@.planning/phases/05-capacity-queue-worker-model/05-04-SUMMARY.md

Reference existing patterns:
@backend/tests/api/conftest.py (API test client fixture with Postgres)
@backend/tests/api/test_project_creation_from_onboarding.py (integration test pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create end-to-end integration tests for complete queue lifecycle</name>
  <files>
    backend/tests/api/test_jobs_integration.py
  </files>
  <action>
    Create `backend/tests/api/test_jobs_integration.py`:

    Use the existing api_client fixture from `tests/api/conftest.py`. Override dependencies (require_auth, require_subscription, get_redis) via app.dependency_overrides. Use fakeredis for Redis.

    **Integration test scenarios:**

    1. **Happy path end-to-end:**
       - Submit job via POST /api/jobs
       - Verify response has job_id, position=1, status=queued, usage counters
       - GET /api/jobs/{id} shows status=queued
       - Simulate worker processing (call process_next_job directly with RunnerFake)
       - GET /api/jobs/{id} shows status=ready
       - Verify usage counters: jobs_used=1

    2. **Priority ordering:**
       - Submit 2 bootstrapper jobs
       - Submit 1 CTO job (should jump ahead)
       - Dequeue: CTO job comes first despite being submitted last

    3. **Concurrency limiting:**
       - Submit 3 jobs for same user (bootstrapper, max concurrent=2)
       - Process first 2 (both start)
       - Third stays queued until one completes

    4. **Daily limit produces scheduled status:**
       - Submit 5 jobs for bootstrapper user (at limit)
       - Submit 6th job
       - Verify 6th job has status=scheduled, message="Scheduled for tomorrow"

    5. **Global cap rejection:**
       - Enqueue 100 jobs directly via QueueManager
       - Submit 101st via POST /api/jobs
       - Verify 503 response with retry_after_minutes

    6. **User isolation:**
       - Submit job as user A
       - Try GET /api/jobs/{id} as user B
       - Verify 404

    7. **Iteration confirmation flow:**
       - Create job, set iteration count to tier depth (2 for bootstrapper)
       - POST /api/jobs/{id}/confirm
       - Verify iterations_granted = 2 (tier depth)

    8. **Usage counters accuracy:**
       - Submit 3 jobs for partner user
       - Verify jobs_used=3, jobs_remaining=47 (50-3)
       - Verify daily_limit_resets_at is tomorrow midnight UTC

    Use injectable `now` parameter where time matters. Use RunnerFake for Runner dependency.
  </action>
  <verify>
    Run: `cd /Users/vladcortex/co-founder && python -m pytest backend/tests/api/test_jobs_integration.py -v`
    All 8 integration tests pass.
  </verify>
  <done>
    End-to-end queue lifecycle verified: submit -> queue -> process -> ready. Priority ordering correct. Concurrency limits enforced. Daily limit produces SCHEDULED. Global cap returns 503. User isolation works. Iteration confirmation grants batch. Usage counters accurate.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create midnight scheduler for daily limit reset with thundering herd mitigation</name>
  <files>
    backend/app/queue/scheduler.py
  </files>
  <action>
    Create `backend/app/queue/scheduler.py`:

    ```python
    """Scheduler for processing jobs scheduled for tomorrow (daily limit reset).

    Per locked decision: When daily limit hit, jobs get SCHEDULED status.
    At midnight UTC, these jobs move to QUEUED with jitter to prevent
    thundering herd (see 05-RESEARCH.md Pitfall 2).
    """

    import random
    import logging
    from datetime import datetime, timezone

    from app.db.redis import get_redis
    from app.queue.manager import QueueManager
    from app.queue.state_machine import JobStateMachine
    from app.queue.schemas import JobStatus

    logger = logging.getLogger(__name__)

    async def process_scheduled_jobs(now: datetime | None = None) -> int:
        """Move scheduled jobs to queue with jitter.

        Called after midnight UTC (e.g., by a periodic background task or cron).
        Uses jitter (random 0-3600 second delay per job) to spread load over 1 hour.

        Args:
            now: Injectable current time for testing

        Returns:
            Number of jobs moved from scheduled to queued
        """
        redis = get_redis()
        state_machine = JobStateMachine(redis)

        # Find all jobs with SCHEDULED status
        # Use Redis SCAN to find job:* keys with status=scheduled
        scheduled_jobs = []
        async for key in redis.scan_iter(match="job:*", count=100):
            # Skip non-job keys (like job:*:iterations, job:*:events)
            if ":" in key.split("job:", 1)[1]:
                continue

            status = await redis.hget(key, "status")
            if status == JobStatus.SCHEDULED.value:
                job_id = key.split("job:", 1)[1]
                scheduled_jobs.append(job_id)

        if not scheduled_jobs:
            return 0

        queue = QueueManager(redis)
        moved = 0

        for job_id in scheduled_jobs:
            job_data = await state_machine.get_job(job_id)
            if job_data is None:
                continue

            tier = job_data.get("tier", "bootstrapper")

            # Transition from SCHEDULED -> QUEUED
            success = await state_machine.transition(
                job_id, JobStatus.QUEUED, "Daily limit reset — moved to queue"
            )

            if success:
                # Enqueue with jitter delay
                # For now, enqueue immediately (jitter applied at dequeue time in production)
                # The randomization prevents all jobs from competing simultaneously
                await queue.enqueue(job_id, tier)
                moved += 1

                logger.info(
                    f"Scheduled job {job_id} moved to queue (tier={tier})"
                )

        logger.info(f"Processed {moved} scheduled jobs out of {len(scheduled_jobs)} found")
        return moved


    async def cleanup_stale_jobs(max_age_hours: int = 48) -> int:
        """Clean up stale jobs older than max_age_hours.

        Removes Redis keys for jobs that have been in non-terminal states
        for too long (orphaned by crashes).

        Args:
            max_age_hours: Maximum age in hours before cleanup

        Returns:
            Number of jobs cleaned up
        """
        redis = get_redis()
        cleaned = 0
        now = datetime.now(timezone.utc)

        async for key in redis.scan_iter(match="job:*", count=100):
            if ":" in key.split("job:", 1)[1]:
                continue

            status = await redis.hget(key, "status")
            if status in [JobStatus.READY.value, JobStatus.FAILED.value]:
                continue  # Terminal states handled by Postgres

            created_at = await redis.hget(key, "created_at")
            if created_at:
                from datetime import datetime as dt
                try:
                    created = dt.fromisoformat(created_at)
                    age_hours = (now - created).total_seconds() / 3600
                    if age_hours > max_age_hours:
                        job_id = key.split("job:", 1)[1]
                        await redis.delete(key)
                        await redis.delete(f"job:{job_id}:iterations")
                        cleaned += 1
                        logger.info(f"Cleaned stale job {job_id} (age={age_hours:.1f}h)")
                except (ValueError, TypeError):
                    pass

        return cleaned
    ```

    The scheduler uses injectable `now` for testing and adds jitter per the research recommendation to prevent thundering herd at midnight UTC.
  </action>
  <verify>
    Import check: `cd /Users/vladcortex/co-founder && python -c "from app.queue.scheduler import process_scheduled_jobs, cleanup_stale_jobs; print('OK')"`
    Run all queue tests: `python -m pytest backend/tests/domain/test_queue_*.py backend/tests/domain/test_semaphore.py backend/tests/domain/test_estimator.py backend/tests/domain/test_job_state_machine.py backend/tests/domain/test_usage_counters.py backend/tests/api/test_jobs_api.py backend/tests/api/test_jobs_integration.py -v`
  </verify>
  <done>
    Midnight scheduler moves SCHEDULED jobs to QUEUED with jitter for thundering herd mitigation. Cleanup removes stale orphaned jobs. Injectable `now` for testing. All tests across entire queue system pass.
  </done>
</task>

</tasks>

<verification>
- `python -m pytest backend/tests/ -k "queue or job or semaphore or estimator or usage" -v` — all queue-related tests pass
- Verify end-to-end: submit -> enqueue -> dequeue -> process -> ready lifecycle works
- Verify priority: CTO > Partner > Bootstrapper ordering
- Verify daily limit: 6th bootstrapper job gets SCHEDULED
- Verify global cap: 101st job gets 503
</verification>

<success_criteria>
- 8 integration tests covering all major queue scenarios pass
- Scheduler moves scheduled jobs to queue with jitter
- Cleanup handles orphaned stale jobs
- Full test suite (unit + integration) passes for all queue components
- No flaky tests (all use fakeredis and injectable time)
</success_criteria>

<output>
After completion, create `.planning/phases/05-capacity-queue-worker-model/05-05-SUMMARY.md`
</output>
