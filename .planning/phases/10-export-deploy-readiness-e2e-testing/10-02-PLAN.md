---
phase: 10-export-deploy-readiness-e2e-testing
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - backend/app/domain/alignment.py
  - backend/app/domain/deploy_checks.py
  - backend/tests/domain/test_alignment.py
  - backend/tests/domain/test_deploy_checks.py
autonomous: true

must_haves:
  truths:
    - "Alignment score computation returns 0-100 score and scope_creep_detected boolean"
    - "Deploy readiness checks workspace files and returns pass/warn/fail per check"
    - "Deploy paths are hardcoded constants (no LLM needed)"
    - "Empty change list returns score 100 (fully aligned)"
  artifacts:
    - path: "backend/app/domain/alignment.py"
      provides: "Alignment score pure function"
      exports: ["compute_alignment_score"]
    - path: "backend/app/domain/deploy_checks.py"
      provides: "Deploy readiness checks and paths"
      exports: ["run_deploy_checks", "DEPLOY_PATHS", "DeployCheck", "DeployPathOption"]
  key_links:
    - from: "backend/app/domain/alignment.py"
      to: "backend/app/services/gate_service.py"
      via: "Called during Gate 2 resolution"
      pattern: "compute_alignment_score"
    - from: "backend/app/domain/deploy_checks.py"
      to: "backend/app/api/routes/deploy_readiness.py"
      via: "Called by deploy readiness endpoint"
      pattern: "run_deploy_checks"
---

<objective>
Implement pure domain functions for alignment scoring and deploy readiness checks.

Purpose: SOLD-02 (alignment check + scope creep), DEPL-01 (readiness + blocking issues + deploy path). These are pure functions with no I/O dependencies — ideal TDD candidates that other plans depend on.

Output: Two domain modules with comprehensive tests.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-export-deploy-readiness-e2e-testing/10-RESEARCH.md

@backend/app/domain/gates.py
@backend/app/domain/risks.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Alignment score domain function (TDD)</name>
  <files>backend/app/domain/alignment.py, backend/tests/domain/test_alignment.py</files>
  <action>
    **RED — Write tests first in `backend/tests/domain/test_alignment.py`:**

    ```python
    def test_no_changes_returns_100():
        score, creep = compute_alignment_score({"core_features": [{"name": "Auth"}, {"name": "Dashboard"}]}, [])
        assert score == 100
        assert creep is False

    def test_all_aligned_changes():
        scope = {"core_features": [{"name": "Auth"}, {"name": "Dashboard"}]}
        changes = [{"description": "Fix auth login flow"}, {"description": "Add dashboard filter"}]
        score, creep = compute_alignment_score(scope, changes)
        assert score == 100
        assert creep is False

    def test_mixed_aligned_and_new():
        scope = {"core_features": [{"name": "Auth"}, {"name": "Dashboard"}]}
        changes = [
            {"description": "Fix auth login flow"},
            {"description": "Add payments integration"},  # New feature
            {"description": "Add dashboard charts"},
        ]
        score, creep = compute_alignment_score(scope, changes)
        assert score == 66  # 2/3 aligned, int truncation
        assert creep is False  # 66 >= 60

    def test_all_new_features_scope_creep():
        scope = {"core_features": [{"name": "Auth"}]}
        changes = [{"description": "Add payments"}, {"description": "Add social media"}]
        score, creep = compute_alignment_score(scope, changes)
        assert score == 0
        assert creep is True  # 0 < 60

    def test_no_original_features_returns_neutral():
        score, creep = compute_alignment_score({}, [{"description": "anything"}])
        assert score == 75
        assert creep is False

    def test_case_insensitive_matching():
        scope = {"core_features": [{"name": "User Auth"}]}
        changes = [{"description": "update USER AUTH flow"}]
        score, creep = compute_alignment_score(scope, changes)
        assert score == 100
    ```

    **GREEN — Implement `backend/app/domain/alignment.py`:**

    `compute_alignment_score(original_scope: dict, requested_changes: list[dict]) -> tuple[int, bool]`:
    1. If no changes, return (100, False)
    2. Extract original_features from scope["core_features"][*]["name"], lowercased
    3. If no original features, return (75, False) — neutral
    4. For each change, check if any original feature name appears in change["description"].lower()
    5. Score = int((aligned_count / total_changes) * 100)
    6. scope_creep = score < 60
    7. Return (score, scope_creep)

    Thresholds: >= 80 = green, 60-79 = yellow, < 60 = red (scope creep).
  </action>
  <verify>
    `cd /Users/vladcortex/co-founder && python -m pytest backend/tests/domain/test_alignment.py -v` — all 6 tests pass.
  </verify>
  <done>Alignment score function works correctly for all edge cases. SOLD-02 (alignment check + scope creep detection) satisfied.</done>
</task>

<task type="auto">
  <name>Task 2: Deploy readiness checks domain function (TDD)</name>
  <files>backend/app/domain/deploy_checks.py, backend/tests/domain/test_deploy_checks.py</files>
  <action>
    **RED — Write tests first in `backend/tests/domain/test_deploy_checks.py`:**

    ```python
    def test_all_checks_pass_with_complete_workspace():
        files = {
            "README.md": "# My App",
            ".env.example": "API_KEY=your_key_here",
            "package.json": '{"scripts": {"start": "node index.js"}, "dependencies": {"express": "4.18"}}',
            "index.js": "const express = require('express')",
        }
        checks = run_deploy_checks(files)
        passing = [c for c in checks if c.status == "pass"]
        assert len(passing) >= 4

    def test_missing_readme_is_warning():
        files = {".env.example": "KEY=val", "package.json": '{"scripts":{"start":"node ."}}'}
        checks = run_deploy_checks(files)
        readme_check = next(c for c in checks if c.id == "readme")
        assert readme_check.status == "warn"

    def test_missing_env_example_is_warning():
        files = {"README.md": "hi", "package.json": '{"scripts":{"start":"node ."}}'}
        checks = run_deploy_checks(files)
        env_check = next(c for c in checks if c.id == "env_example")
        assert env_check.status == "warn"

    def test_hardcoded_secrets_fail():
        files = {"config.py": 'API_KEY="sk-1234567890abcdef"', "README.md": "hi"}
        checks = run_deploy_checks(files)
        secrets_check = next(c for c in checks if c.id == "no_secrets")
        assert secrets_check.status == "fail"
        assert secrets_check.fix_instruction is not None

    def test_no_start_script_fail():
        files = {"README.md": "hi", "main.py": "print('hello')"}
        checks = run_deploy_checks(files)
        start_check = next(c for c in checks if c.id == "start_script")
        assert start_check.status == "fail"

    def test_deploy_paths_constant():
        assert len(DEPLOY_PATHS) == 3
        ids = [p.id for p in DEPLOY_PATHS]
        assert "vercel" in ids
        assert "railway" in ids
        assert "aws" in ids

    def test_overall_status_from_checks():
        files = {"README.md": "hi", ".env.example": "K=V", "package.json": '{"scripts":{"start":"node ."}}'}
        checks = run_deploy_checks(files)
        overall = compute_overall_status(checks)
        assert overall in ["green", "yellow", "red"]
    ```

    **GREEN — Implement `backend/app/domain/deploy_checks.py`:**

    Dataclasses:
    - `DeployCheck(id, title, status: Literal["pass","warn","fail"], message, fix_instruction: str|None)`
    - `DeployPathOption(id, name, description, difficulty, cost, tradeoffs: list[str], steps: list[str])`

    Function `run_deploy_checks(workspace_files: dict[str, str]) -> list[DeployCheck]`:
    1. Check README.md exists → pass/warn
    2. Check .env.example exists → pass/warn
    3. Check start script (package.json with scripts.start, OR Makefile, OR Procfile, OR main.py) → pass/fail
    4. Check no hardcoded secrets (scan all file contents for patterns like `API_KEY=["']sk-`, `SECRET=["']`, `password=["']` — not in .env.example) → pass/fail with fix_instruction
    5. Check dependencies pinned (package.json or requirements.txt exists) → pass/warn

    Function `compute_overall_status(checks: list[DeployCheck]) -> str`:
    - Any "fail" → "red"
    - Any "warn" → "yellow"
    - All "pass" → "green"

    Constant `DEPLOY_PATHS: list[DeployPathOption]` with 3 options per research (Vercel, Railway, AWS ECS). Per locked decision: include tradeoffs and step-by-step instructions.
  </action>
  <verify>
    `cd /Users/vladcortex/co-founder && python -m pytest backend/tests/domain/test_deploy_checks.py -v` — all 7 tests pass.
  </verify>
  <done>Deploy checks detect missing files, hardcoded secrets. DEPLOY_PATHS has 3 options with tradeoffs. DEPL-01 (readiness boolean, blocking issues, deploy path) satisfied at domain layer.</done>
</task>

</tasks>

<verification>
- `compute_alignment_score` returns correct scores for all edge cases
- `run_deploy_checks` detects README, env, start script, secrets, deps
- `DEPLOY_PATHS` has 3 options with name, steps, tradeoffs
- `compute_overall_status` returns green/yellow/red correctly
- All 13 tests pass
</verification>

<success_criteria>
- `python -m pytest backend/tests/domain/test_alignment.py backend/tests/domain/test_deploy_checks.py -v` — all pass
- No external dependencies needed (pure functions)
</success_criteria>

<output>
After completion, create `.planning/phases/10-export-deploy-readiness-e2e-testing/10-02-SUMMARY.md`
</output>
