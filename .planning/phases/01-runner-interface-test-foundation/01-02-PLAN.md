---
phase: 01-runner-interface-test-foundation
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - backend/app/agent/runner_fake.py
  - backend/tests/domain/test_runner_fake.py
autonomous: true

must_haves:
  truths:
    - "RunnerFake satisfies the Runner protocol (isinstance check passes)"
    - "RunnerFake('happy_path') returns complete, realistic responses for all 5 methods"
    - "RunnerFake('llm_failure') raises RuntimeError on run/step/generate calls"
    - "RunnerFake('partial_build') returns state with test failures (exit_code=1)"
    - "RunnerFake('rate_limited') raises RuntimeError with estimated wait info"
    - "All 4 scenarios return instantly (no delays, no LLM calls)"
    - "Scenario responses contain realistic content (plausible code, briefs, artifacts — not lorem ipsum)"
  artifacts:
    - path: "backend/app/agent/runner_fake.py"
      provides: "Scenario-based test double for Runner protocol"
      contains: "class RunnerFake"
    - path: "backend/tests/domain/test_runner_fake.py"
      provides: "Tests for all 4 RunnerFake scenarios"
      min_lines: 100
  key_links:
    - from: "backend/app/agent/runner_fake.py"
      to: "backend/app/agent/runner.py"
      via: "Runner protocol satisfaction"
      pattern: "from app\\.agent\\.runner import Runner"
    - from: "backend/app/agent/runner_fake.py"
      to: "backend/app/agent/state.py"
      via: "CoFounderState type usage"
      pattern: "from app\\.agent\\.state import CoFounderState"
---

<objective>
Implement RunnerFake with 4 named scenarios providing deterministic test doubles for the entire test suite.

Purpose: Enable all tests to run without LLM calls, external services, or network access. RunnerFake is the cornerstone of the TDD approach — every future phase's tests will use it.

Output: `runner_fake.py` (4 scenarios), `test_runner_fake.py` (scenario behavior tests)
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-runner-interface-test-foundation/01-RESEARCH.md

# Source files for state schema and protocol
@backend/app/agent/state.py
@backend/app/agent/runner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write RunnerFake scenario tests (RED)</name>
  <files>backend/tests/domain/test_runner_fake.py</files>
  <action>
Create `backend/tests/domain/test_runner_fake.py` with comprehensive tests for all 4 scenarios.

**Test structure** (all tests are async, use `@pytest.mark.asyncio` or rely on `asyncio_mode = "auto"`):

**happy_path scenario tests:**
- `test_happy_path_run_returns_complete_state`: RunnerFake("happy_path").run(state) returns state with `is_complete=True`, non-empty `plan`, non-empty `working_files`
- `test_happy_path_step_returns_stage_state`: RunnerFake("happy_path").step(state, "architect") returns state with `current_node="architect"` and non-empty `plan`
- `test_happy_path_generate_questions_returns_questions`: Returns list of 5+ dicts, each with `id`, `text`, `required` keys
- `test_happy_path_generate_brief_returns_brief`: Returns dict with all 8 required keys (problem_statement, target_user, value_prop, differentiation, monetization_hypothesis, assumptions, risks, smallest_viable_experiment)
- `test_happy_path_generate_artifacts_returns_artifacts`: Returns dict with 5 keys (product_brief, mvp_scope, milestones, risk_log, how_it_works), each non-empty string
- `test_happy_path_plan_has_realistic_content`: Plan steps have descriptive text (not "test" or "placeholder"), files_to_modify lists have plausible paths
- `test_happy_path_code_has_realistic_content`: working_files values contain actual Python/JS code (check for `def ` or `class ` or `function ` or `import `)

**llm_failure scenario tests:**
- `test_llm_failure_run_raises_runtime_error`: RunnerFake("llm_failure").run(state) raises RuntimeError with "rate limit" or "API" in message
- `test_llm_failure_step_raises_runtime_error`: Same for step()
- `test_llm_failure_generate_questions_raises`: Same for generate_questions()

**partial_build scenario tests:**
- `test_partial_build_run_returns_incomplete`: run() returns state with `is_complete=False`, non-empty `active_errors`, `last_command_exit_code=1`
- `test_partial_build_has_plan_and_code`: State still has `plan` and `working_files` (code was generated, tests failed)

**rate_limited scenario tests:**
- `test_rate_limited_run_raises_with_wait_info`: run() raises RuntimeError with "capacity" or "wait" in message
- `test_rate_limited_step_raises`: Same for step()

**Cross-scenario tests:**
- `test_invalid_scenario_raises_value_error`: RunnerFake("nonexistent") raises ValueError
- `test_runner_fake_satisfies_protocol`: isinstance(RunnerFake(), Runner) is True
- `test_all_scenarios_return_instantly`: Time run() for happy_path — must complete in <100ms (no real LLM calls)

Use `create_initial_state` from `app.agent.state` to build test states. Import RunnerFake from `app.agent.runner_fake` (does not exist yet — tests will fail).
  </action>
  <verify>Run `cd /Users/vladcortex/co-founder/backend && python -m pytest tests/domain/test_runner_fake.py -v 2>&1 | head -50`. ALL tests MUST FAIL with ImportError (RunnerFake doesn't exist yet). This confirms RED state.</verify>
  <done>17+ tests written for all 4 scenarios. All fail because RunnerFake doesn't exist (RED).</done>
</task>

<task type="auto">
  <name>Task 2: Implement RunnerFake with 4 deterministic scenarios (GREEN)</name>
  <files>backend/app/agent/runner_fake.py</files>
  <action>
Create `backend/app/agent/runner_fake.py` implementing the Runner protocol with scenario-based deterministic responses.

**Design decisions (Claude's discretion):**
- Instant returns (no configurable delays) — fastest and most reliable for CI
- Fully deterministic (same scenario = identical output every time) — most practical for CI assertions
- No GenericFakeChatModel dependency — RunnerFake returns pre-built data directly, simpler and faster

**Class structure:**

```python
class RunnerFake:
    """Scenario-based test double for Runner protocol.

    Provides deterministic, instant responses for 4 named scenarios:
    - happy_path: Full successful flow with realistic content
    - llm_failure: API/rate limit failure at any stage
    - partial_build: Code generated but tests fail
    - rate_limited: Worker capacity exceeded
    """

    VALID_SCENARIOS = {"happy_path", "llm_failure", "partial_build", "rate_limited"}

    def __init__(self, scenario: str = "happy_path"):
        if scenario not in self.VALID_SCENARIOS:
            raise ValueError(f"Unknown scenario: {scenario}. Valid: {self.VALID_SCENARIOS}")
        self.scenario = scenario
```

**Scenario data requirements (REALISTIC content, per locked decisions):**

**happy_path:**
- `plan`: 3-4 PlanStep dicts describing building a simple inventory tracker (realistic descriptions like "Create Product model with name, SKU, quantity, and price fields", plausible file paths like "src/models/product.py")
- `working_files`: 2-3 FileChange dicts with actual Python code (real imports, classes, functions — 20+ lines each)
- `questions`: 5 dicts with realistic onboarding questions (e.g., "Who is your target customer?", "What's the core problem you're solving?")
- `brief`: Full 8-field dict with realistic inventory app brief content
- `artifacts`: 5 document strings with realistic content (product brief paragraph, MVP scope list, milestones with dates, risk log entries, how-it-works explanation)
- `run()` returns state merged with: plan, working_files, is_complete=True, current_node="git_manager", exit_code=0
- `step()` returns state merged with: current_node=stage, plan (if architect), code (if coder), etc.

**llm_failure:**
- All methods raise `RuntimeError("Anthropic API rate limit exceeded. Retry after 60 seconds.")`

**partial_build:**
- `run()` returns state with: plan (same as happy_path), working_files (same), but is_complete=False, last_command_exit_code=1, active_errors=[ErrorInfo with realistic TypeError message], current_node="executor"

**rate_limited:**
- All methods raise `RuntimeError("Worker capacity exceeded. Estimated wait: 5 minutes. Current queue depth: 12.")`

**Method implementations:**
- `run()`: Check scenario, return pre-built state or raise
- `step()`: Validate stage name (architect/coder/executor/debugger/reviewer/git_manager), return stage-specific partial state or raise. Raise ValueError for invalid stage.
- `generate_questions()`: Return pre-built questions list or raise
- `generate_brief()`: Return pre-built brief dict or raise
- `generate_artifacts()`: Return pre-built artifacts dict or raise
  </action>
  <verify>Run `cd /Users/vladcortex/co-founder/backend && python -m pytest tests/domain/test_runner_fake.py -v`. ALL tests MUST PASS.</verify>
  <done>RunnerFake implements all 4 scenarios with realistic content. All 17+ scenario tests pass. isinstance(RunnerFake(), Runner) is True. Each scenario completes in <100ms.</done>
</task>

</tasks>

<verification>
1. `python -c "from app.agent.runner_fake import RunnerFake; print(RunnerFake.VALID_SCENARIOS)"` shows 4 scenarios
2. `python -c "from app.agent.runner_fake import RunnerFake; from app.agent.runner import Runner; assert isinstance(RunnerFake(), Runner)"` passes
3. `cd backend && python -m pytest tests/domain/test_runner_fake.py -v` — all tests pass
4. `cd backend && python -m pytest tests/domain/ -v --durations=0` — all domain tests complete in <5 seconds total
</verification>

<success_criteria>
- RunnerFake has 4 scenarios: happy_path, llm_failure, partial_build, rate_limited
- happy_path returns realistic content (plausible code, briefs, questions, artifacts)
- llm_failure and rate_limited raise RuntimeError with descriptive messages
- partial_build returns incomplete state with test failure info
- All scenarios return instantly (no LLM calls, no delays)
- RunnerFake satisfies Runner protocol
- All scenario tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-runner-interface-test-foundation/01-02-SUMMARY.md`
</output>
