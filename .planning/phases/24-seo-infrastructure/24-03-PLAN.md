---
phase: 24-seo-infrastructure
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - marketing/next-sitemap.config.js
  - marketing/package.json
  - marketing/scripts/validate-jsonld.mjs
autonomous: true
requirements:
  - SEO-06
  - SEO-07

must_haves:
  truths:
    - "Running npm run build generates sitemap.xml in the out/ directory"
    - "Running npm run build generates robots.txt in the out/ directory"
    - "sitemap.xml contains URLs for all 8 marketing pages with trailing slashes"
    - "robots.txt references the sitemap URL and allows all user agents"
    - "Build-time JSON-LD validation runs as part of postbuild and exits 0 on valid schemas"
  artifacts:
    - path: "marketing/next-sitemap.config.js"
      provides: "next-sitemap configuration for static export"
      contains: "outDir"
    - path: "marketing/package.json"
      provides: "postbuild script running next-sitemap and JSON-LD validation"
      contains: "postbuild"
    - path: "marketing/scripts/validate-jsonld.mjs"
      provides: "Build-time JSON-LD validation for Organization, WebSite, SoftwareApplication"
  key_links:
    - from: "marketing/package.json"
      to: "marketing/next-sitemap.config.js"
      via: "postbuild script calls next-sitemap"
      pattern: "next-sitemap"
    - from: "marketing/package.json"
      to: "marketing/scripts/validate-jsonld.mjs"
      via: "postbuild script calls node scripts/validate-jsonld.mjs"
      pattern: "validate-jsonld"
    - from: "marketing/next-sitemap.config.js"
      to: "marketing/out/"
      via: "outDir: 'out' directs output to static export directory"
      pattern: "outDir.*out"
---

<objective>
Set up next-sitemap for XML sitemap and robots.txt generation, plus a build-time JSON-LD validation script.

Purpose: Search engines need sitemap.xml to discover all pages and robots.txt for crawl guidance. The JSON-LD validation script catches schema regressions at build time, preventing broken structured data from reaching production.

Output: `next-sitemap.config.js`, updated `package.json` with postbuild script, `scripts/validate-jsonld.mjs` validation script.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-seo-infrastructure/24-RESEARCH.md

@marketing/package.json
@marketing/next.config.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install next-sitemap and create configuration</name>
  <files>
    marketing/next-sitemap.config.js
    marketing/package.json
  </files>
  <action>
    1. Install next-sitemap as a dev dependency:
       ```bash
       cd /Users/vladcortex/co-founder/marketing && npm install next-sitemap --save-dev
       ```

    2. Create `marketing/next-sitemap.config.js`:
       ```javascript
       /** @type {import('next-sitemap').IConfig} */
       module.exports = {
         siteUrl: 'https://getinsourced.ai',
         output: 'export',
         outDir: 'out',
         generateRobotsTxt: true,
         generateIndexSitemap: false,
         autoLastmod: true,
         changefreq: 'weekly',
         priority: 0.7,
         trailingSlash: true,
         robotsTxtOptions: {
           policies: [
             { userAgent: '*', allow: '/' },
           ],
         },
       }
       ```

       Key configuration choices:
       - `output: 'export'` + `outDir: 'out'` — matches Next.js static export output directory. Without this, sitemap goes to `public/` and is never deployed (deploy pipeline syncs `marketing/out/` to S3).
       - `generateIndexSitemap: false` — no need for sitemap index with only 8 pages.
       - `generateRobotsTxt: true` — auto-generates robots.txt with sitemap reference.
       - `trailingSlash: true` — matches next.config.ts trailingSlash setting.
       - All crawlers allowed (user decision: allow all AI crawlers, no blocking).
       - All 8 pages included (user discretion decision: include privacy + terms for complete sitemap).

    3. Update `marketing/package.json` scripts to add postbuild:
       ```json
       "scripts": {
         "dev": "next dev",
         "build": "next build",
         "postbuild": "next-sitemap",
         "start": "next start",
         "lint": "next lint"
       }
       ```
       The `postbuild` script runs automatically after `npm run build` by npm convention. Do NOT chain in the `build` script itself.
  </action>
  <verify>
    - `cd /Users/vladcortex/co-founder/marketing && npm run build`
    - `ls out/sitemap.xml out/robots.txt` — both files exist in out/
    - `cat out/sitemap.xml` — contains URLs for all 8 pages with trailing slashes
    - `cat out/robots.txt` — contains `Allow: /` and `Sitemap: https://getinsourced.ai/sitemap.xml`
    - `grep -c 'getinsourced.ai' out/sitemap.xml` — returns 8 (one per page)
  </verify>
  <done>
    next-sitemap generates sitemap.xml and robots.txt into out/ directory during postbuild. Sitemap lists all 8 pages. robots.txt allows all crawlers and references sitemap.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create build-time JSON-LD validation script</name>
  <files>
    marketing/scripts/validate-jsonld.mjs
    marketing/package.json
  </files>
  <action>
    1. Create the `marketing/scripts/` directory if it does not exist.

    2. Create `marketing/scripts/validate-jsonld.mjs`:
       ```javascript
       #!/usr/bin/env node
       /**
        * Build-time JSON-LD validation script.
        * Reads built HTML from out/ and validates JSON-LD schemas
        * against Google Rich Results required fields.
        *
        * Run: node scripts/validate-jsonld.mjs
        * Called automatically by postbuild script.
        */
       import { readFileSync, existsSync } from 'node:fs'
       import { join } from 'node:path'

       const OUT_DIR = join(process.cwd(), 'out')
       const errors = []
       const warnings = []
       let totalSchemas = 0

       /**
        * Extract JSON-LD blocks from an HTML file
        */
       function extractSchemas(htmlPath) {
         if (!existsSync(htmlPath)) {
           errors.push(`File not found: ${htmlPath}`)
           return []
         }
         const html = readFileSync(htmlPath, 'utf8')
         const regex = /<script type="application\/ld\+json">([\s\S]*?)<\/script>/g
         const schemas = []
         let match
         while ((match = regex.exec(html)) !== null) {
           try {
             schemas.push(JSON.parse(match[1]))
           } catch (e) {
             errors.push(`Invalid JSON-LD in ${htmlPath}: ${e.message}`)
           }
         }
         return schemas
       }

       /**
        * Validate Organization schema (Google Rich Results requirements)
        */
       function validateOrganization(schema, file) {
         if (!schema.name) errors.push(`${file}: Organization missing 'name'`)
         if (!schema.url) errors.push(`${file}: Organization missing 'url'`)
         if (!schema.logo) errors.push(`${file}: Organization missing 'logo' (required for Logo rich result)`)
         if (schema.sameAs && Array.isArray(schema.sameAs) && schema.sameAs.length === 0) {
           warnings.push(`${file}: Organization has empty 'sameAs' array — remove or populate`)
         }
       }

       /**
        * Validate WebSite schema (Google Rich Results requirements)
        */
       function validateWebSite(schema, file) {
         if (!schema.name) errors.push(`${file}: WebSite missing 'name'`)
         if (!schema.url) errors.push(`${file}: WebSite missing 'url'`)
       }

       /**
        * Validate SoftwareApplication schema (Google Rich Results requirements)
        */
       function validateSoftwareApplication(schema, file) {
         if (!schema.name) errors.push(`${file}: SoftwareApplication missing 'name'`)
         if (!schema.offers) {
           errors.push(`${file}: SoftwareApplication missing 'offers' (required)`)
         } else {
           if (schema.offers.price === undefined) {
             errors.push(`${file}: SoftwareApplication missing 'offers.price' (required)`)
           }
           if (!schema.offers.priceCurrency) {
             errors.push(`${file}: SoftwareApplication missing 'offers.priceCurrency' (required)`)
           }
         }
         if (!schema.aggregateRating && !schema.review) {
           warnings.push(`${file}: SoftwareApplication has no aggregateRating or review — star ratings will not show in search results (acceptable for new products)`)
         }
       }

       // Pages to validate
       // Homepage has Organization + WebSite
       // /cofounder has SoftwareApplication
       const pagesToValidate = [
         { path: 'index.html', expectedTypes: ['Organization', 'WebSite'] },
         { path: 'cofounder/index.html', expectedTypes: ['SoftwareApplication'] },
       ]

       console.log('Validating JSON-LD schemas...\n')

       for (const page of pagesToValidate) {
         const htmlPath = join(OUT_DIR, page.path)
         const schemas = extractSchemas(htmlPath)

         if (schemas.length === 0 && page.expectedTypes.length > 0) {
           errors.push(`${page.path}: No JSON-LD schemas found (expected: ${page.expectedTypes.join(', ')})`)
           continue
         }

         for (const schema of schemas) {
           totalSchemas++
           const type = schema['@type']

           if (!schema['@context'] || !schema['@context'].includes('schema.org')) {
             errors.push(`${page.path}: Schema missing '@context' with schema.org`)
           }

           switch (type) {
             case 'Organization':
               validateOrganization(schema, page.path)
               break
             case 'WebSite':
               validateWebSite(schema, page.path)
               break
             case 'SoftwareApplication':
               validateSoftwareApplication(schema, page.path)
               break
             default:
               warnings.push(`${page.path}: Unknown schema type '${type}'`)
           }
         }

         // Check expected types are present
         const foundTypes = schemas.map(s => s['@type'])
         for (const expected of page.expectedTypes) {
           if (!foundTypes.includes(expected)) {
             errors.push(`${page.path}: Missing expected schema type '${expected}'`)
           }
         }

         console.log(`  ${page.path}: ${schemas.length} schema(s) — ${schemas.map(s => s['@type']).join(', ')}`)
       }

       console.log('')

       if (warnings.length > 0) {
         console.warn('Warnings:')
         warnings.forEach(w => console.warn(`  ⚠ ${w}`))
         console.log('')
       }

       if (errors.length > 0) {
         console.error('Errors:')
         errors.forEach(e => console.error(`  ✗ ${e}`))
         console.log('')
         console.error(`JSON-LD validation FAILED — ${errors.length} error(s)`)
         process.exit(1)
       }

       console.log(`JSON-LD validation passed — ${totalSchemas} schema(s) validated across ${pagesToValidate.length} page(s)`)
       ```

    3. Update `marketing/package.json` postbuild script to chain the validation:
       ```json
       "postbuild": "next-sitemap && node scripts/validate-jsonld.mjs"
       ```
       This runs next-sitemap first (generates sitemap.xml + robots.txt), then validates JSON-LD in the built HTML. Both must pass for the build to succeed.

    **IMPORTANT:** This validation script will initially fail if run before Plan 01 and Plan 02 complete (SoftwareApplication won't be in cofounder/index.html yet). That is expected — this plan runs in Wave 1, and the validation will pass once Plan 02 (Wave 2) is also complete. The script itself is correct; it validates the final state.

    To handle the wave ordering, make the validation script tolerant of missing cofounder page (only error if the file exists but schemas are wrong):

    Modify the validation loop to skip pages where the HTML file does not exist (the cofounder/index.html may not have SoftwareApplication until Plan 02 runs). Use a warning instead of an error for missing expected types when running as part of the first build.

    Actually — simpler approach: since `npm run build` is the final verification command, and Plan 02 (Wave 2) runs after Plan 01 + Plan 03 (Wave 1), the validation will run during Plan 02's verification build and will pass at that point. The script does not need to be tolerant — it just needs to be correct for the final state.
  </action>
  <verify>
    - `ls marketing/scripts/validate-jsonld.mjs` — file exists
    - `grep 'postbuild' marketing/package.json` — shows next-sitemap and validate-jsonld
    - After full build (once Plan 01 + 02 are complete):
      `cd /Users/vladcortex/co-founder/marketing && npm run build` — postbuild runs both next-sitemap and validation without errors
    - `node marketing/scripts/validate-jsonld.mjs` (from marketing dir) — exits 0 with "validation passed" message
  </verify>
  <done>
    JSON-LD validation script validates Organization, WebSite, and SoftwareApplication schemas against Google Rich Results required fields. Postbuild script chains next-sitemap and validation. Both pass on clean build.
  </done>
</task>

</tasks>

<verification>
1. `cd /Users/vladcortex/co-founder/marketing && npm run build` — full build with postbuild passes
2. `ls out/sitemap.xml out/robots.txt` — both exist
3. `cat out/sitemap.xml | grep -c 'getinsourced.ai'` — returns 8 (all pages)
4. `cat out/robots.txt` — shows Allow: / and Sitemap reference
5. Build logs show "JSON-LD validation passed" message
</verification>

<success_criteria>
- sitemap.xml generated in out/ with all 8 page URLs (SEO-06)
- robots.txt generated in out/ with Allow: / and Sitemap reference (SEO-07)
- JSON-LD validation script runs at build time and catches missing required fields
- postbuild script chains next-sitemap and validation
- All trailing slashes in sitemap URLs
</success_criteria>

<output>
After completion, create `.planning/phases/24-seo-infrastructure/24-03-SUMMARY.md`
</output>
