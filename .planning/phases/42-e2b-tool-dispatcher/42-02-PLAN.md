---
phase: 42-e2b-tool-dispatcher
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - backend/app/agent/sync/__init__.py
  - backend/app/agent/sync/s3_snapshot.py
  - backend/tests/agent/test_s3_snapshot.py
autonomous: true
requirements: [MIGR-04]

must_haves:
  truths:
    - "After an agent phase commit, a tar.gz snapshot of project source files is uploaded to S3"
    - "node_modules/, .next/, dist/, build/, .git/, __pycache__/, .venv/ are excluded from the snapshot"
    - "S3 key follows format projects/{project_id}/snapshots/{ISO-timestamp}.tar.gz with pure numeric timestamp"
    - "Rolling retention keeps only the last 5 snapshots per project — older ones are deleted"
    - "Snapshot sync retries 3 times on failure, then returns None and continues (non-fatal)"
    - "Sandbox TTL is proactively extended when remaining time drops below 5 minutes"
  artifacts:
    - path: "backend/app/agent/sync/s3_snapshot.py"
      provides: "S3SnapshotService with sync(), restore(), and TTL management"
      exports: ["S3SnapshotService"]
    - path: "backend/app/agent/sync/__init__.py"
      provides: "Package marker"
    - path: "backend/tests/agent/test_s3_snapshot.py"
      provides: "Unit tests for sync, retry, retention, key format, TTL check"
      min_lines: 150
  key_links:
    - from: "backend/app/agent/sync/s3_snapshot.py"
      to: "backend/app/sandbox/e2b_runtime.py"
      via: "runtime.run_command() for tar creation, runtime._sandbox.files.read() for tar bytes"
      pattern: "runtime\\.run_command.*tar"
    - from: "backend/app/agent/sync/s3_snapshot.py"
      to: "boto3 S3 client"
      via: "asyncio.to_thread(s3.put_object()), list_objects_v2, delete_objects"
      pattern: "asyncio\\.to_thread.*put_object"
---

<objective>
TDD implementation of S3SnapshotService — syncs E2B sandbox project files to S3 as rolling tar.gz snapshots after each agent phase commit, and provides proactive sandbox TTL management to prevent unexpected expiry.

Purpose: Mitigates E2B Issue #884 (file loss on multi-resume). Without this, a sandbox crash or TTL expiry during a long build loses all agent progress. With snapshots, a new sandbox can be restored from the latest S3 snapshot in Phase 43's wake lifecycle.
Output: S3SnapshotService class with sync/restore/TTL methods, and comprehensive tests.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/42-e2b-tool-dispatcher/42-RESEARCH.md
@backend/app/sandbox/e2b_runtime.py
@backend/app/services/screenshot_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED — Write failing tests for S3SnapshotService</name>
  <files>
    backend/tests/agent/test_s3_snapshot.py
  </files>
  <action>
Create `backend/tests/agent/test_s3_snapshot.py` with the following test cases. All tests mock E2BSandboxRuntime and boto3 — no real E2B or S3 calls.

**Test structure:**
- Use `pytest.mark.unit` marker
- Use `pytest.mark.asyncio` for async tests
- Create a `mock_runtime` fixture returning MagicMock with async run_command (returns dict with stdout/stderr/exit_code) and `_sandbox` attribute with `files.read` as AsyncMock returning bytes, and `get_info`/`set_timeout` as AsyncMocks
- Patch `boto3.client` to return a MagicMock S3 client

**Test cases (8 minimum):**

1. `test_sync_uploads_tar_gz` — `sync(runtime, project_id="proj-1")` runs tar command in sandbox, reads tar bytes via `runtime._sandbox.files.read("/tmp/snap.tar.gz", format="bytes")`, uploads to S3 via `put_object`. Returns an S3 key string (not None).

2. `test_s3_key_format` — The S3 key returned by `sync()` matches pattern `projects/proj-1/snapshots/YYYYMMDDTHHMMSSZ.tar.gz` (pure numeric, no hyphens/colons). Verify using regex.

3. `test_tar_command_excludes_artifacts` — The tar command passed to `run_command` includes `--exclude=node_modules --exclude=.next --exclude=dist --exclude=build --exclude=.git --exclude=__pycache__ --exclude=.venv`. Verify by inspecting the command string argument.

4. `test_sync_retries_3x` — If S3 `put_object` raises Exception on first 2 calls and succeeds on 3rd, `sync()` returns a valid S3 key (not None). Verify put_object called 3 times.

5. `test_sync_returns_none_after_3_failures` — If S3 `put_object` raises Exception on all 3 calls, `sync()` returns None (non-fatal). No exception raised.

6. `test_prune_keeps_last_5` — After uploading, `_prune_old_snapshots()` calls `list_objects_v2` and `delete_objects` to remove snapshots beyond the 5-snapshot retention limit. Mock `list_objects_v2` to return 7 objects. Verify `delete_objects` called with the 2 oldest keys.

7. `test_prune_no_delete_when_under_limit` — If `list_objects_v2` returns 3 objects, `delete_objects` is NOT called.

8. `test_maybe_extend_ttl_extends_when_low` — `maybe_extend_ttl(runtime)` checks `runtime._sandbox.get_info()` which returns `end_at` = now + 3 minutes. Calls `runtime._sandbox.set_timeout(3600)`. Verify set_timeout called.

9. `test_maybe_extend_ttl_skips_when_healthy` — `maybe_extend_ttl(runtime)` with `end_at` = now + 30 minutes. set_timeout NOT called.

10. `test_sync_handles_tar_failure` — If tar command returns exit_code=1, sync retries. After 3 tar failures, returns None.

**Important:** Tests should FAIL initially because `s3_snapshot.py` does not exist yet.

Commit: `test(42-02): add failing tests for S3SnapshotService (8+ cases)`
  </action>
  <verify>
    <automated>cd /Users/vladcortex/co-founder/backend && python -m pytest tests/agent/test_s3_snapshot.py -x -q 2>&1 | tail -5</automated>
    <manual>All tests should FAIL (import error or assertion errors) — this is the RED phase</manual>
    <sampling_rate>run after this task commits, before next task begins</sampling_rate>
  </verify>
  <done>Test file exists with 8+ test cases. All tests fail because S3SnapshotService does not exist yet.</done>
</task>

<task type="auto">
  <name>Task 2: GREEN — Implement S3SnapshotService with sync, retention, and TTL</name>
  <files>
    backend/app/agent/sync/__init__.py
    backend/app/agent/sync/s3_snapshot.py
  </files>
  <action>
**Step 1: Create package marker** `backend/app/agent/sync/__init__.py` — empty file.

**Step 2: Create `backend/app/agent/sync/s3_snapshot.py`:**

```python
"""S3 snapshot service — syncs E2B sandbox project files to S3.

Tar-in-sandbox strategy: runs `tar czf` inside the sandbox to create a
compressed archive excluding build artifacts, reads the bytes via the E2B
files API, and uploads as a single S3 PutObject. Rolling retention keeps
the last 5 snapshots per project.

Phase 42: Mitigates E2B Issue #884 (file loss on multi-resume).
"""
```

**Module-level constants:**
- `SNAPSHOT_RETENTION = 5` — keep last 5 snapshots per project
- `EXCLUDE_DIRS = ["node_modules", ".next", "dist", "build", ".git", "__pycache__", ".venv"]` — per CONTEXT.md locked decision
- `TTL_EXTEND_THRESHOLD = 300` — extend TTL when < 5 minutes remaining (seconds)
- `TTL_EXTEND_SECONDS = 3600` — extend by 1 hour

**S3SnapshotService class:**

Constructor: `__init__(self, bucket: str, region: str = "us-east-1")` — stores bucket name and region.

`async def sync(self, runtime, project_id: str, project_path: str = "/home/user") -> str | None`:
1. Generate timestamp: `datetime.datetime.now(datetime.timezone.utc).strftime("%Y%m%dT%H%M%SZ")` (pure numeric, no hyphens/colons — per Pitfall 5 in RESEARCH.md)
2. Build S3 key: `projects/{project_id}/snapshots/{timestamp}.tar.gz`
3. Build tar command with excludes: `tar czf /tmp/snap.tar.gz {excludes} -C {project_path} . 2>&1`
4. Retry loop (3 attempts):
   a. Run tar via `runtime.run_command(tar_cmd, timeout=120)`
   b. Check exit_code — if non-zero, log warning, continue to next attempt
   c. Read tar bytes: `await runtime._sandbox.files.read("/tmp/snap.tar.gz", format="bytes")`
   d. Upload to S3: `await asyncio.to_thread(self._put_s3, s3_key, tar_bytes)` — use asyncio.to_thread per STATE.md locked pattern
   e. Prune old snapshots: `await asyncio.to_thread(self._prune_old_snapshots, project_id)`
   f. Log success, return s3_key
5. On exception at any step: log warning, continue loop
6. After 3 failures: log error, return None (non-fatal per CONTEXT.md)

`def _put_s3(self, key: str, body: bytes) -> None`:
- Create `boto3.client("s3", region_name=self._region)`
- Call `s3.put_object(Bucket=self._bucket, Key=key, Body=body, ContentType="application/gzip")`

`def _prune_old_snapshots(self, project_id: str) -> None`:
- List objects with prefix `projects/{project_id}/snapshots/`
- Sort by key descending (newest first — ISO timestamps sort lexicographically)
- Delete objects beyond SNAPSHOT_RETENTION limit
- Handle empty list gracefully (no delete call)

`async def maybe_extend_ttl(self, runtime) -> None`:
- Static method or standalone — called by E2BToolDispatcher before each dispatch
- Read `info = await runtime._sandbox.get_info()`
- Compare `info.end_at` to `datetime.datetime.now(datetime.timezone.utc)` (MUST use timezone-aware — per Pitfall 6 in RESEARCH.md)
- If remaining < TTL_EXTEND_THRESHOLD seconds: `await runtime._sandbox.set_timeout(TTL_EXTEND_SECONDS)`
- Log the extension event
- Wrap everything in try/except — TTL check failure should never crash the agent

**Run all tests:** `cd backend && pytest tests/agent/test_s3_snapshot.py -x -q`

Commit: `feat(42-02): implement S3SnapshotService with tar sync, rolling retention, TTL management`
  </action>
  <verify>
    <automated>cd /Users/vladcortex/co-founder/backend && python -m pytest tests/agent/test_s3_snapshot.py -x -q</automated>
    <manual>All S3SnapshotService tests pass (GREEN). No real S3 or E2B calls made.</manual>
    <sampling_rate>run after this task commits, before next task begins</sampling_rate>
  </verify>
  <done>S3SnapshotService syncs tar.gz to S3 with rolling 5-snapshot retention, retries 3x on failure, returns None on total failure (non-fatal). TTL management extends sandbox when < 5 minutes remaining. All tests green.</done>
</task>

</tasks>

<verification>
1. `cd backend && pytest tests/agent/test_s3_snapshot.py -x -q` — All 8+ S3SnapshotService tests pass
2. `python -c "from app.agent.sync.s3_snapshot import S3SnapshotService; print('import OK')"` from backend dir
3. Verify no real boto3 or E2B calls in tests (all mocked)
4. Verify S3 key format uses pure numeric timestamps (regex check in test)
</verification>

<success_criteria>
- S3SnapshotService class exists with sync(), _prune_old_snapshots(), maybe_extend_ttl() methods
- tar command excludes node_modules, .next, dist, build, .git, __pycache__, .venv
- S3 key format: projects/{project_id}/snapshots/{YYYYMMDDTHHMMSSZ}.tar.gz
- Rolling retention keeps last 5 snapshots, deletes older
- Sync retries 3x then returns None (non-fatal)
- TTL extension triggers when < 5 minutes remaining
- All tests pass with mocked dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/42-e2b-tool-dispatcher/42-02-SUMMARY.md`
</output>
