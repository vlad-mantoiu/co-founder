---
phase: 16-cloudwatch-observability
plan: 03
type: execute
wave: 2
depends_on: ["16-01"]
files_modified:
  - backend/app/metrics/__init__.py
  - backend/app/metrics/cloudwatch.py
  - backend/app/agent/runner_real.py
  - backend/app/api/routes/billing.py
  - backend/app/services/generation_service.py
autonomous: true
requirements:
  - MON-08
  - MON-09

must_haves:
  truths:
    - "Each RunnerReal LLM method emits a CloudWatch metric with method name, duration, and model as dimensions"
    - "New subscription events emit a CoFounder/Business metric with event=new_subscription"
    - "Subscription cancellation events emit a CoFounder/Business metric with event=subscription_cancelled"
    - "Artifact generation completion emits a CoFounder/Business metric with event=artifact_generated"
    - "Metric emission is fire-and-forget and never blocks the async event loop or causes request failures"
  artifacts:
    - path: "backend/app/metrics/__init__.py"
      provides: "Package init for metrics module"
      contains: ""
    - path: "backend/app/metrics/cloudwatch.py"
      provides: "emit_llm_latency and emit_business_event functions using boto3 put_metric_data"
      contains: "emit_llm_latency"
    - path: "backend/app/agent/runner_real.py"
      provides: "LLM call timing with emit_llm_latency after each _invoke_with_retry"
      contains: "emit_llm_latency"
    - path: "backend/app/api/routes/billing.py"
      provides: "Business event emission on checkout.session.completed and subscription.deleted"
      contains: "emit_business_event"
    - path: "backend/app/services/generation_service.py"
      provides: "Business event emission on artifact generation pipeline completion"
      contains: "emit_business_event"
  key_links:
    - from: "backend/app/metrics/cloudwatch.py"
      to: "CloudWatch CoFounder/LLM namespace"
      via: "boto3 put_metric_data with Method and Model dimensions"
      pattern: "CoFounder/LLM"
    - from: "backend/app/metrics/cloudwatch.py"
      to: "CloudWatch CoFounder/Business namespace"
      via: "boto3 put_metric_data with Event dimension"
      pattern: "CoFounder/Business"
    - from: "backend/app/agent/runner_real.py"
      to: "backend/app/metrics/cloudwatch.py"
      via: "emit_llm_latency called after each _invoke_with_retry"
      pattern: "emit_llm_latency"
    - from: "backend/app/api/routes/billing.py"
      to: "backend/app/metrics/cloudwatch.py"
      via: "emit_business_event called in webhook handlers"
      pattern: "emit_business_event"
---

<objective>
Add custom CloudWatch metrics for LLM call latency (per RunnerReal method) and business events (subscriptions, artifact generation) using boto3 put_metric_data.

Purpose: LLM latency metrics surface slowdowns in Claude API calls before they cascade into user-facing timeouts. Business metrics track subscription and artifact events for operational visibility. Both are fire-and-forget to never impact request latency.

Output: `backend/app/metrics/cloudwatch.py` with emit functions, timing instrumentation in runner_real.py, business event calls in billing.py and generation_service.py.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-cloudwatch-observability/16-RESEARCH.md
@.planning/phases/16-cloudwatch-observability/16-01-SUMMARY.md
@backend/app/agent/runner_real.py
@backend/app/api/routes/billing.py
@backend/app/services/generation_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CloudWatch metrics module with LLM latency and business event emitters</name>
  <files>backend/app/metrics/__init__.py, backend/app/metrics/cloudwatch.py</files>
  <action>
    **Step 1: Create `backend/app/metrics/__init__.py`**

    Empty file (package init).

    **Step 2: Create `backend/app/metrics/cloudwatch.py`**

    Follow 16-RESEARCH.md Pattern 3 with these adjustments:

    ```python
    """CloudWatch custom metric emission for LLM latency and business events.

    All functions are fire-and-forget: they catch exceptions internally and log
    warnings via structlog. They NEVER raise or block the caller.

    Metrics are emitted via boto3 put_metric_data. Since boto3 is synchronous,
    calls are dispatched to a ThreadPoolExecutor to avoid blocking the async event loop.
    """

    import asyncio
    import time
    from concurrent.futures import ThreadPoolExecutor
    from datetime import datetime, timezone

    import boto3
    import structlog

    logger = structlog.get_logger(__name__)

    _cw_client = None
    _executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="cw-metrics")

    def _get_client():
        global _cw_client
        if _cw_client is None:
            _cw_client = boto3.client("cloudwatch", region_name="us-east-1")
        return _cw_client

    def _put_llm_latency(method_name: str, duration_ms: float, model: str) -> None:
        """Synchronous put_metric_data for LLM latency. Runs in thread pool."""
        try:
            _get_client().put_metric_data(
                Namespace="CoFounder/LLM",
                MetricData=[{
                    "MetricName": "Latency",
                    "Dimensions": [
                        {"Name": "Method", "Value": method_name},
                        {"Name": "Model", "Value": model},
                    ],
                    "Value": duration_ms,
                    "Unit": "Milliseconds",
                    "Timestamp": datetime.now(timezone.utc),
                }],
            )
        except Exception as e:
            logger.warning("llm_latency_emit_failed", error=str(e), method=method_name)

    def _put_business_event(event_name: str, user_id: str | None = None) -> None:
        """Synchronous put_metric_data for business events. Runs in thread pool."""
        dimensions = [{"Name": "Event", "Value": event_name}]
        if user_id:
            dimensions.append({"Name": "UserId", "Value": user_id})
        try:
            _get_client().put_metric_data(
                Namespace="CoFounder/Business",
                MetricData=[{
                    "MetricName": "EventCount",
                    "Dimensions": dimensions,
                    "Value": 1.0,
                    "Unit": "Count",
                    "Timestamp": datetime.now(timezone.utc),
                }],
            )
        except Exception as e:
            logger.warning("business_event_emit_failed", error=str(e), event=event_name)

    async def emit_llm_latency(method_name: str, duration_ms: float, model: str) -> None:
        """Emit LLM call latency metric. Non-blocking, fire-and-forget."""
        loop = asyncio.get_event_loop()
        loop.run_in_executor(_executor, _put_llm_latency, method_name, duration_ms, model)

    async def emit_business_event(event_name: str, user_id: str | None = None) -> None:
        """Emit business event metric. Non-blocking, fire-and-forget."""
        loop = asyncio.get_event_loop()
        loop.run_in_executor(_executor, _put_business_event, event_name, user_id)
    ```

    **Key design decisions:**
    - `_executor` = ThreadPoolExecutor(max_workers=2) — boto3 calls run off the event loop
    - `run_in_executor` returns a Future but we don't await it — true fire-and-forget
    - All exceptions caught internally — metric emission NEVER fails the request
    - Lazy boto3 client initialization (first call creates it)
    - Namespace `CoFounder/LLM` for latency, `CoFounder/Business` for events (per research)
  </action>
  <verify>
    Run: `cd /Users/vladcortex/co-founder/backend && python -c "from app.metrics.cloudwatch import emit_llm_latency, emit_business_event; print('metrics module OK')"` — should import without errors.
    Run: `grep 'CoFounder/LLM' backend/app/metrics/cloudwatch.py` — should find namespace.
    Run: `grep 'CoFounder/Business' backend/app/metrics/cloudwatch.py` — should find namespace.
    Run: `grep 'ThreadPoolExecutor' backend/app/metrics/cloudwatch.py` — should find executor.
  </verify>
  <done>Metrics module created with async fire-and-forget emit functions. LLM latency tracks Method + Model dimensions. Business events track Event + optional UserId. boto3 calls dispatched to thread pool.</done>
</task>

<task type="auto">
  <name>Task 2: Instrument RunnerReal methods with LLM latency timing and add business event emission</name>
  <files>backend/app/agent/runner_real.py, backend/app/api/routes/billing.py, backend/app/services/generation_service.py</files>
  <action>
    **Part A: Instrument RunnerReal (MON-08)**

    In `backend/app/agent/runner_real.py`, add LLM latency timing to EVERY method that calls `_invoke_with_retry`. There are 9 such methods:

    1. `generate_questions` (line ~201)
    2. `generate_brief` (line ~264)
    3. `generate_understanding_questions` (line ~329)
    4. `generate_idea_brief` (line ~424)
    5. `check_question_relevance` (line ~499)
    6. `assess_section_confidence` (line ~543)
    7. `generate_execution_options` (line ~616)
    8. `generate_artifacts` (line ~717)

    For EACH method, apply this pattern around the `_invoke_with_retry` call:

    ```python
    import time
    from app.metrics.cloudwatch import emit_llm_latency

    # Before the _invoke_with_retry call:
    t0 = time.perf_counter()

    response = await _invoke_with_retry(llm, [system_msg, human_msg])

    # After the _invoke_with_retry call (same indentation level):
    await emit_llm_latency(
        method_name="generate_questions",  # Use the actual method name
        duration_ms=(time.perf_counter() - t0) * 1000,
        model=model,  # The model variable from create_tracked_llm
    )
    ```

    **Important:** Some methods have a retry pattern where `_invoke_with_retry` is called twice (first attempt + strict retry on JSONDecodeError). Place the timing around BOTH calls as a unit — start `t0` before the first attempt, emit after the final successful call. This measures total time including retries, which is what matters operationally.

    For methods with the try/except JSONDecodeError retry pattern:
    ```python
    t0 = time.perf_counter()
    try:
        response = await _invoke_with_retry(llm, [system_msg, human_msg])
    except json.JSONDecodeError:
        response = await _invoke_with_retry(llm, [strict_system, human_msg])
    await emit_llm_latency(
        method_name="the_method_name",
        duration_ms=(time.perf_counter() - t0) * 1000,
        model=model,
    )
    ```

    The `model` variable is available in each method because `create_tracked_llm` resolves it. Check each method for how the model string is available — it may be a local variable like `model` or derived from the llm config.

    **Part B: Business event emission (MON-09)**

    **In `backend/app/api/routes/billing.py`:**

    1. Add import: `from app.metrics.cloudwatch import emit_business_event`

    2. In `_handle_checkout_completed` (line ~341), AFTER the successful subscription activation (after the user_settings update completes, before the return):
       ```python
       await emit_business_event("new_subscription", user_id=clerk_user_id)
       ```

    3. In `_handle_subscription_deleted` (line ~408), AFTER the subscription deactivation logic:
       ```python
       await emit_business_event("subscription_cancelled", user_id=clerk_user_id)
       ```
       Extract `clerk_user_id` from the subscription metadata (same pattern as checkout handler).

    **In `backend/app/services/generation_service.py`:**

    1. Add import: `from app.metrics.cloudwatch import emit_business_event`

    2. After the build pipeline completes successfully (after the job transitions to a completed/done state, NOT in the finally block), emit:
       ```python
       await emit_business_event("artifact_generated", user_id=user_id)
       ```
       The `user_id` should be extracted from the job or project context — check what's available in the `run_pipeline` / build method scope.

    **Note:** Only emit `artifact_generated` on SUCCESSFUL completion, NOT on failure. The locked decision specifies 3 events: `new_subscription`, `subscription_cancelled`, `artifact_generated`.
  </action>
  <verify>
    Run: `cd /Users/vladcortex/co-founder/backend && grep -c 'emit_llm_latency' app/agent/runner_real.py` — should be >= 8 (one per LLM method).
    Run: `cd /Users/vladcortex/co-founder/backend && grep -c 'emit_business_event' app/api/routes/billing.py` — should be >= 2 (new_subscription + subscription_cancelled).
    Run: `cd /Users/vladcortex/co-founder/backend && grep 'emit_business_event' app/services/generation_service.py` — should find artifact_generated emission.
    Run: `cd /Users/vladcortex/co-founder/backend && python -c "from app.agent.runner_real import RunnerReal; print('runner imports OK')"` — should succeed.
    Run: `cd /Users/vladcortex/co-founder/backend && python -c "from app.api.routes.billing import router; print('billing imports OK')"` — should succeed.
  </verify>
  <done>All RunnerReal LLM methods emit latency metrics with method name, duration, and model. Billing webhooks emit new_subscription and subscription_cancelled events. Generation service emits artifact_generated on successful pipeline completion. All emissions are fire-and-forget and non-blocking.</done>
</task>

</tasks>

<verification>
1. `grep -c 'emit_llm_latency' backend/app/agent/runner_real.py` — returns >= 8
2. `grep -c 'emit_business_event' backend/app/api/routes/billing.py` — returns >= 2
3. `grep 'emit_business_event' backend/app/services/generation_service.py` — finds artifact_generated
4. `grep 'ThreadPoolExecutor' backend/app/metrics/cloudwatch.py` — confirms async emission
5. `python -c "from app.main import app"` — no import errors
6. `grep 'CoFounder/LLM' backend/app/metrics/cloudwatch.py` — namespace defined
7. `grep 'CoFounder/Business' backend/app/metrics/cloudwatch.py` — namespace defined
</verification>

<success_criteria>
- Every RunnerReal method that calls _invoke_with_retry emits a latency metric with method name and model
- Timing wraps the entire LLM call including JSON retries (measures total wall-clock time)
- billing.py emits new_subscription on checkout.session.completed
- billing.py emits subscription_cancelled on customer.subscription.deleted
- generation_service.py emits artifact_generated on successful build completion only
- All metric emission uses ThreadPoolExecutor to avoid blocking async event loop
- No metric emission failure can cause a request to fail (fire-and-forget with exception catch)
</success_criteria>

<output>
After completion, create `.planning/phases/16-cloudwatch-observability/16-03-SUMMARY.md`
</output>
