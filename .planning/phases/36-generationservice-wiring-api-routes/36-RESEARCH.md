# Phase 36: GenerationService Wiring & API Routes - Research

**Researched:** 2026-02-24
**Domain:** FastAPI SSE streaming, Claude Haiku narration service, build pipeline wiring, Redis Pub/Sub
**Confidence:** HIGH

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

**Narration voice & personality**
- Calm expert personality — confident, measured, like a senior engineer calmly explaining progress
- Collaborative "we" pronoun — "We're setting up your project structure now." Not "I" or passive
- One sentence per narration, 10-20 words. Scannable, not essays
- Project-specific references — mention actual features/pages from the spec. "We're creating your dashboard and team management pages."
- Claude-generated fresh on each stage transition (not templates)
- Always confident tone — never acknowledge delays, never apologize
- No narration on build completion (UI state change handles that)
- No narration on build failure (UI error state handles that, avoids awkward AI apologizing)

**Stage-to-narration mapping**
- All 5 stages get narration: SCAFFOLD, CODE, DEPS, CHECKS, READY
- Stage-to-agent-role mapping: SCAFFOLD → Architect, CODE → Coder, DEPS → Coder, CHECKS → Reviewer, READY → Reviewer
- Agent role label included alongside narration sentence (e.g., "Architect: Setting up your project layout with the pages you described.")
- Rough time estimates included per stage (from historical averages or sensible defaults)
- Haiku model for narration generation (same as doc generation)
- Lightweight spec summary as input — project name + key features list, not full spec
- One narration per stage transition, generated on-demand (not pre-generated upfront)
- Non-fatal: if narration generation fails, emit event with a generic fallback sentence. Build never blocked
- Same safety guardrails as doc generation — strip internal paths, stack traces, secrets, framework names

**Changelog content**
- Spec diff comparison — compare v0.1 spec to v0.2 spec, highlight what founder asked to change/add
- Added/Changed/Removed list format (classic changelog style)
- Stored as fifth Redis hash key: `changelog` in `job:{id}:docs`
- Omitted entirely for v0.1 (first build) — no `changelog` key written. Frontend checks key existence
- Generated by Haiku (same pattern as doc generation) with both specs as input
- Version-labeled: "v0.2 Changes" header followed by the list
- Same founder-safe content rules as docs — dual-layer safety filter applies to changelog too

**SSE event payloads**
- Rich payloads — each event carries everything frontend needs to render, no follow-up API calls needed
- `build.stage.started`: includes narration text, agent role, stage name, time estimate
- `build.stage.completed`: includes stage name, actual duration
- `snapshot.updated`: includes CloudFront URL for the latest screenshot
- `documentation.updated`: includes section key that was just written (e.g., "overview")
- 15-second heartbeat keepalive to stay within ALB idle timeout (60s default)
- Bootstrap from REST then stream — frontend calls GET /api/jobs/{id}/docs and GET /api/generation/{id}/status first, then opens SSE. No replay/Last-Event-ID mechanism in SSE itself
- Authenticated via Clerk JWT — same auth as all other endpoints

### Claude's Discretion
- Exact generic fallback narration sentences when generation fails
- Specific time estimate values per stage (reasonable defaults)
- Narration prompt structure and system prompt content

### Deferred Ideas (OUT OF SCOPE)
None — discussion stayed within phase scope
</user_constraints>

---

<phase_requirements>
## Phase Requirements

| ID | Description | Research Support |
|----|-------------|-----------------|
| NARR-02 | Claude generates idea-specific narration per stage transition | NarrationService using claude-3-5-haiku-20241022, same pattern as DocGenerationService; called via asyncio.create_task() in execute_build() at each stage transition |
| NARR-04 | Narration uses first-person co-founder voice | System prompt enforces "we" pronoun, calm expert tone; agent role label (Architect/Coder/Reviewer) added to event payload |
| NARR-08 | Safety guardrails strip internal paths, stack traces, and secrets from narration | Reuse _SAFETY_PATTERNS from doc_generation_service.py or mirror identical patterns; apply before emit |
| SNAP-03 | SSE snapshot.updated event emitted when new screenshot is available | ScreenshotService._upload_and_persist() already emits SSEEventType.SNAPSHOT_UPDATED via publish_event() — wiring means calling screenshot_service.capture() in execute_build() after CHECKS and READY stage transitions |
| DOCS-09 | Changelog generated comparing build iterations | ChangelogService (or method on DocGenerationService); reads previous spec from job data, calls Haiku, writes `changelog` key to job:{id}:docs hash; skipped for build_v0_1 |
</phase_requirements>

---

## Summary

Phase 36 is primarily a **wiring and new-service** phase. The infrastructure is already in place: `ScreenshotService` and `DocGenerationService` exist with full implementations and SSE emission logic. `JobStateMachine.publish_event()` is the correct publication mechanism. `SSEEventType` constants are defined. The Redis Pub/Sub channel `job:{id}:events` is live.

What does NOT exist yet: (1) a `NarrationService` that generates Claude-powered narration per stage, (2) wiring of `ScreenshotService.capture()` into `execute_build()` and `execute_iteration_build()` at the post-CHECKS and post-READY insertion points, (3) wiring of changelog generation into iteration builds (v0.2+), and (4) the new `GET /api/jobs/{id}/events/stream` SSE endpoint with heartbeat keepalive and typed-event streaming.

The existing `/api/jobs/{job_id}/stream` endpoint (in `jobs.py`) streams raw state-machine events but lacks: the typed-event envelope with rich payloads, heartbeat keepalive at 15s intervals, and snapshot/documentation event pass-through. A new endpoint `GET /api/jobs/{id}/events/stream` must be added to `generation.py` following the existing SSE pattern from `logs.py`.

**Primary recommendation:** Build `NarrationService` as a standalone module mirroring `DocGenerationService` architecture, wire all three services (NarrationService, ScreenshotService, DocGenerationService changelog) into `GenerationService.execute_build()` and `execute_iteration_build()`, then add the typed SSE stream endpoint to `generation.py`.

---

## Standard Stack

### Core

| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| `anthropic` (AsyncAnthropic) | `>=0.40.0` (already in pyproject.toml) | NarrationService Claude Haiku calls | Same pattern as DocGenerationService — direct client, no LangChain wrapper |
| `fastapi.responses.StreamingResponse` | FastAPI `>=0.115` (already used) | SSE endpoint response type | Project already uses this in `logs.py` and `jobs.py` — established pattern |
| `fakeredis[aioredis]` | `>=2.26.0` (already in dev deps) | Test doubles for Redis Pub/Sub in narration/SSE tests | Already used across all service tests |
| `pytest-asyncio` | `>=0.24.0` (already installed) | `asyncio_mode = "auto"` in pyproject.toml | All async tests use this |

### Supporting

| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| `structlog` | `>=25.0.0` | Structured logging in NarrationService | Consistent with all other services |
| `asyncio.create_task()` | stdlib | Fire-and-forget narration generation per stage | Same pattern as DocGenerationService wiring |
| `asyncio.wait_for()` | stdlib | Timeout wrapper for Claude API call | Same pattern as DocGenerationService — 30s timeout |

### Alternatives Considered

| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Separate NarrationService module | Method on GenerationService | Separation of concerns — NarrationService is independently testable |
| `asyncio.create_task()` per stage | `asyncio.gather()` upfront for all stages | On-demand generation avoids pre-generating stale narration |
| `GET /api/jobs/{id}/events/stream` in generation.py | Separate file or extending jobs.py | Co-location with other generation endpoints is cleaner; jobs.py already has its own stream |

---

## Architecture Patterns

### Recommended Project Structure

```
backend/app/services/
├── narration_service.py     # NEW — NarrationService, mirrors DocGenerationService structure
├── doc_generation_service.py  # EXISTS — add _generate_changelog() method
├── screenshot_service.py    # EXISTS — no changes needed
└── generation_service.py    # MODIFY — wire in NarrationService + ScreenshotService

backend/app/api/routes/
└── generation.py            # MODIFY — add GET /{job_id}/events/stream endpoint

backend/tests/services/
├── test_narration_service.py    # NEW
├── test_narration_wiring.py     # NEW
└── test_changelog_wiring.py     # NEW (or extend test_doc_generation_wiring.py)

backend/tests/api/
└── test_generation_routes.py    # MODIFY — add SSE stream endpoint tests
```

### Pattern 1: NarrationService Architecture (mirror of DocGenerationService)

**What:** Stateless service that generates one Claude Haiku sentence per stage transition. Called via `asyncio.create_task()` per stage, never raises, always emits an SSE event (real narration or fallback).

**When to use:** At each of the 5 stage transitions in `execute_build()` and `execute_iteration_build()`, after the `state_machine.transition()` call.

**Example (mirrors DocGenerationService pattern):**
```python
# In narration_service.py
NARRATION_MODEL: str = "claude-3-5-haiku-20241022"
NARRATION_MAX_TOKENS: int = 80  # One sentence, 10-20 words
NARRATION_TIMEOUT_SECONDS: float = 10.0  # Short — single sentence

STAGE_AGENT_ROLES: dict[str, str] = {
    "scaffold": "Architect",
    "code": "Coder",
    "deps": "Coder",
    "checks": "Reviewer",
    "ready": "Reviewer",
}

STAGE_TIME_ESTIMATES: dict[str, str] = {
    "scaffold": "~15s",
    "code": "~60s",
    "deps": "~45s",
    "checks": "~20s",
    "ready": "~5s",
}

_FALLBACK_NARRATIONS: dict[str, str] = {
    "scaffold": "We're setting up your project structure.",
    "code": "We're writing the code for your application.",
    "deps": "We're installing the dependencies.",
    "checks": "We're running the final checks.",
    "ready": "Your build is ready.",
}


class NarrationService:
    async def narrate(
        self,
        job_id: str,
        stage: str,
        spec: str,
        redis: object,
    ) -> None:
        """Generate stage narration and emit build.stage.started SSE event.

        Never raises. Falls back to generic sentence on any failure.
        Always emits the SSE event even when Claude call fails.
        """
        ...
```

**Wiring in `execute_build()` (after each `state_machine.transition()`):**
```python
# Module-level singleton (same pattern as _doc_generation_service)
_narration_service = NarrationService()

# In execute_build(), after transition to SCAFFOLD:
await state_machine.transition(job_id, JobStatus.SCAFFOLD, "Scaffolding project state")
if _settings.narration_enabled and _redis is not None:
    asyncio.create_task(
        _narration_service.narrate(job_id=job_id, stage="scaffold", spec=job_data.get("goal", ""), redis=_redis)
    )
```

**NOTE:** The state_machine `transition()` already publishes a `build.stage.started` event with a basic payload. The NarrationService must either (a) emit a SEPARATE follow-up event with narration data, or (b) the transition event must be enriched. Given the locked decision that `build.stage.started` includes narration text, agent role, stage name, and time estimate, **the NarrationService must publish its own enriched `build.stage.started` event** via `state_machine.publish_event()` AFTER the plain transition event. The frontend should use the last `build.stage.started` event received for a stage — the narration-enriched one overrides the plain one.

**Alternative approach:** Add a `narration_enabled` feature flag parallel to `docs_generation_enabled` — or simply re-use the fact that Redis is available as the gate condition.

### Pattern 2: ScreenshotService Wiring in execute_build()

**What:** After CHECKS and READY stage transitions, call `screenshot_service.capture()` as a fire-and-forget `asyncio.create_task()`.

**Current state:** `ScreenshotService` is fully implemented with `capture()`, `validate()`, `upload()`, and `_upload_and_persist()`. The `_upload_and_persist()` already writes `snapshot_url` to the Redis hash AND emits `SSEEventType.SNAPSHOT_UPDATED` via `publish_event()`. **SNAP-03 is already satisfied by the existing ScreenshotService implementation** — it just needs to be wired into `execute_build()`.

**Insertion points in `execute_build()`:**
1. After `await state_machine.transition(job_id, JobStatus.CHECKS, ...)` — capture at checks stage
2. After `preview_url = await sandbox.start_dev_server(...)` — capture at ready stage

```python
# Module-level singleton
_screenshot_service = ScreenshotService()

# In execute_build(), after CHECKS transition:
if _settings.screenshot_enabled and _redis is not None and preview_url_available:
    asyncio.create_task(
        _screenshot_service.capture(preview_url=preview_url, job_id=job_id, stage="checks", redis=_redis)
    )
```

**Critical note:** At CHECKS stage, `preview_url` is not yet available (dev server hasn't started). The CHECKS capture must happen AFTER `sandbox.start_dev_server()` returns — not immediately after the CHECKS transition. The worker calls `start_dev_server()` after CHECKS, so the insertion sequence is:
1. Transition → CHECKS
2. `sandbox.run_command("health-check-ok")`
3. `preview_url = await sandbox.start_dev_server(...)`
4. Fire-and-forget: `_screenshot_service.capture(stage="checks")`
5. Fire-and-forget: `_screenshot_service.capture(stage="ready")`  (same preview_url, different stage key)

Wait — reviewing `CAPTURE_STAGES = frozenset({"checks", "ready"})`. Both captures happen after dev server starts. The "checks" and "ready" captures are essentially the same screenshot at different stage names, giving two S3 keys (`screenshots/{job_id}/checks.png` and `screenshots/{job_id}/ready.png`). The second capture happens after the READY transition.

The `execute_build()` currently does: CHECKS transition → health check → start_dev_server → compute build version. Screenshots should be captured after dev server is live, before the function returns.

### Pattern 3: New SSE Stream Endpoint

**What:** `GET /api/jobs/{id}/events/stream` in `generation.py` — subscribes to `job:{id}:events` Redis Pub/Sub channel and streams typed events to the frontend.

**Based on:** The existing `jobs.py stream_job_status` endpoint pattern, enhanced with:
1. 15-second heartbeat (currently 20s in `logs.py` — use 15s as locked decision)
2. Pass-through of ALL event types (build.stage.started, build.stage.completed, snapshot.updated, documentation.updated)
3. Terminus on `status in ("ready", "failed")` detection from the event payload
4. Registered under `generation.py` router (not `jobs.py`) at path `/{job_id}/events/stream`

**NOTE: Path collision check.** The `generation.py` router is registered with prefix `/generation` → full path is `/api/generation/{job_id}/events/stream`. The CONTEXT.md says `GET /api/jobs/{id}/events/stream`. This is a naming decision — the CONTEXT.md references the URL in the context of the SUCCESS CRITERIA. The `generation.py` router prefix is `/generation`. Either: (a) add the endpoint to `generation.py` at `/{job_id}/events/stream` → `/api/generation/{job_id}/events/stream`, or (b) add it to `jobs.py` at `/{job_id}/events/stream` → `/api/jobs/{job_id}/events/stream`. Based on the success criteria phrasing "GET /api/jobs/{id}/events/stream" the jobs.py location is more accurate. **However**, the existing `jobs.py` stream endpoint is at `/{job_id}/stream` → `/api/jobs/{job_id}/stream`. The new endpoint would be at `/{job_id}/events/stream` — a different path — in the same `jobs.py` file. This is clean and avoids a router registration change.

**SSE stream generator pattern (from logs.py):**
```python
@router.get("/{job_id}/events/stream")
async def stream_job_events(
    job_id: str,
    request: Request,
    user: ClerkUser = Depends(require_auth),
    redis=Depends(get_redis),
):
    """Stream typed build events via SSE with heartbeat keepalive."""
    state_machine = JobStateMachine(redis)
    job_data = await state_machine.get_job(job_id)
    if not job_data or job_data.get("user_id") != user.user_id:
        raise HTTPException(status_code=404, detail="Job not found")

    async def event_generator():
        # Check if already terminal — emit final status and close
        current_status = job_data.get("status")
        if current_status in ("ready", "failed"):
            yield f"data: {json.dumps({'type': 'build.stage.started', 'status': current_status, 'job_id': job_id})}\n\n"
            return

        pubsub = redis.pubsub()
        channel = f"job:{job_id}:events"
        await pubsub.subscribe(channel)
        last_heartbeat = time.monotonic()

        try:
            while True:
                if await request.is_disconnected():
                    return

                # 15-second heartbeat (ALB keepalive)
                now = time.monotonic()
                if now - last_heartbeat >= 15:
                    yield "event: heartbeat\ndata: {}\n\n"
                    last_heartbeat = now

                # Non-blocking poll with timeout
                message = await asyncio.wait_for(
                    pubsub.get_message(ignore_subscribe_messages=True),
                    timeout=1.0
                )
                if message and message["type"] == "message":
                    yield f"data: {message['data']}\n\n"
                    data = json.loads(message["data"])
                    if data.get("status") in ("ready", "failed"):
                        return
        except asyncio.TimeoutError:
            pass  # Normal — heartbeat interval check will fire next loop
        finally:
            await pubsub.unsubscribe(channel)
            await pubsub.close()

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive", "X-Accel-Buffering": "no"},
    )
```

**Important: pubsub.listen() vs pubsub.get_message().** The existing `jobs.py` uses `async for message in pubsub.listen()` which blocks indefinitely and cannot interleave heartbeat checks. The `logs.py` uses `redis.xread(block=_POLL_BLOCK_MS)` with a short block. For the new event stream endpoint, use `pubsub.get_message(ignore_subscribe_messages=True)` with `asyncio.wait_for(timeout=1.0)` to enable heartbeat interleaving. This is the correct pattern for ALB keepalive compliance.

### Pattern 4: Changelog Generation (DOCS-09)

**What:** For v0.2+ builds (`build_version != "build_v0_1"`), after doc generation starts, compare the current spec to the previous job's goal string and generate a changelog section written to `job:{id}:docs` as `changelog` key.

**Where to implement:** A `_generate_changelog()` method on `DocGenerationService`, called from `execute_iteration_build()` in `GenerationService` via `asyncio.create_task()` (same fire-and-forget pattern as `generate()`).

**Input data:** The current job's `goal` (spec) and the previous job's `goal` (spec). The previous job's goal must be fetched — look up the most recent READY job for the same `project_id` from Postgres (`Job` model).

**Output:** Written to `job:{id}:docs` hash key `changelog`. A `documentation.updated` SSE event is emitted with `section: "changelog"`.

**Version detection:** Check `build_version` against `"build_v0_1"` — if it is, skip changelog. This is determined in `execute_build()` after `_get_next_build_version()` returns. But `execute_iteration_build()` is the correct entry point for changelogs since first builds are never iterations.

### Anti-Patterns to Avoid

- **Blocking narration in execute_build():** Never `await narration_service.narrate()` inline — use `asyncio.create_task()` exclusively. Narration latency is 500ms-3s; blocking would slow the build.
- **Publishing narration as a completely new SSE event type:** The locked decision says `build.stage.started` carries narration text. Emit a second `build.stage.started` event with enriched payload (narration text + agent role + time estimate) after the state machine's plain transition event. Frontend processes the last one received.
- **Using `pubsub.listen()` loop for the new event stream endpoint:** This blocks indefinitely with no way to interleave heartbeats. Use `pubsub.get_message()` with `asyncio.wait_for()` timeout instead.
- **Re-compiling safety regex patterns per narration:** Compile patterns at module level (already done in `doc_generation_service.py` as `_SAFETY_PATTERNS`). Import and reuse those patterns in `NarrationService` — don't duplicate.
- **Screenshot capture before dev server starts:** `CAPTURE_STAGES = frozenset({"checks", "ready"})` — but `preview_url` is only available after `start_dev_server()`. Both captures happen after `start_dev_server()` returns, using the same `preview_url`.

---

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| SSE heartbeat | Custom timer thread | `asyncio.wait_for(timeout=1.0)` loop with monotonic clock check | Already proven in logs.py; asyncio-native |
| Content safety filter | New regex patterns | Import `_SAFETY_PATTERNS` from `doc_generation_service.py` | Already compiled at module level; tested |
| Claude API retry | Custom backoff | Mirror `_call_claude_with_retry()` from DocGenerationService | One retry with 2.5s backoff, same error types |
| Redis Pub/Sub subscription | Raw socket | `redis.pubsub()` with `get_message()` | Already used in jobs.py |
| Spec extraction for narration | Full goal string | Lightweight summary: project name + feature list (first 200 chars of goal) | Haiku input; keep prompt short for latency |

**Key insight:** All infrastructure for this phase already exists. NarrationService is the only net-new module. Everything else is wiring and a new endpoint following established patterns.

---

## Common Pitfalls

### Pitfall 1: State Machine Transition Already Emits build.stage.started
**What goes wrong:** Developer adds narration enrichment to `state_machine.transition()` — this breaks the architecture because transition() doesn't know about narration, specs, or time estimates.
**Why it happens:** The `transition()` method already emits `BUILD_STAGE_STARTED` (see `state_machine.py` line 127). A developer might try to extend that.
**How to avoid:** Keep `transition()` unchanged. NarrationService emits a SECOND `build.stage.started` event with richer payload via `publish_event()`. Frontend should handle multiple events of the same type for the same stage (overwrite with latest).
**Warning signs:** Any import of `NarrationService` inside `state_machine.py`.

### Pitfall 2: Screenshot Capture Before preview_url Exists
**What goes wrong:** `execute_build()` attempts screenshot capture immediately after the CHECKS state transition — but `preview_url` is only available after `start_dev_server()` returns.
**Why it happens:** The stage sequence is CHECKS → health check → start_dev_server → READY. Screenshots at "checks" stage happen physically AFTER the dev server starts (they just use stage="checks" for the S3 key).
**How to avoid:** Both screenshot calls (stage="checks" and stage="ready") must be placed AFTER `preview_url = await sandbox.start_dev_server(...)`. They use `preview_url` as the capture target.
**Warning signs:** `AttributeError: 'NoneType' object has no attribute` for preview_url.

### Pitfall 3: SSE Loop Blocking on pubsub.listen()
**What goes wrong:** Using `async for message in pubsub.listen()` blocks the generator, preventing heartbeat interleaving and making ALB idle timeout (60s) kill connections before a build completes.
**Why it happens:** `pubsub.listen()` is a blocking async iterator. The existing `jobs.py` stream uses this (legacy code) without heartbeats.
**How to avoid:** Use `pubsub.get_message(ignore_subscribe_messages=True)` inside a `asyncio.wait_for(timeout=1.0)` to allow the heartbeat clock check every iteration.
**Warning signs:** Frontend SSE connections drop after 60s on long builds (DEPS stage often takes 45-90s).

### Pitfall 4: Narration Singleton Created at Import Before Redis Initialized
**What goes wrong:** `_narration_service = NarrationService()` at module level is fine (NarrationService is stateless, no Redis in constructor). But adding Redis to the constructor would break test environments.
**Why it happens:** Copying too much from DocGenerationService — that service has a Redis-free constructor.
**How to avoid:** NarrationService constructor must be zero-argument like DocGenerationService. Redis is passed per-call.

### Pitfall 5: Changelog for v0.1 First Build
**What goes wrong:** Changelog generation is called for every iteration build, but the `execute_build()` function handles first builds. `execute_iteration_build()` handles v0.2+.
**Why it happens:** The methods overlap in behavior — both compute a `build_version` at the end.
**How to avoid:** Only call changelog generation from `execute_iteration_build()`, never from `execute_build()`. Or check `build_version != "build_v0_1"` as a guard before launching the task.

### Pitfall 6: NarrationService Spec Input Too Long
**What goes wrong:** Passing the full `goal` string (often 500-2000 chars) to NarrationService causes slower Haiku responses and increases token cost.
**Why it happens:** The locked decision says "lightweight spec summary as input — project name + key features list, not full spec."
**How to avoid:** In NarrationService, truncate spec to first 300 characters or extract a summary. A simple approach: `spec[:300]` as the spec input — sufficient for narration generation without full context.

---

## Code Examples

### NarrationService — emit enriched build.stage.started event

```python
# Source: mirrors doc_generation_service.py pattern
async def narrate(self, job_id: str, stage: str, spec: str, redis: object) -> None:
    """Generate narration for stage and emit enriched build.stage.started event."""
    try:
        narration_text = await asyncio.wait_for(
            self._call_claude(stage, spec),
            timeout=NARRATION_TIMEOUT_SECONDS,
        )
        narration_text = self._apply_safety_filter(narration_text)
    except Exception:
        narration_text = _FALLBACK_NARRATIONS.get(stage, "We're making progress on your build.")

    state_machine = JobStateMachine(redis)  # type: ignore[arg-type]
    await state_machine.publish_event(
        job_id,
        {
            "type": SSEEventType.BUILD_STAGE_STARTED,
            "stage": stage,
            "narration": narration_text,
            "agent_role": STAGE_AGENT_ROLES.get(stage, "Engineer"),
            "time_estimate": STAGE_TIME_ESTIMATES.get(stage, "~30s"),
        },
    )
```

### Wiring in execute_build() (one stage shown, pattern repeats for all 5)

```python
# After: await state_machine.transition(job_id, JobStatus.SCAFFOLD, "Scaffolding project state")
if _redis is not None:
    asyncio.create_task(
        _narration_service.narrate(
            job_id=job_id,
            stage="scaffold",
            spec=job_data.get("goal", "")[:300],
            redis=_redis,
        )
    )
```

### Screenshot wiring in execute_build() (after start_dev_server)

```python
# After: preview_url = await sandbox.start_dev_server(...)
if _settings.screenshot_enabled and _redis is not None:
    asyncio.create_task(
        _screenshot_service.capture(
            preview_url=preview_url,
            job_id=job_id,
            stage="checks",   # Stage key for S3 path
            redis=_redis,
        )
    )
    asyncio.create_task(
        _screenshot_service.capture(
            preview_url=preview_url,
            job_id=job_id,
            stage="ready",
            redis=_redis,
        )
    )
```

### SSE stream endpoint using get_message() with heartbeat

```python
# Source: pattern from logs.py + jobs.py, adapted for pub/sub
HEARTBEAT_INTERVAL_SECONDS: int = 15

async def event_generator():
    pubsub = redis.pubsub()
    await pubsub.subscribe(f"job:{job_id}:events")
    last_heartbeat = time.monotonic()
    try:
        while True:
            if await request.is_disconnected():
                return
            now = time.monotonic()
            if now - last_heartbeat >= HEARTBEAT_INTERVAL_SECONDS:
                yield "event: heartbeat\ndata: {}\n\n"
                last_heartbeat = now
            try:
                msg = await asyncio.wait_for(
                    pubsub.get_message(ignore_subscribe_messages=True),
                    timeout=1.0,
                )
            except asyncio.TimeoutError:
                continue
            if msg and msg["type"] == "message":
                yield f"data: {msg['data']}\n\n"
                parsed = json.loads(msg["data"])
                if parsed.get("status") in ("ready", "failed"):
                    return
    finally:
        await pubsub.unsubscribe(f"job:{job_id}:events")
        await pubsub.close()
```

### Changelog trigger in execute_iteration_build()

```python
# In execute_iteration_build(), after doc generation task (v0.2+ only)
if _redis is not None and build_version != "build_v0_1":
    prev_spec = await _fetch_previous_spec(project_id)  # reads from Job table
    asyncio.create_task(
        _doc_generation_service.generate_changelog(
            job_id=job_id,
            current_spec=job_data.get("goal", ""),
            previous_spec=prev_spec,
            build_version=build_version,
            redis=_redis,
        )
    )
```

---

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| `pubsub.listen()` async iterator for SSE | `pubsub.get_message()` + `asyncio.wait_for(timeout)` loop | This phase | Enables heartbeat interleaving; prevents ALB timeout on 60s+ builds |
| Generic stage labels in SSE events | Claude-generated narration per stage | Phase 36 | Frontend gets human context, not raw status strings |
| No screenshot in live pipeline | ScreenshotService.capture() wired to CHECKS/READY | Phase 36 | `snapshot.updated` SSE events become live |

**Deprecated/outdated:**
- `jobs.py` `/stream` endpoint: Lacks heartbeat, lacks typed events, lacks narration payload. NOT replaced — still used by older frontend polling. The new `/events/stream` endpoint is additive.

---

## Open Questions

1. **NarrationService: separate flag or always-on?**
   - What we know: DocGenerationService has `docs_generation_enabled` flag; ScreenshotService has `screenshot_enabled` flag (both in `Settings`).
   - What's unclear: Should NarrationService have its own `narration_enabled` flag in `Settings`? The CONTEXT.md doesn't mention a flag.
   - Recommendation: Add `narration_enabled: bool = True` to `Settings` following the established pattern. Gate with `_redis is not None` check for test environments.

2. **Changelog previous spec fetching**
   - What we know: `execute_iteration_build()` has access to `project_id` and `job_data`. The `Job` model has `goal` field. The most recent READY job for the project has the previous spec.
   - What's unclear: Should this query be inline in `execute_iteration_build()` or a helper on `GenerationService`?
   - Recommendation: Private helper `_fetch_previous_spec(project_id)` on `GenerationService`, mirrors `_get_next_build_version()` pattern — DB query wrapped in try/except, returns empty string on failure.

3. **Two screenshot captures vs one**
   - What we know: `CAPTURE_STAGES = frozenset({"checks", "ready"})` means two S3 keys. Both use same `preview_url`. The locked decision for `snapshot.updated` event timing: "within 2 seconds of screenshot upload."
   - What's unclear: Is capturing twice intentional (progressive snapshots) or does READY supersede CHECKS? Since the dev server is live for both, both will capture valid screenshots.
   - Recommendation: Keep both captures as designed (two separate S3 objects for time-lapse potential). The second `snapshot.updated` event with the `ready` screenshot will be the last one frontend shows.

---

## Validation Architecture

### Test Framework

| Property | Value |
|----------|-------|
| Framework | pytest 8.3+ with pytest-asyncio 0.24+ |
| Config file | `backend/pyproject.toml` — `[tool.pytest.ini_options]` `asyncio_mode = "auto"` |
| Quick run command | `cd backend && python -m pytest tests/services/test_narration_service.py -x -m unit` |
| Full suite command | `cd backend && python -m pytest tests/ -x -m unit --tb=short` |
| Estimated runtime | ~5-10 seconds for unit tests |

### Phase Requirements → Test Map

| Req ID | Behavior | Test Type | Automated Command | File Exists? |
|--------|----------|-----------|-------------------|-------------|
| NARR-02 | Claude generates idea-specific narration per stage | unit | `pytest tests/services/test_narration_service.py -x -m unit` | ❌ Wave 0 gap |
| NARR-04 | Narration uses first-person co-founder voice (we pronoun, agent role) | unit | `pytest tests/services/test_narration_service.py::test_narration_event_includes_agent_role -x` | ❌ Wave 0 gap |
| NARR-08 | Safety guardrails strip paths, stack traces, secrets | unit | `pytest tests/services/test_narration_service.py::test_narration_safety_filter -x` | ❌ Wave 0 gap |
| SNAP-03 | SSE snapshot.updated event emitted after screenshot | unit | `pytest tests/services/test_screenshot_service.py -x -m unit` | ✅ exists |
| SNAP-03 | ScreenshotService.capture() called in execute_build() pipeline | unit | `pytest tests/services/test_narration_wiring.py -x -m unit` | ❌ Wave 0 gap |
| DOCS-09 | Changelog generated for v0.2+ builds, skipped for v0.1 | unit | `pytest tests/services/test_changelog_wiring.py -x -m unit` | ❌ Wave 0 gap |
| INFRA-03 | SSE stream endpoint delivers typed events with heartbeat | unit | `pytest tests/api/test_generation_routes.py::test_events_stream_endpoint -x` | ❌ Wave 0 gap |

### Nyquist Sampling Rate

- **Minimum sample interval:** After every committed task → run: `cd backend && python -m pytest tests/services/ -x -m unit --tb=short`
- **Full suite trigger:** Before merging final task of any plan wave
- **Phase-complete gate:** Full suite green before `/gsd:verify-work` runs
- **Estimated feedback latency per task:** ~5-8 seconds

### Wave 0 Gaps (must be created before implementation)

- [ ] `tests/services/test_narration_service.py` — covers NARR-02, NARR-04, NARR-08
- [ ] `tests/services/test_narration_wiring.py` — covers SNAP-03 wiring + screenshot wiring in execute_build()
- [ ] `tests/services/test_changelog_wiring.py` — covers DOCS-09
- [ ] `app/services/narration_service.py` — the NarrationService implementation stub
- [ ] `backend/pyproject.toml` — no new dependencies needed (anthropic already present)

---

## Sources

### Primary (HIGH confidence)
- `/Users/vladcortex/co-founder/backend/app/services/doc_generation_service.py` — NarrationService mirrors this architecture exactly
- `/Users/vladcortex/co-founder/backend/app/services/screenshot_service.py` — SNAP-03 already implemented; wiring analysis
- `/Users/vladcortex/co-founder/backend/app/services/generation_service.py` — execute_build() and execute_iteration_build() insertion points
- `/Users/vladcortex/co-founder/backend/app/queue/state_machine.py` — SSEEventType constants, publish_event() API
- `/Users/vladcortex/co-founder/backend/app/api/routes/logs.py` — SSE stream pattern with heartbeat
- `/Users/vladcortex/co-founder/backend/app/api/routes/jobs.py` — existing pubsub stream (no heartbeat — pitfall source)
- `/Users/vladcortex/co-founder/backend/app/api/routes/generation.py` — DocsResponse, docs endpoint, route prefix
- `/Users/vladcortex/co-founder/backend/app/api/routes/__init__.py` — route registration; no new registrations needed
- `/Users/vladcortex/co-founder/backend/pyproject.toml` — test config, asyncio_mode=auto confirmed
- `/Users/vladcortex/co-founder/backend/tests/services/test_doc_generation_wiring.py` — test pattern for wiring tests (FakeSandboxRuntime, patch strategy)

### Secondary (MEDIUM confidence)
- STATE.md: "New SSE events extend existing job:{id}:events Redis Pub/Sub channel with backward-compatible type field" — confirmed by state_machine.py code
- STATE.md: "asyncio.create_task() for doc generation — never inline await in execute_build() critical path" — confirmed as locked pattern; extended to narration

---

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH — all libraries already in use; no new dependencies needed
- Architecture: HIGH — all patterns derived from existing, tested, production code in the codebase
- Pitfalls: HIGH — derived from direct code reading (pubsub.listen() pattern in jobs.py is the live risk)
- Validation: HIGH — pytest config confirmed, existing test patterns directly reusable

**Research date:** 2026-02-24
**Valid until:** 2026-03-24 (30 days — stable stack)
