---
phase: 36-generationservice-wiring-api-routes
plan: 03
type: execute
wave: 2
depends_on: [36-01]
files_modified:
  - backend/app/api/routes/jobs.py
  - backend/tests/api/test_events_stream.py
autonomous: true
requirements: [SNAP-03, NARR-02]

must_haves:
  truths:
    - "GET /api/jobs/{id}/events/stream delivers typed SSE events to authenticated client"
    - "SSE stream emits heartbeat every 15 seconds to prevent ALB idle timeout"
    - "SSE stream closes cleanly when job reaches ready or failed status"
    - "Already-terminal jobs emit final status and close immediately"
    - "Unauthenticated or wrong-user requests return 404"
  artifacts:
    - path: "backend/app/api/routes/jobs.py"
      provides: "stream_job_events endpoint at /{job_id}/events/stream"
      contains: "async def stream_job_events"
    - path: "backend/tests/api/test_events_stream.py"
      provides: "Unit tests for the SSE events stream endpoint"
      min_lines: 80
  key_links:
    - from: "backend/app/api/routes/jobs.py"
      to: "Redis Pub/Sub"
      via: "pubsub.subscribe(f'job:{job_id}:events') with get_message() polling"
      pattern: "pubsub\\.subscribe.*events"
    - from: "backend/app/api/routes/jobs.py"
      to: "StreamingResponse"
      via: "text/event-stream media type with heartbeat"
      pattern: "StreamingResponse.*event-stream"
---

<objective>
Add the typed SSE events stream endpoint `GET /api/jobs/{id}/events/stream` to jobs.py. This endpoint subscribes to the `job:{id}:events` Redis Pub/Sub channel and streams all typed events (build.stage.started, build.stage.completed, snapshot.updated, documentation.updated) with 15-second heartbeat keepalive.

Purpose: Provides the frontend SSE consumer (Phase 37) with a reliable, ALB-compatible event stream. Supports SNAP-03 (snapshot.updated events pass through) and NARR-02 (narration-enriched build.stage.started events pass through).
Output: New endpoint in jobs.py + test suite.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/36-generationservice-wiring-api-routes/36-CONTEXT.md
@.planning/phases/36-generationservice-wiring-api-routes/36-RESEARCH.md
@backend/app/api/routes/jobs.py
@backend/app/api/routes/logs.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add GET /{job_id}/events/stream typed SSE endpoint to jobs.py</name>
  <files>
    backend/app/api/routes/jobs.py
  </files>
  <action>
    Add the new SSE endpoint to `backend/app/api/routes/jobs.py` AFTER the existing `stream_job_status` endpoint (which is at `/{job_id}/stream`). The new endpoint is at `/{job_id}/events/stream` — a different path that coexists with the legacy stream.

    Add imports at top of jobs.py (add to existing imports):
    ```python
    import asyncio
    import time
    from fastapi import Request
    from fastapi.responses import StreamingResponse  # already imported for stream_job_status
    ```

    Add constant:
    ```python
    _EVENTS_HEARTBEAT_INTERVAL = 15  # seconds (locked decision: ALB keepalive)
    ```

    Add endpoint:
    ```python
    @router.get("/{job_id}/events/stream")
    async def stream_job_events(
        job_id: str,
        request: Request,
        user: ClerkUser = Depends(require_auth),
        redis=Depends(get_redis),
    ):
        """Stream typed build events via SSE with 15-second heartbeat keepalive.

        Subscribes to job:{job_id}:events Redis Pub/Sub channel and passes through
        all event types: build.stage.started, build.stage.completed, snapshot.updated,
        documentation.updated.

        Sends heartbeat events every 15 seconds to prevent ALB idle timeout (60s default).
        Closes stream when job reaches terminal state (ready/failed).

        If job is already terminal on connect, emits final status and closes immediately.

        Args:
            job_id: Job UUID
            request: FastAPI Request (for disconnect detection)
            user: Authenticated user from JWT
            redis: Redis client (injected)

        Returns:
            StreamingResponse with text/event-stream

        Raises:
            HTTPException(404): If job not found or user mismatch
        """
        state_machine = JobStateMachine(redis)
        job_data = await state_machine.get_job(job_id)

        if not job_data or job_data.get("user_id") != user.user_id:
            raise HTTPException(status_code=404, detail="Job not found")

        async def event_generator():
            # Check if already terminal — emit final status and close
            current_status = job_data.get("status")
            if current_status in ("ready", "failed"):
                yield f"data: {json.dumps({'type': 'build.stage.started', 'status': current_status, 'job_id': job_id})}\n\n"
                return

            pubsub = redis.pubsub()
            channel = f"job:{job_id}:events"
            await pubsub.subscribe(channel)
            last_heartbeat = time.monotonic()

            try:
                while True:
                    if await request.is_disconnected():
                        return

                    # 15-second heartbeat (ALB keepalive)
                    now = time.monotonic()
                    if now - last_heartbeat >= _EVENTS_HEARTBEAT_INTERVAL:
                        yield "event: heartbeat\ndata: {}\n\n"
                        last_heartbeat = now

                    # Non-blocking poll with 1-second timeout
                    try:
                        message = await asyncio.wait_for(
                            pubsub.get_message(ignore_subscribe_messages=True),
                            timeout=1.0,
                        )
                    except asyncio.TimeoutError:
                        continue

                    if message and message["type"] == "message":
                        yield f"data: {message['data']}\n\n"
                        last_heartbeat = time.monotonic()  # Reset heartbeat on data
                        try:
                            data = json.loads(message["data"])
                            if data.get("status") in ("ready", "failed"):
                                return
                        except (json.JSONDecodeError, TypeError):
                            pass
            finally:
                await pubsub.unsubscribe(channel)
                await pubsub.close()

        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )
    ```

    KEY DIFFERENCES from existing `stream_job_status`:
    1. Uses `pubsub.get_message()` with `asyncio.wait_for(timeout=1.0)` instead of `async for message in pubsub.listen()` — enables heartbeat interleaving
    2. Emits heartbeat every 15 seconds (not 20 like logs.py)
    3. Passes through raw event data (already contains type, stage, narration, etc.)
    4. Resets heartbeat timer on data events (avoid unnecessary heartbeats during active streaming)
    5. Handles already-terminal jobs immediately
  </action>
  <verify>
    <automated>cd backend && python -m pytest tests/api/test_events_stream.py -x -v --tb=short</automated>
    <manual>Endpoint registered correctly. No import errors.</manual>
  </verify>
  <done>
    - GET /api/jobs/{id}/events/stream endpoint exists in jobs.py
    - Uses pubsub.get_message() pattern (not pubsub.listen())
    - 15-second heartbeat keepalive
    - Closes on terminal status
    - Already-terminal jobs get immediate response
    - Auth check: 404 for wrong user or missing job
  </done>
</task>

<task type="auto">
  <name>Task 2: Write tests for SSE events stream endpoint</name>
  <files>
    backend/tests/api/test_events_stream.py
  </files>
  <action>
    Create `backend/tests/api/test_events_stream.py` with unit tests for the new endpoint.

    Testing strategy: The SSE stream endpoint is an async generator. Test it by:
    1. Creating a fakeredis instance
    2. Setting up job data in the fake Redis
    3. Using FastAPI TestClient (httpx) for endpoint tests OR testing the generator directly

    For SSE streaming endpoints, the most reliable test approach is to test the auth/validation logic synchronously and test event emission logic via the generator function pattern used in existing tests. Since the endpoint uses pubsub which is complex to test end-to-end, focus on:

    ```python
    pytestmark = pytest.mark.unit
    ```

    Tests:

    1. `test_events_stream_returns_404_for_unknown_job`: Call endpoint with non-existent job_id, verify 404 response.

    2. `test_events_stream_returns_404_for_wrong_user`: Create job for user A, request with user B, verify 404.

    3. `test_events_stream_terminal_job_emits_final_and_closes`: Create a job with status="ready" in fakeredis. Call the endpoint. The generator should yield one data event with the terminal status and then return (stop iteration). Parse the yielded SSE line and verify it contains `"status": "ready"`.

    4. `test_events_stream_terminal_failed_job`: Same as above but with status="failed".

    5. `test_events_stream_returns_streaming_response`: Create a non-terminal job (status="scaffold"). Call the endpoint. Verify response media_type is "text/event-stream" and headers include Cache-Control, Connection, X-Accel-Buffering.

    Test pattern: Use FastAPI's TestClient with `app.dependency_overrides` for auth and redis, similar to test_generation_routes.py. For the auth override:
    ```python
    from app.core.auth import require_auth, ClerkUser

    async def mock_auth():
        return ClerkUser(user_id="test-user-events-001", email="test@test.com")

    app.dependency_overrides[require_auth] = mock_auth
    ```

    For redis override, use fakeredis:
    ```python
    fake_redis = fakeredis.aioredis.FakeRedis(decode_responses=True)

    async def mock_redis():
        return fake_redis

    app.dependency_overrides[get_redis] = mock_redis
    ```

    For the terminal job tests, pre-populate the job hash:
    ```python
    await fake_redis.hset(f"job:{job_id}", mapping={
        "status": "ready",
        "user_id": "test-user-events-001",
        "project_id": "00000000-0000-0000-0000-000000000099",
    })
    ```

    Use `httpx.AsyncClient(app=app)` for async test client that can handle streaming responses, or use the sync TestClient for non-streaming assertions (auth/404 checks).
  </action>
  <verify>
    <automated>cd backend && python -m pytest tests/api/test_events_stream.py -x -v --tb=short</automated>
    <manual>All 5 event stream tests pass.</manual>
  </verify>
  <done>
    - 5 tests covering auth, 404, terminal-job-ready, terminal-job-failed, streaming response type
    - Tests use fakeredis (no real Redis required)
    - Tests use dependency overrides for auth and redis
    - All tests pass, no warnings
  </done>
</task>

</tasks>

<verification>
```bash
# Run all new tests
cd backend && python -m pytest tests/api/test_events_stream.py -x -v --tb=short

# Verify no regressions in existing job tests
cd backend && python -m pytest tests/ -x --tb=short -m unit -q
```
</verification>

<success_criteria>
- GET /api/jobs/{id}/events/stream endpoint live and returning typed SSE events
- 15-second heartbeat prevents ALB timeout
- Already-terminal jobs get immediate response and close
- Auth enforced (404 for wrong user)
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/36-generationservice-wiring-api-routes/36-03-SUMMARY.md`
</output>
