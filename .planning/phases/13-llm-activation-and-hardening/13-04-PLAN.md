---
phase: 13-llm-activation-and-hardening
plan: 04
type: execute
wave: 2
depends_on:
  - 13-01
  - 13-02
requirements:
  - LLM-07
files_modified:
  - backend/app/api/routes/onboarding.py
  - backend/app/api/routes/understanding.py
  - backend/app/api/routes/execution_plans.py
  - backend/app/services/understanding_service.py
  - backend/app/services/artifact_service.py
  - backend/app/agent/llm_helpers.py
autonomous: true

must_haves:
  truths:
    - "get_runner() in onboarding routes returns RunnerReal with checkpointer from app.state"
    - "get_runner() in understanding routes returns RunnerReal with checkpointer from app.state"
    - "get_runner() in execution_plans routes returns RunnerReal with checkpointer from app.state"
    - "get_runner() falls back to RunnerFake when ANTHROPIC_API_KEY is not set (local dev)"
    - "UnderstandingService.start_session passes user_id, session_id, and tier_slug in context dict to runner"
    - "UnderstandingService.edit_answer passes idea_text (not empty string) to check_question_relevance"
    - "All services that call runner methods pass user_id and session_id for usage tracking"
    - "UnderstandingService.finalize injects _tier into answers before calling generate_idea_brief"
    - "ExecutionPlanService injects _tier into brief dict before calling generate_execution_options"
    - "ArtifactGenerator receives tier in brief_context for generate_artifacts calls"
    - "Exhausted OverloadedError in route handlers returns 202 with 'Added to queue' message and enqueues a background retry"
  artifacts:
    - path: "backend/app/api/routes/onboarding.py"
      provides: "get_runner() returns RunnerReal in production"
      contains: "RunnerReal"
    - path: "backend/app/api/routes/understanding.py"
      provides: "get_runner() returns RunnerReal in production, catches exhausted OverloadedError"
      contains: "RunnerReal"
    - path: "backend/app/api/routes/execution_plans.py"
      provides: "get_runner() returns RunnerReal in production, catches exhausted OverloadedError"
      contains: "RunnerReal"
    - path: "backend/app/services/understanding_service.py"
      provides: "Context dict with user_id, session_id, tier_slug; _tier injected into answers before generate_idea_brief"
      contains: "tier_slug"
    - path: "backend/app/services/artifact_service.py"
      provides: "Passes tier in brief_context to generator.generate_cascade"
      contains: "_tier"
  key_links:
    - from: "backend/app/api/routes/onboarding.py"
      to: "backend/app/agent/runner_real.py"
      via: "get_runner returns RunnerReal"
      pattern: "RunnerReal"
    - from: "backend/app/api/routes/understanding.py"
      to: "backend/app/main.py"
      via: "request.app.state.checkpointer"
      pattern: "app.state.checkpointer"
    - from: "backend/app/services/understanding_service.py"
      to: "backend/app/core/llm_config.py"
      via: "get_or_create_user_settings for tier resolution"
      pattern: "get_or_create_user_settings"
    - from: "backend/app/services/understanding_service.py"
      to: "backend/app/agent/runner_real.py"
      via: "_tier key in answers dict reaches generate_idea_brief"
      pattern: "_tier"
    - from: "backend/app/services/artifact_service.py"
      to: "backend/app/artifacts/generator.py"
      via: "_tier in onboarding_data reaches generate_artifacts"
      pattern: "_tier"
---

<objective>
Wire RunnerReal into all route handlers, fix service layer context passing, inject tier into data dicts for downstream methods, and implement queue-on-exhausted-retries (LOCKED DECISION).

Purpose: The route files currently hardcode `RunnerFake()` in their `get_runner()` dependencies. This plan swaps them to `RunnerReal` with the checkpointer from `app.state`. It fixes the UnderstandingService to pass `user_id`, `session_id`, and `tier_slug` in the context dict. It injects `_tier` into the answers dict (before generate_idea_brief), the brief dict (before generate_execution_options), and the onboarding_data (before generate_artifacts) so tier-differentiated prompts in Plan 13-05 receive the tier value. It also implements the locked decision to queue requests when all retries are exhausted.

Output: All 3 route files return RunnerReal in production. UnderstandingService passes complete context. Tier flows through to all downstream runner methods. Exhausted OverloadedError returns 202 with queue message.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/13-llm-activation-and-hardening/13-CONTEXT.md
@.planning/phases/13-llm-activation-and-hardening/13-RESEARCH.md

@backend/app/api/routes/onboarding.py
@backend/app/api/routes/understanding.py
@backend/app/api/routes/execution_plans.py
@backend/app/services/understanding_service.py
@backend/app/services/artifact_service.py
@backend/app/core/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update get_runner() in all 3 route files to return RunnerReal</name>
  <files>
    backend/app/api/routes/onboarding.py
    backend/app/api/routes/understanding.py
    backend/app/api/routes/execution_plans.py
  </files>
  <action>
Update `get_runner()` in all 3 route files to conditionally return RunnerReal (production) or RunnerFake (local dev without API key).

The pattern is identical for all 3 files. Replace the existing `get_runner()` function:

**From (current in all 3 files):**
```python
def get_runner() -> Runner:
    """Dependency that provides Runner instance.
    ...
    """
    return RunnerFake()
```

**To (new in all 3 files):**
```python
from fastapi import Request

def get_runner(request: Request) -> Runner:
    """Dependency that provides Runner instance.

    Returns RunnerReal in production (when ANTHROPIC_API_KEY is set).
    Falls back to RunnerFake for local dev without API key.
    Override this dependency in tests via app.dependency_overrides.
    """
    from app.core.config import get_settings
    settings = get_settings()

    if settings.anthropic_api_key:
        from app.agent.runner_real import RunnerReal
        checkpointer = getattr(request.app.state, "checkpointer", None)
        return RunnerReal(checkpointer=checkpointer)
    else:
        return RunnerFake()
```

**Add `Request` to the FastAPI import** in each file if not already present:
```python
from fastapi import APIRouter, Depends, HTTPException, Request
```

**IMPORTANT:** The `runner: Runner = Depends(get_runner)` signatures in route functions do NOT need to change — FastAPI automatically passes `Request` to the `get_runner` dependency when it sees the `Request` type hint in the function signature.

**BUT** route functions that also need the runner must keep their existing `Depends(get_runner)` pattern. FastAPI handles the `Request` injection automatically for dependencies.

Apply this identical change to:
1. `backend/app/api/routes/onboarding.py`
2. `backend/app/api/routes/understanding.py`
3. `backend/app/api/routes/execution_plans.py`
  </action>
  <verify>
Run `cd /Users/vladcortex/co-founder/backend && python -c "
from app.api.routes.onboarding import get_runner
from app.api.routes.understanding import get_runner as get_runner2
from app.api.routes.execution_plans import get_runner as get_runner3
print('All get_runner imports OK')
"` — no import errors.

Grep for `RunnerFake()` as sole return in route files — should NOT find bare `return RunnerFake()` without the conditional check.
Grep for `RunnerReal` in all 3 route files — should find it in each.
  </verify>
  <done>
All 3 route files (onboarding, understanding, execution_plans) return RunnerReal in production when ANTHROPIC_API_KEY is set, with checkpointer from app.state. Falls back to RunnerFake for local dev. Tests can override via dependency_overrides.
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix UnderstandingService context passing and inject _tier into data dicts for downstream methods</name>
  <files>
    backend/app/services/understanding_service.py
    backend/app/services/artifact_service.py
  </files>
  <action>
Fix `UnderstandingService.start_session()` and related methods to pass complete context to RunnerReal. Additionally, inject `_tier` into the answers dict before `generate_idea_brief` and into the brief dict before `generate_execution_options` / `generate_artifacts` — so Plan 13-05's tier-differentiated prompts receive the tier value.

**1. Fix `start_session` (lines 77-82)** — Add user_id, session_id, and tier_slug to the context dict:

Change:
```python
            # Generate understanding questions via Runner
            context = {
                "idea_text": onboarding.idea_text,
                "onboarding_answers": onboarding.answers,
            }
```

To:
```python
            # Resolve tier for the user
            from app.core.llm_config import get_or_create_user_settings
            user_settings = await get_or_create_user_settings(clerk_user_id)
            tier_slug = user_settings.plan_tier.slug if user_settings.plan_tier else "bootstrapper"

            # Generate understanding questions via Runner
            context = {
                "idea_text": onboarding.idea_text,
                "onboarding_answers": onboarding.answers,
                "user_id": clerk_user_id,
                "session_id": str(onboarding.id),
                "tier": tier_slug,
            }
```

**2. Fix `edit_answer` (lines 190-195)** — Pass the actual idea_text instead of empty string:

Change:
```python
            relevance_check = await self.runner.check_question_relevance(
                idea="",  # Not needed for fake
                answered=answered_questions,
                answers=understanding.answers,
                remaining=remaining_questions,
            )
```

To:
```python
            # Load onboarding session for idea_text
            onboarding_result = await session.execute(
                select(OnboardingSession).where(
                    OnboardingSession.id == understanding.onboarding_session_id
                )
            )
            onboarding = onboarding_result.scalar_one()

            relevance_check = await self.runner.check_question_relevance(
                idea=onboarding.idea_text,
                answered=answered_questions,
                answers=understanding.answers,
                remaining=remaining_questions,
            )
```

**3. Fix `re_interview` (lines 439-446)** — Add user_id, session_id, tier to context:

Change:
```python
            context = {
                "idea_text": onboarding.idea_text,
                "existing_brief": (
                    understanding.answers if understanding.answers else None
                ),  # Context from previous attempt
            }
```

To:
```python
            # Resolve tier for the user
            from app.core.llm_config import get_or_create_user_settings
            user_settings = await get_or_create_user_settings(clerk_user_id)
            tier_slug = user_settings.plan_tier.slug if user_settings.plan_tier else "bootstrapper"

            context = {
                "idea_text": onboarding.idea_text,
                "onboarding_answers": onboarding.answers,
                "existing_brief": (
                    understanding.answers if understanding.answers else None
                ),
                "user_id": clerk_user_id,
                "session_id": str(understanding.id),
                "tier": tier_slug,
            }
```

**4. Fix `finalize` (around line 257)** — Inject `_tier` into the answers dict before calling `generate_idea_brief`:

Before the `generate_idea_brief` call:
```python
            # Resolve tier for the user
            from app.core.llm_config import get_or_create_user_settings
            user_settings = await get_or_create_user_settings(clerk_user_id)
            tier_slug = user_settings.plan_tier.slug if user_settings.plan_tier else "bootstrapper"

            # Inject tier into answers so generate_idea_brief can read it
            answers_with_tier = {**understanding.answers, "_tier": tier_slug}

            # Generate Idea Brief via Runner
            brief_content = await self.runner.generate_idea_brief(
                idea=onboarding.idea_text,
                questions=understanding.questions,
                answers=answers_with_tier,
            )
```

This ensures `generate_idea_brief` in RunnerReal can access tier via `answers.get("_tier", "bootstrapper")`.

**5. Inject `_tier` into the brief artifact content** so downstream methods (`generate_execution_options`, `generate_artifacts`) can read it:

After `brief_content` is generated, add:
```python
            # Inject _tier into brief content for downstream tier-differentiation
            brief_content["_tier"] = tier_slug
```

This ensures `generate_execution_options` reads `brief.get("_tier", "bootstrapper")` and `generate_artifacts` reads `brief.get("_tier", "bootstrapper")`.

**6. Update `artifact_service.py`** — In `generate_all()`, inject `_tier` into onboarding_data so the ArtifactGenerator (which calls `runner.generate_artifacts(brief_context)`) can pass tier through:

In `artifact_service.py`'s `generate_all` method, the `onboarding_data` dict is passed to `self.generator.generate_cascade(onboarding_data=onboarding_data)`. The `tier` is already a separate parameter. Ensure tier reaches `generate_artifacts` by injecting it into onboarding_data:

Before the `generate_cascade` call:
```python
            # Inject tier into onboarding_data so it reaches generate_artifacts
            onboarding_data_with_tier = {**onboarding_data, "_tier": tier}
```

Then pass `onboarding_data_with_tier` to `generate_cascade`:
```python
            completed, failed = await self.generator.generate_cascade(
                onboarding_data=onboarding_data_with_tier,
                existing_artifacts=existing_artifacts if existing_artifacts else None,
            )
```

This makes `_tier` available in `brief_context` when `ArtifactGenerator.generate_artifact` calls `runner.generate_artifacts(brief_context)`.

**7. Move the `OnboardingSession` import to the top of the file** — it's already imported. Verify it's in the imports block.
  </action>
  <verify>
Run `cd /Users/vladcortex/co-founder/backend && python -c "from app.services.understanding_service import UnderstandingService; print('Service import OK')"` — no errors.
Grep for `idea=""` in understanding_service.py — should return zero results (no empty idea string).
Grep for `tier` in understanding_service.py context dicts — should find it in start_session, re_interview, and finalize.
Grep for `_tier` in understanding_service.py — should find it injected into answers and brief_content.
Grep for `_tier` in artifact_service.py — should find it injected into onboarding_data.
  </verify>
  <done>
UnderstandingService passes complete context (user_id, session_id, tier) to RunnerReal for all runner calls. edit_answer passes actual idea_text. finalize injects _tier into answers and brief for tier-differentiated prompts. artifact_service injects _tier into onboarding_data for generate_artifacts. All downstream methods (generate_idea_brief, generate_execution_options, generate_artifacts) can now read tier.
  </done>
</task>

<task type="auto">
  <name>Task 3: Catch exhausted OverloadedError in route handlers and queue the request (LOCKED DECISION)</name>
  <files>
    backend/app/api/routes/understanding.py
    backend/app/api/routes/execution_plans.py
    backend/app/api/routes/onboarding.py
    backend/app/agent/llm_helpers.py
  </files>
  <action>
Implement the LOCKED decision: "All retries exhausted: queue the request. 'Added to queue — we'll continue automatically when capacity is available.' Auto-retry later."

**1. Add a queue helper to `backend/app/agent/llm_helpers.py`:**

Add at the end of the file:
```python
async def enqueue_failed_request(user_id: str, session_id: str, action: str, payload: dict) -> None:
    """Enqueue a failed LLM request for background retry.

    Stores the request in a Redis list for later processing.
    Non-blocking: logs and returns on Redis failure.

    Args:
        user_id: Clerk user ID
        session_id: Session or project ID for correlation
        action: Action identifier (e.g., "generate_understanding_questions", "finalize")
        payload: Request payload to replay
    """
    try:
        from app.db.redis import get_redis
        r = get_redis()
        entry = json.dumps({
            "user_id": user_id,
            "session_id": session_id,
            "action": action,
            "payload": payload,
            "queued_at": __import__("datetime").datetime.now(__import__("datetime").timezone.utc).isoformat(),
        })
        await r.rpush("cofounder:llm_queue", entry)
        logger.info("Queued LLM request for retry: user=%s action=%s", user_id, action)
    except Exception as e:
        logger.warning("Failed to enqueue LLM request (non-blocking): %s", e)
```

**2. Wrap LLM-calling route handlers with OverloadedError catch.**

In each route file that calls runner methods through services (understanding.py, execution_plans.py, onboarding.py), wrap the service call in a try/except that catches `OverloadedError` (from `anthropic._exceptions`):

```python
from anthropic._exceptions import OverloadedError
from fastapi.responses import JSONResponse
from app.agent.llm_helpers import enqueue_failed_request
```

For each route handler that calls a service method which calls a runner method (e.g., `start_session`, `finalize`, `edit_answer`, `generate_options`), wrap the call:

```python
try:
    result = await service.start_session(...)  # or finalize, generate_options, etc.
except OverloadedError:
    await enqueue_failed_request(
        user_id=clerk_user_id,
        session_id=session_id,
        action="start_session",
        payload={...relevant request data...},
    )
    return JSONResponse(
        status_code=202,
        content={
            "status": "queued",
            "message": "Added to queue \u2014 we'll continue automatically when capacity is available.",
        },
    )
```

Apply this pattern to:
- `understanding.py`: `start_session`, `finalize`, `edit_answer`, `re_interview` endpoints
- `execution_plans.py`: `generate_options` endpoint
- `onboarding.py`: any endpoint that calls `runner.generate_questions` or `runner.generate_brief`

**IMPORTANT:** The `_invoke_with_retry` in llm_helpers.py already has `reraise=True`, so after 4 attempts the OverloadedError propagates up through the service layer to the route handler. This catch is the last line of defense — it queues the request instead of returning a 500.

**NOTE on auto-retry:** The queue consumer (background worker that reads from `cofounder:llm_queue` and replays requests) is out of scope for this plan. The queue stores enough context (action, payload, user_id) that a future background worker can read and retry. For now, the founder sees the 202 response and can come back later. The queue infrastructure (Redis list) is immediately usable by a future consumer.
  </action>
  <verify>
Grep for `enqueue_failed_request` in llm_helpers.py — should find the function definition.
Grep for `OverloadedError` in understanding.py — should find the import and catch.
Grep for `202` or `"queued"` in understanding.py — should find the JSONResponse.
Grep for `cofounder:llm_queue` in llm_helpers.py — should find the Redis key.
  </verify>
  <done>
Route handlers catch exhausted OverloadedError, enqueue the request to Redis list `cofounder:llm_queue`, and return 202 with "Added to queue — we'll continue automatically when capacity is available." message. The queue stores user_id, session_id, action, and payload for a future background consumer to auto-retry.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.api.routes.onboarding import get_runner"` succeeds
2. `grep -c "RunnerReal" backend/app/api/routes/onboarding.py` returns at least 1
3. `grep -c "RunnerReal" backend/app/api/routes/understanding.py` returns at least 1
4. `grep -c "RunnerReal" backend/app/api/routes/execution_plans.py` returns at least 1
5. No `idea=""` in understanding_service.py
6. `grep "tier" backend/app/services/understanding_service.py` finds tier in context dicts
7. `grep "_tier" backend/app/services/understanding_service.py` finds _tier injected into answers and brief
8. `grep "_tier" backend/app/services/artifact_service.py` finds _tier injected into onboarding_data
9. `grep "enqueue_failed_request" backend/app/agent/llm_helpers.py` finds function definition
10. `grep "OverloadedError" backend/app/api/routes/understanding.py` finds import and catch
11. `grep "202" backend/app/api/routes/understanding.py` finds queued response
</verification>

<success_criteria>
- RunnerReal is the default runner in production (when ANTHROPIC_API_KEY is set)
- RunnerFake is fallback for local dev without API key
- Tests can override get_runner() via dependency_overrides (existing pattern preserved)
- RunnerReal receives checkpointer from app.state for concurrent-safe checkpointing
- UnderstandingService passes user_id, session_id, tier in context for model resolution and usage tracking
- edit_answer passes real idea_text for meaningful relevance checking
- _tier is injected into answers before generate_idea_brief, into brief before generate_execution_options, into onboarding_data before generate_artifacts
- Exhausted OverloadedError returns 202 with "Added to queue" message and enqueues to Redis
- RunnerReal.run() executes full LangGraph pipeline with real Claude code generation
</success_criteria>

<output>
After completion, create `.planning/phases/13-llm-activation-and-hardening/13-04-SUMMARY.md`
</output>
