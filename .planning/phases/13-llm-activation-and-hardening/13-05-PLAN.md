---
phase: 13-llm-activation-and-hardening
plan: 05
type: execute
wave: 3
depends_on:
  - 13-03
requirements:
  - LLM-15
files_modified:
  - backend/app/agent/runner_real.py
autonomous: true

must_haves:
  truths:
    - "Bootstrapper tier generates 6-8 interview questions"
    - "Partner tier generates 10-12 interview questions"
    - "cto_scale tier generates 14-16 interview questions"
    - "Higher tiers unlock extra brief sections (competitive_analysis, scalability_notes, risk_deep_dive)"
    - "Execution plan options include richer engineering impact analysis for higher tiers"
    - "Artifact generation includes tier-conditional sections (market_analysis, competitive_strategy for higher tiers)"
    - "Model selection uses existing create_tracked_llm() tier-to-model mapping (not overridden)"
  artifacts:
    - path: "backend/app/agent/runner_real.py"
      provides: "Tier-differentiated prompts and section lists"
      contains: "BRIEF_SECTIONS_BY_TIER"
    - path: "backend/app/agent/runner_real.py"
      provides: "Tier-conditional interview depth"
      contains: "bootstrapper.*6-8"
  key_links:
    - from: "backend/app/agent/runner_real.py"
      to: "backend/app/core/llm_config.py"
      via: "create_tracked_llm resolves model by tier automatically"
      pattern: "create_tracked_llm"
---

<objective>
Add tier-differentiated behavior to RunnerReal prompts and responses.

Purpose: Higher-paying tiers should receive richer analysis. Bootstrapper gets the essentials (6-8 questions, core brief sections). Partner and cto_scale get deeper interviews, extra brief sections (competitive analysis, scalability notes, risk deep-dives), and richer engineering impact analysis in execution plan options. Model selection remains via create_tracked_llm() (already tier-aware).

Output: RunnerReal with tier-conditional prompt sections, question counts, and brief structures.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/13-llm-activation-and-hardening/13-CONTEXT.md
@.planning/phases/13-llm-activation-and-hardening/13-RESEARCH.md

@backend/app/agent/runner_real.py
@backend/app/db/seed.py — tier model mappings
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add tier-differentiated constants and update prompts</name>
  <files>
    backend/app/agent/runner_real.py
  </files>
  <action>
Add tier-differentiation constants and update RunnerReal methods to use them.

**1. Add tier constants** at module level (after COFOUNDER_SYSTEM):

```python
# Tier-based interview question counts (locked decision)
QUESTION_COUNT_BY_TIER = {
    "bootstrapper": "6-8",
    "partner": "10-12",
    "cto_scale": "14-16",
}

# Tier-based brief sections (locked decision)
BRIEF_SECTIONS_BY_TIER = {
    "bootstrapper": [
        "problem_statement", "target_user", "value_prop",
        "key_constraints", "assumptions", "risks",
        "smallest_viable_experiment", "confidence_scores",
    ],
    "partner": [
        "problem_statement", "target_user", "value_prop",
        "differentiation", "monetization_hypothesis", "market_context",
        "key_constraints", "assumptions", "risks",
        "smallest_viable_experiment", "confidence_scores",
    ],
    "cto_scale": [
        "problem_statement", "target_user", "value_prop",
        "differentiation", "monetization_hypothesis", "market_context",
        "competitive_analysis", "scalability_notes", "risk_deep_dive",
        "key_constraints", "assumptions", "risks",
        "smallest_viable_experiment", "confidence_scores",
    ],
}

# Tier-based execution plan richness
EXEC_PLAN_DETAIL_BY_TIER = {
    "bootstrapper": "For each option, provide a brief engineering impact summary (2-3 sentences).",
    "partner": "For each option, provide detailed engineering impact analysis including team composition, coordination overhead, technical risk areas, and dependency chain. Include a cost_note with estimated budget range.",
    "cto_scale": "For each option, provide comprehensive engineering impact analysis including team composition, skill requirements, coordination overhead, technical risk areas, dependency chains, infrastructure requirements, and scaling considerations. Include detailed cost_note with budget breakdown by role. Add a 'technical_deep_dive' field with architecture recommendations.",
}

# Tier-based artifact richness
ARTIFACT_TIER_SECTIONS = {
    "bootstrapper": "Include core fields only for each artifact (no market_analysis, competitive_strategy, resource_plan, scalability_plan, financial_risks, strategic_risks, integration_points, security_compliance, risk_mitigation_timeline).",
    "partner": "Include core fields plus business-tier sections: market_analysis in brief, technical_architecture in mvp_scope, resource_plan in milestones, financial_risks in risk_log, integration_points in how_it_works.",
    "cto_scale": "Include all fields — core, business-tier, and strategic-tier sections: competitive_strategy in brief, scalability_plan in mvp_scope, risk_mitigation_timeline in milestones, strategic_risks in risk_log, security_compliance in how_it_works.",
}
```

**2. Update `generate_understanding_questions`** — use `QUESTION_COUNT_BY_TIER`:
```python
tier = context.get("tier", "bootstrapper")
question_count = QUESTION_COUNT_BY_TIER.get(tier, "6-8")
# Use question_count in the system prompt
```

**3. Update `generate_idea_brief`** — use `BRIEF_SECTIONS_BY_TIER` to tell Claude which sections to include:
```python
tier = context.get("tier", "bootstrapper") if isinstance(context, dict) else "bootstrapper"
# Extract tier from the answers or brief context
sections = BRIEF_SECTIONS_BY_TIER.get(tier, BRIEF_SECTIONS_BY_TIER["bootstrapper"])
sections_instruction = f"Include these sections in the brief: {', '.join(sections)}"
# Add sections_instruction to the system prompt
```

NOTE: The `generate_idea_brief` method signature is `(self, idea, questions, answers)` — tier must be extracted from the answers dict if stored there, or from a `_context` key. Update the method to accept an optional `context` parameter or extract tier from answers if it has a `_tier` key. The simplest approach: check `answers.get("_tier", "bootstrapper")`.

**4. Update `generate_execution_options`** — use `EXEC_PLAN_DETAIL_BY_TIER`:
```python
tier = brief.get("_tier", "bootstrapper")
detail_instruction = EXEC_PLAN_DETAIL_BY_TIER.get(tier, EXEC_PLAN_DETAIL_BY_TIER["bootstrapper"])
# Add detail_instruction to the system prompt
```

**5. Update `generate_artifacts`** — use `ARTIFACT_TIER_SECTIONS`:
```python
tier = brief.get("_tier", "bootstrapper")
tier_sections = ARTIFACT_TIER_SECTIONS.get(tier, ARTIFACT_TIER_SECTIONS["bootstrapper"])
# Add tier_sections to the system prompt
```

**6. Update service layer to pass tier in context** — In `UnderstandingService.finalize()`, add `_tier` to the answers dict before calling `generate_idea_brief`:

Wait — this is in understanding_service.py. Since Plan 13-04 handles that file, and this plan depends on 13-03 not 13-04, we need to handle the tier threading differently.

**Approach:** RunnerReal methods that need tier should extract it from the context. For methods called by services that already pass context (generate_understanding_questions), the tier is in `context["tier"]`. For methods where tier isn't in the signature (generate_idea_brief, generate_execution_options, generate_artifacts), extract from the data if available or default to bootstrapper.

For `generate_idea_brief(idea, questions, answers)`:
- Check `answers.get("_tier", "bootstrapper")`

For `generate_execution_options(brief, feedback)`:
- Check `brief.get("_tier", "bootstrapper")`

For `generate_artifacts(brief)`:
- Check `brief.get("_tier", "bootstrapper")`

The service layer (Plan 13-04) will inject `_tier` into the data dicts.
  </action>
  <verify>
Run `cd /Users/vladcortex/co-founder/backend && python -c "
from app.agent.runner_real import QUESTION_COUNT_BY_TIER, BRIEF_SECTIONS_BY_TIER, EXEC_PLAN_DETAIL_BY_TIER
assert QUESTION_COUNT_BY_TIER['bootstrapper'] == '6-8'
assert QUESTION_COUNT_BY_TIER['partner'] == '10-12'
assert QUESTION_COUNT_BY_TIER['cto_scale'] == '14-16'
assert 'competitive_analysis' not in BRIEF_SECTIONS_BY_TIER['bootstrapper']
assert 'competitive_analysis' in BRIEF_SECTIONS_BY_TIER['cto_scale']
print('Tier constants verified')
"` — prints "Tier constants verified".

Grep for `QUESTION_COUNT_BY_TIER` in runner_real.py — should find it in the constant definition and in generate_understanding_questions.
Grep for `BRIEF_SECTIONS_BY_TIER` in runner_real.py — should find it in the constant definition and in generate_idea_brief.
  </verify>
  <done>
RunnerReal uses tier-differentiated constants for question count (6-8 / 10-12 / 14-16), brief sections (core / +business / +strategic), execution plan detail level, and artifact richness. Model selection remains via create_tracked_llm() tier-to-model mapping. Higher tiers receive genuinely richer analysis without changing the API contract.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.agent.runner_real import QUESTION_COUNT_BY_TIER, BRIEF_SECTIONS_BY_TIER"` succeeds
2. Bootstrapper gets 6-8 questions, partner 10-12, cto_scale 14-16
3. Bootstrapper brief has 8 sections, cto_scale has 13 sections
4. `grep -c "BRIEF_SECTIONS_BY_TIER" backend/app/agent/runner_real.py` returns at least 2
5. `grep -c "EXEC_PLAN_DETAIL_BY_TIER" backend/app/agent/runner_real.py` returns at least 2
</verification>

<success_criteria>
- Bootstrapper: 6-8 interview questions, core brief sections only
- Partner: 10-12 questions, core + business sections (differentiation, monetization, market_context)
- cto_scale: 14-16 questions, all sections including competitive_analysis, scalability_notes, risk_deep_dive
- Execution plan options: same count (2-3) across tiers, but richer engineering impact for higher tiers
- Artifact generation: tier-conditional extra sections per artifact type
- Model selection unchanged (create_tracked_llm handles tier-to-model mapping)
</success_criteria>

<output>
After completion, create `.planning/phases/13-llm-activation-and-hardening/13-05-SUMMARY.md`
</output>
