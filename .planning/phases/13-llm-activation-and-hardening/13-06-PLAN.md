---
phase: 13-llm-activation-and-hardening
plan: 06
type: execute
wave: 2
depends_on:
  - 13-01
requirements:
  - LLM-11
  - LLM-12
files_modified:
  - backend/app/domain/risks.py
  - backend/app/services/dashboard_service.py
  - backend/app/services/journey.py
  - backend/tests/domain/test_risks.py
autonomous: true

must_haves:
  truths:
    - "detect_llm_risks() queries Redis for daily usage ratio and returns risk when >80%"
    - "detect_llm_risks() logs failures at WARNING level instead of crashing"
    - "detect_llm_risks() is async and accepts user_id and session parameters"
    - "build_failure_count is computed from Job.status == 'failed' rows, not hardcoded 0"
    - "dashboard_service.py passes real build_failure_count to detect_system_risks()"
    - "journey.py passes real build_failure_count to detect_system_risks()"
    - "Unit tests cover detect_llm_risks with high usage and normal usage scenarios"
    - "Unit tests cover build_failure_count query"
  artifacts:
    - path: "backend/app/domain/risks.py"
      provides: "Real detect_llm_risks with Redis usage check"
      contains: "cofounder:usage"
    - path: "backend/app/services/dashboard_service.py"
      provides: "Real build_failure_count from Job table"
      contains: "Job.status"
    - path: "backend/app/services/journey.py"
      provides: "Real build_failure_count from Job table"
      contains: "Job.status"
    - path: "backend/tests/domain/test_risks.py"
      provides: "Tests for detect_llm_risks and detect_system_risks"
  key_links:
    - from: "backend/app/domain/risks.py"
      to: "backend/app/db/redis.py"
      via: "Redis daily usage counter"
      pattern: "get_redis"
    - from: "backend/app/services/dashboard_service.py"
      to: "backend/app/db/models/job.py"
      via: "Job.status == 'failed' count query"
      pattern: "Job.status.*failed"
    - from: "backend/app/services/journey.py"
      to: "backend/app/db/models/job.py"
      via: "Job.status == 'failed' count query"
      pattern: "Job.status.*failed"
---

<objective>
Replace risk signal stubs with real data from Redis usage tracking and Job failure counts.

Purpose: The risk dashboard currently shows empty list from detect_llm_risks() and hardcoded 0 for build_failure_count. After this plan, founders see real signals: high token usage warnings and actual build failure counts. Operators get visibility into usage patterns and system health.

Output: detect_llm_risks() with Redis usage check, real build_failure_count from Job table, unit tests.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/13-llm-activation-and-hardening/13-CONTEXT.md
@.planning/phases/13-llm-activation-and-hardening/13-RESEARCH.md

@backend/app/domain/risks.py
@backend/app/services/dashboard_service.py
@backend/app/services/journey.py
@backend/app/core/llm_config.py — Redis key pattern
@backend/app/db/models/job.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement real detect_llm_risks() with Redis usage check</name>
  <files>
    backend/app/domain/risks.py
    backend/tests/domain/test_risks.py
  </files>
  <action>
**1. Rewrite `detect_llm_risks` in `backend/app/domain/risks.py`:**

Replace the stub function with a real async implementation. Note: this changes the function from sync to async, and from pure to requiring Redis/DB access. The domain purity is traded for real risk signals.

```python
import logging

logger = logging.getLogger(__name__)


async def detect_llm_risks(user_id: str, session) -> list[dict]:
    """Detect LLM-related risks from Redis usage data.

    Checks daily token usage ratio against the user's plan limit.
    Returns risk signal when usage exceeds 80% of daily limit.

    Args:
        user_id: Clerk user ID
        session: SQLAlchemy async session (for user settings lookup)

    Returns:
        List of risk dicts with structure: {"type": "llm", "rule": str, "message": str}
    """
    risks: list[dict] = []

    try:
        from datetime import date
        from app.db.redis import get_redis
        from app.core.llm_config import get_or_create_user_settings

        r = get_redis()
        today = date.today().isoformat()
        key = f"cofounder:usage:{user_id}:{today}"
        used_tokens = int(await r.get(key) or 0)

        user_settings = await get_or_create_user_settings(user_id)
        tier = user_settings.plan_tier
        max_tokens = (
            user_settings.override_max_tokens_per_day
            if user_settings.override_max_tokens_per_day is not None
            else tier.max_tokens_per_day
        )

        if max_tokens != -1 and max_tokens > 0:
            ratio = used_tokens / max_tokens
            if ratio > 0.8:
                risks.append({
                    "type": "llm",
                    "rule": "high_token_usage",
                    "message": f"We're at {ratio:.0%} of today's token budget. Consider upgrading to avoid interruptions.",
                })
    except Exception as e:
        logger.warning("detect_llm_risks check failed (non-blocking): %s", e)

    return risks
```

**2. Keep `detect_system_risks` unchanged** — it remains a pure function.

**3. Update `backend/tests/domain/test_risks.py`** — add tests for the new detect_llm_risks. Since the existing test file tests detect_system_risks (pure function), add a separate test class for detect_llm_risks:

```python
"""Tests for LLM risk detection."""
import pytest
from unittest.mock import AsyncMock, MagicMock, patch

from app.domain.risks import detect_llm_risks


class TestDetectLlmRisks:
    @pytest.mark.asyncio
    async def test_high_usage_returns_risk(self):
        """When usage > 80% of daily limit, return high_token_usage risk."""
        mock_redis = AsyncMock()
        mock_redis.get = AsyncMock(return_value=b"85000")

        mock_settings = MagicMock()
        mock_settings.plan_tier.max_tokens_per_day = 100000
        mock_settings.override_max_tokens_per_day = None

        with patch("app.domain.risks.get_redis", return_value=mock_redis), \
             patch("app.domain.risks.get_or_create_user_settings", return_value=mock_settings):
            risks = await detect_llm_risks("user_123", None)

        assert len(risks) == 1
        assert risks[0]["rule"] == "high_token_usage"
        assert "85%" in risks[0]["message"]

    @pytest.mark.asyncio
    async def test_normal_usage_returns_empty(self):
        """When usage < 80% of daily limit, return no risks."""
        mock_redis = AsyncMock()
        mock_redis.get = AsyncMock(return_value=b"50000")

        mock_settings = MagicMock()
        mock_settings.plan_tier.max_tokens_per_day = 100000
        mock_settings.override_max_tokens_per_day = None

        with patch("app.domain.risks.get_redis", return_value=mock_redis), \
             patch("app.domain.risks.get_or_create_user_settings", return_value=mock_settings):
            risks = await detect_llm_risks("user_123", None)

        assert len(risks) == 0

    @pytest.mark.asyncio
    async def test_unlimited_plan_returns_empty(self):
        """When plan has unlimited tokens (-1), return no risks."""
        mock_redis = AsyncMock()
        mock_redis.get = AsyncMock(return_value=b"999999")

        mock_settings = MagicMock()
        mock_settings.plan_tier.max_tokens_per_day = -1
        mock_settings.override_max_tokens_per_day = None

        with patch("app.domain.risks.get_redis", return_value=mock_redis), \
             patch("app.domain.risks.get_or_create_user_settings", return_value=mock_settings):
            risks = await detect_llm_risks("user_123", None)

        assert len(risks) == 0

    @pytest.mark.asyncio
    async def test_redis_failure_returns_empty(self):
        """When Redis fails, return empty list (non-blocking)."""
        with patch("app.domain.risks.get_redis", side_effect=Exception("Redis down")):
            risks = await detect_llm_risks("user_123", None)

        assert len(risks) == 0
```

NOTE: The existing tests for `detect_system_risks` should remain unchanged. Append the new test class to the file or create a separate test file if the existing one doesn't exist.
  </action>
  <verify>
Run `cd /Users/vladcortex/co-founder/backend && python -c "from app.domain.risks import detect_llm_risks; import asyncio; print('Import OK')"` — verify async function imports.
Run `cd /Users/vladcortex/co-founder/backend && python -m pytest tests/domain/test_risks.py -v -k "llm"` — all new tests pass.
  </verify>
  <done>
detect_llm_risks queries Redis for daily token usage, compares against plan tier limit, and returns high_token_usage risk when >80%. Logs failures at WARNING (non-blocking). Unit tests cover high usage, normal usage, unlimited plan, and Redis failure scenarios.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire real build_failure_count and detect_llm_risks into services</name>
  <files>
    backend/app/services/dashboard_service.py
    backend/app/services/journey.py
  </files>
  <action>
**1. Update `dashboard_service.py`** — Replace hardcoded `build_failure_count=0` with a real query:

Before the `detect_system_risks` call (around line 152), add a query:

```python
        # Count failed jobs for this project
        from sqlalchemy import func, and_
        failed_jobs_result = await session.execute(
            select(func.count(Job.id)).where(
                and_(
                    Job.project_id == project_id,
                    Job.status == "failed",
                )
            )
        )
        build_failure_count = failed_jobs_result.scalar() or 0
```

Then update the `detect_system_risks` call:
```python
        risks = detect_system_risks(
            last_gate_decision_at=last_gate_decision_at,
            build_failure_count=build_failure_count,
            last_activity_at=project.updated_at,
            now=datetime.now(timezone.utc),
        )
```

Also add detect_llm_risks call (since detect_llm_risks is now async):
```python
        # Detect LLM risks
        from app.domain.risks import detect_llm_risks
        llm_risks = await detect_llm_risks(user_id, session)

        # Combine system + LLM risks
        all_risks = risks + [
            {"type": r["type"], "rule": r["rule"], "message": r["message"]}
            for r in llm_risks
        ]
```

Then use `all_risks` instead of `risks` when building `risk_flags`.

**Add `func` and `and_` to SQLAlchemy imports** at top of file if not already present.

**2. Update `journey.py`** — Same pattern. Replace hardcoded `build_failure_count=0`:

Around line 578-584, add the same failed jobs count query:

```python
        # Count failed jobs for this project
        from sqlalchemy import func, and_
        failed_jobs_result = await self.session.execute(
            select(func.count(Job.id)).where(
                and_(
                    Job.project_id == project_id,
                    Job.status == "failed",
                )
            )
        )
        build_failure_count = failed_jobs_result.scalar() or 0
```

Update the `detect_system_risks` call to use real build_failure_count.

Update the `detect_llm_risks()` call (line 587) to pass user_id and session:
```python
        # Detect LLM risks
        llm_risks = await detect_llm_risks(user_id, self.session)
```

NOTE: journey.py needs to know user_id. Check if it's available in the method scope — it should be, since the method likely receives project_id and can get clerk_user_id from the project. If not, load it from `project.clerk_user_id`.

**Verify `Job` model is imported** in both files — dashboard_service.py already imports it (line 13). journey.py should also have it. Add `from app.db.models.job import Job` if missing.
  </action>
  <verify>
Run `cd /Users/vladcortex/co-founder/backend && python -c "from app.services.dashboard_service import DashboardService; print('Dashboard OK')"` — no import errors.
Run `cd /Users/vladcortex/co-founder/backend && python -c "from app.services.journey import JourneyService; print('Journey OK')"` — no import errors.
Grep for `build_failure_count=0` in both files — should return zero results (no hardcoded 0).
Grep for `Job.status.*failed` in both files — should find the real query in each.
  </verify>
  <done>
dashboard_service.py and journey.py both query Job.status == 'failed' for real build_failure_count. Both call detect_llm_risks with user_id and session for real LLM risk signals. No hardcoded 0 remains.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.domain.risks import detect_llm_risks"` succeeds (async function)
2. `python -m pytest tests/domain/test_risks.py -v` — all tests pass
3. `grep -c "build_failure_count=0" backend/app/services/dashboard_service.py` returns 0
4. `grep -c "build_failure_count=0" backend/app/services/journey.py` returns 0
5. `grep -c "Job.status" backend/app/services/dashboard_service.py` returns at least 1
6. `grep -c "detect_llm_risks" backend/app/services/dashboard_service.py` returns at least 1
</verification>

<success_criteria>
- detect_llm_risks returns real risk when token usage >80% of daily limit
- detect_llm_risks returns empty list on Redis failure (non-blocking)
- build_failure_count queries actual Job.status == 'failed' rows
- Risk dashboard shows real signal from Redis usage data
- Build failure count reflects actual executor failure counts
- No hardcoded 0 for build_failure_count in any service file
</success_criteria>

<output>
After completion, create `.planning/phases/13-llm-activation-and-hardening/13-06-SUMMARY.md`
</output>
