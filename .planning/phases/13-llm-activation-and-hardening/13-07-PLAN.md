---
phase: 13-llm-activation-and-hardening
plan: 07
type: execute
wave: 4
depends_on:
  - 13-03
  - 13-04
  - 13-05
  - 13-06
requirements:
  - LLM-01
  - LLM-02
  - LLM-03
  - LLM-04
  - LLM-05
  - LLM-06
  - LLM-07
  - LLM-08
  - LLM-09
  - LLM-10
  - LLM-11
  - LLM-12
  - LLM-13
  - LLM-14
  - LLM-15
files_modified:
  - backend/tests/agent/test_runner_real.py
  - backend/tests/agent/test_llm_retry.py
  - backend/tests/services/test_understanding_service_real.py
autonomous: true

must_haves:
  truths:
    - "RunnerReal.generate_understanding_questions returns valid question list when mocked LLM returns JSON"
    - "RunnerReal.generate_idea_brief returns dict with confidence_scores when mocked LLM returns JSON"
    - "RunnerReal.check_question_relevance returns dict with needs_regeneration key"
    - "RunnerReal.assess_section_confidence returns one of strong/moderate/needs_depth"
    - "RunnerReal.generate_execution_options returns dict with options array"
    - "RunnerReal.generate_artifacts returns dict with 5 artifact keys"
    - "_invoke_with_retry retries on OverloadedError and stops after max attempts"
    - "_invoke_with_retry does NOT retry on non-overload exceptions"
    - "JSON fence stripping works before json.loads in RunnerReal methods"
    - "Malformed JSON triggers one silent retry with stricter prompt"
    - "RunnerReal uses co-founder voice (no third-party analyst tone in prompts)"
    - "Tier differentiation produces different question counts per tier (bootstrapper 6-8 in prompt, cto_scale 14-16 in prompt)"
    - "All tests use mocked LLM (no real API calls)"
  artifacts:
    - path: "backend/tests/agent/test_runner_real.py"
      provides: "Tests for all 10 RunnerReal protocol methods with mocked LLM"
    - path: "backend/tests/agent/test_llm_retry.py"
      provides: "Tests for retry logic on OverloadedError"
    - path: "backend/tests/services/test_understanding_service_real.py"
      provides: "Integration-style tests for understanding service with RunnerReal"
  key_links:
    - from: "backend/tests/agent/test_runner_real.py"
      to: "backend/app/agent/runner_real.py"
      via: "Tests RunnerReal methods"
      pattern: "RunnerReal"
    - from: "backend/tests/agent/test_llm_retry.py"
      to: "backend/app/agent/llm_helpers.py"
      via: "Tests _invoke_with_retry"
      pattern: "_invoke_with_retry"
---

<objective>
Write comprehensive tests for all Phase 13 changes using mocked LLM calls.

Purpose: Verify that RunnerReal correctly calls Claude, parses responses, handles errors, retries on 529, and produces tier-differentiated output. All tests use mocked LLM (no real API calls) so they run in CI without ANTHROPIC_API_KEY.

Output: 3 test files covering RunnerReal methods, retry logic, and service integration.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/13-llm-activation-and-hardening/13-CONTEXT.md
@.planning/phases/13-llm-activation-and-hardening/13-RESEARCH.md

@backend/app/agent/runner_real.py
@backend/app/agent/llm_helpers.py
@backend/app/core/llm_config.py
@backend/tests/test_llm_helpers.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Test RunnerReal methods with mocked LLM</name>
  <files>
    backend/tests/agent/test_runner_real.py
  </files>
  <action>
Create `backend/tests/agent/test_runner_real.py` with tests for all RunnerReal methods.

**Mocking strategy:** Mock `create_tracked_llm` to return a mock LLM whose `ainvoke` returns a mock response with `.content` set to the expected JSON string.

```python
"""Tests for RunnerReal with mocked LLM calls.

All tests mock create_tracked_llm to avoid real API calls.
These verify prompt construction, response parsing, and error handling.
"""
import json
import pytest
from unittest.mock import AsyncMock, MagicMock, patch

from app.agent.runner_real import RunnerReal


def _mock_llm_response(content: str):
    """Create a mock LLM response with .content attribute."""
    response = MagicMock()
    response.content = content
    return response


def _mock_create_tracked_llm(response_content: str):
    """Create a mock create_tracked_llm that returns LLM with preset response."""
    mock_llm = AsyncMock()
    mock_llm.ainvoke = AsyncMock(return_value=_mock_llm_response(response_content))

    async def mock_factory(**kwargs):
        return mock_llm

    return mock_factory, mock_llm


@pytest.fixture
def runner():
    return RunnerReal()


class TestGenerateUnderstandingQuestions:
    @pytest.mark.asyncio
    async def test_returns_question_list(self, runner):
        questions = [
            {"id": "uq1", "text": "Who have we talked to?", "input_type": "textarea", "required": True, "options": None, "follow_up_hint": "Be specific"},
            {"id": "uq2", "text": "What's our biggest risk?", "input_type": "textarea", "required": True, "options": None, "follow_up_hint": None},
        ]
        factory, _ = _mock_create_tracked_llm(json.dumps(questions))

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            result = await runner.generate_understanding_questions({
                "idea_text": "An inventory tracker for small shops",
                "user_id": "user_123",
                "session_id": "sess_abc",
                "tier": "bootstrapper",
            })

        assert isinstance(result, list)
        assert len(result) == 2
        assert result[0]["id"] == "uq1"

    @pytest.mark.asyncio
    async def test_handles_fenced_json(self, runner):
        questions = [{"id": "uq1", "text": "Test?", "input_type": "text", "required": True, "options": None, "follow_up_hint": None}]
        fenced = f"```json\n{json.dumps(questions)}\n```"
        factory, _ = _mock_create_tracked_llm(fenced)

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            result = await runner.generate_understanding_questions({
                "idea_text": "test", "user_id": "u1", "session_id": "s1",
            })

        assert len(result) == 1


class TestGenerateIdeaBrief:
    @pytest.mark.asyncio
    async def test_returns_brief_with_confidence(self, runner):
        brief = {
            "problem_statement": "Small shops waste time on inventory",
            "target_user": "Retail shop owners",
            "value_prop": "Simple inventory tracking",
            "confidence_scores": {
                "problem_statement": "strong",
                "target_user": "moderate",
                "value_prop": "needs_depth",
            },
            "_schema_version": 1,
        }
        factory, _ = _mock_create_tracked_llm(json.dumps(brief))

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            result = await runner.generate_idea_brief(
                idea="Inventory tracker",
                questions=[{"id": "q1", "text": "Who?"}],
                answers={"q1": "Shop owners"},
            )

        assert "confidence_scores" in result
        assert result["confidence_scores"]["problem_statement"] == "strong"


class TestCheckQuestionRelevance:
    @pytest.mark.asyncio
    async def test_returns_relevance_dict(self, runner):
        relevance = {"needs_regeneration": False, "preserve_indices": [0, 1]}
        factory, _ = _mock_create_tracked_llm(json.dumps(relevance))

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            result = await runner.check_question_relevance(
                idea="Test idea",
                answered=[{"id": "q1", "text": "Q1"}],
                answers={"q1": "Answer 1"},
                remaining=[{"id": "q2", "text": "Q2"}],
            )

        assert "needs_regeneration" in result
        assert isinstance(result["preserve_indices"], list)


class TestAssessSectionConfidence:
    @pytest.mark.asyncio
    async def test_returns_strong(self, runner):
        factory, _ = _mock_create_tracked_llm("strong")

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            result = await runner.assess_section_confidence(
                "problem_statement",
                "We validated this through 12 customer interviews with specific data points.",
            )

        assert result == "strong"

    @pytest.mark.asyncio
    async def test_extracts_from_verbose_response(self, runner):
        factory, _ = _mock_create_tracked_llm("Based on the evidence, I would assess this as moderate confidence.")

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            result = await runner.assess_section_confidence("target_user", "Some vague description")

        assert result == "moderate"

    @pytest.mark.asyncio
    async def test_defaults_to_moderate(self, runner):
        factory, _ = _mock_create_tracked_llm("I'm not sure how to assess this.")

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            result = await runner.assess_section_confidence("value_prop", "Something")

        assert result == "moderate"


class TestGenerateExecutionOptions:
    @pytest.mark.asyncio
    async def test_returns_options(self, runner):
        options = {
            "options": [
                {"id": "fast-mvp", "name": "Fast MVP", "is_recommended": True, "risk_level": "low"},
                {"id": "full", "name": "Full Build", "is_recommended": False, "risk_level": "high"},
            ],
            "recommended_id": "fast-mvp",
        }
        factory, _ = _mock_create_tracked_llm(json.dumps(options))

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            result = await runner.generate_execution_options(
                brief={"problem_statement": "Test"},
            )

        assert "options" in result
        assert len(result["options"]) == 2
        assert result["recommended_id"] == "fast-mvp"


class TestGenerateArtifacts:
    @pytest.mark.asyncio
    async def test_returns_artifact_cascade(self, runner):
        artifacts = {
            "brief": {"problem_statement": "Test", "_schema_version": 1},
            "mvp_scope": {"core_features": []},
            "milestones": {"milestones": []},
            "risk_log": {"technical_risks": []},
            "how_it_works": {"user_journey": []},
        }
        factory, _ = _mock_create_tracked_llm(json.dumps(artifacts))

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            result = await runner.generate_artifacts(
                brief={"problem_statement": "Test"},
            )

        assert "brief" in result
        assert "mvp_scope" in result
        assert "milestones" in result
        assert "risk_log" in result
        assert "how_it_works" in result


class TestJsonRetryOnMalformedOutput:
    @pytest.mark.asyncio
    async def test_retries_with_strict_prompt_on_bad_json(self, runner):
        """First call returns bad JSON, second call returns valid JSON."""
        bad_response = _mock_llm_response("Here are the questions:\n{invalid json}")
        good_response = _mock_llm_response('[{"id": "q1", "text": "Test?", "input_type": "text", "required": true, "options": null, "follow_up_hint": null}]')

        mock_llm = AsyncMock()
        mock_llm.ainvoke = AsyncMock(side_effect=[bad_response, good_response])

        async def mock_factory(**kwargs):
            return mock_llm

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=mock_factory):
            result = await runner.generate_understanding_questions({
                "idea_text": "test", "user_id": "u1", "session_id": "s1",
            })

        assert len(result) == 1
        assert mock_llm.ainvoke.call_count == 2


class TestTierDifferentiation:
    @pytest.mark.asyncio
    async def test_bootstrapper_gets_6_8_questions(self, runner):
        """Bootstrapper tier produces 6-8 question count in prompt."""
        questions = [{"id": f"uq{i}", "text": f"Q{i}?", "input_type": "textarea", "required": True, "options": None, "follow_up_hint": None} for i in range(7)]
        factory, mock_llm = _mock_create_tracked_llm(json.dumps(questions))

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            await runner.generate_understanding_questions({
                "idea_text": "test", "user_id": "u1", "session_id": "s1",
                "tier": "bootstrapper",
            })

        call_args = mock_llm.ainvoke.call_args[0][0]
        system_content = call_args[0].content
        assert "6-8" in system_content

    @pytest.mark.asyncio
    async def test_cto_scale_gets_14_16_questions(self, runner):
        """cto_scale tier produces 14-16 question count in prompt."""
        questions = [{"id": f"uq{i}", "text": f"Q{i}?", "input_type": "textarea", "required": True, "options": None, "follow_up_hint": None} for i in range(15)]
        factory, mock_llm = _mock_create_tracked_llm(json.dumps(questions))

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            await runner.generate_understanding_questions({
                "idea_text": "test", "user_id": "u1", "session_id": "s1",
                "tier": "cto_scale",
            })

        call_args = mock_llm.ainvoke.call_args[0][0]
        system_content = call_args[0].content
        assert "14-16" in system_content


class TestCofounderVoice:
    @pytest.mark.asyncio
    async def test_system_prompt_uses_we_voice(self, runner):
        """Verify system prompts contain co-founder voice markers."""
        factory, mock_llm = _mock_create_tracked_llm('[]')

        with patch("app.agent.runner_real.create_tracked_llm", side_effect=factory):
            await runner.generate_understanding_questions({
                "idea_text": "test", "user_id": "u1", "session_id": "s1",
            })

        # Check the system message content
        call_args = mock_llm.ainvoke.call_args[0][0]  # first positional arg (messages list)
        system_content = call_args[0].content  # first message is system
        assert "co-founder" in system_content.lower() or "we" in system_content.lower()
```

Make sure `backend/tests/agent/__init__.py` exists (create empty file if not).
  </action>
  <verify>
Run `cd /Users/vladcortex/co-founder/backend && python -m pytest tests/agent/test_runner_real.py -v` — all tests pass.
  </verify>
  <done>
Comprehensive RunnerReal tests cover all 10 protocol methods with mocked LLM. Tests verify JSON parsing, fence stripping, retry on malformed JSON, confidence extraction, co-founder voice, and tier differentiation. All tests use mocked LLM (no API calls).
  </done>
</task>

<task type="auto">
  <name>Task 2: Test retry logic on OverloadedError</name>
  <files>
    backend/tests/agent/test_llm_retry.py
  </files>
  <action>
Create `backend/tests/agent/test_llm_retry.py` with tests specifically for the retry behavior:

```python
"""Tests for LLM retry logic with OverloadedError."""
import pytest
from unittest.mock import AsyncMock, MagicMock

from anthropic._exceptions import OverloadedError
from tenacity import wait_none
from app.agent.llm_helpers import _invoke_with_retry


def _make_overloaded_error():
    """Create a realistic OverloadedError instance."""
    response = MagicMock()
    response.status_code = 529
    response.headers = {}
    response.text = "Overloaded"
    return OverloadedError.construct(
        message="Overloaded",
        response=response,
        body=None,
    )


class TestInvokeWithRetry:
    @pytest.mark.asyncio
    async def test_success_on_first_try(self):
        """Normal call succeeds without retry."""
        mock_llm = AsyncMock()
        response = MagicMock()
        response.content = "OK"
        mock_llm.ainvoke = AsyncMock(return_value=response)

        result = await _invoke_with_retry(mock_llm, [])

        assert result.content == "OK"
        assert mock_llm.ainvoke.call_count == 1

    @pytest.mark.asyncio
    async def test_retries_on_overloaded(self):
        """Retries on OverloadedError and succeeds on second attempt."""
        response = MagicMock()
        response.content = "OK"

        mock_llm = AsyncMock()
        mock_llm.ainvoke = AsyncMock(
            side_effect=[_make_overloaded_error(), response]
        )

        result = await _invoke_with_retry.retry_with(
            wait=wait_none()  # disable wait for faster tests
        )(mock_llm, [])

        assert result.content == "OK"
        assert mock_llm.ainvoke.call_count == 2

    @pytest.mark.asyncio
    async def test_does_not_retry_on_other_errors(self):
        """Non-overload exceptions propagate immediately without retry."""
        mock_llm = AsyncMock()
        mock_llm.ainvoke = AsyncMock(side_effect=ValueError("Bad input"))

        with pytest.raises(ValueError, match="Bad input"):
            await _invoke_with_retry(mock_llm, [])

        assert mock_llm.ainvoke.call_count == 1

    @pytest.mark.asyncio
    async def test_exhausted_retries_reraise(self):
        """After max retries, OverloadedError is re-raised."""
        mock_llm = AsyncMock()
        mock_llm.ainvoke = AsyncMock(side_effect=_make_overloaded_error())

        with pytest.raises(OverloadedError):
            await _invoke_with_retry.retry_with(
                wait=wait_none()  # disable wait for faster tests
            )(mock_llm, [])

        assert mock_llm.ainvoke.call_count == 4  # 1 original + 3 retries
```

NOTE: The `_make_overloaded_error()` helper needs to create a valid OverloadedError instance. The exact constructor may need adjustment based on the anthropic library version. Check the actual constructor signature:
- If `OverloadedError.__init__` takes `message` and `response`, use that.
- If it takes different args, inspect `anthropic._exceptions.OverloadedError.__init__` and adjust.
- A simpler approach: create a MagicMock that looks like OverloadedError for retry matching. But tenacity's `retry_if_exception_type` checks `isinstance`, so we need a real instance.

Alternative for creating OverloadedError:
```python
import httpx
response = httpx.Response(status_code=529, text="Overloaded")
error = OverloadedError(message="Overloaded", response=response, body=None)
```

Try the httpx approach first — the anthropic library uses httpx internally.
  </action>
  <verify>
Run `cd /Users/vladcortex/co-founder/backend && python -m pytest tests/agent/test_llm_retry.py -v` — all tests pass.
  </verify>
  <done>
Retry tests verify: success on first try, retry on OverloadedError, no retry on other exceptions, and exhausted retries re-raise. All tests use mocked LLM with no real API calls.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/agent/test_runner_real.py -v` — all tests pass
2. `python -m pytest tests/agent/test_llm_retry.py -v` — all tests pass
3. `python -m pytest tests/test_llm_helpers.py -v` — fence stripping tests pass
4. `python -m pytest tests/domain/test_risks.py -v` — risk detection tests pass
5. No test calls real API (all mocked)
</verification>

<success_criteria>
- All RunnerReal methods tested with mocked LLM responses
- Retry logic tested: success, retry-then-success, no-retry-on-other-errors, exhausted-retries
- JSON fence stripping tested in RunnerReal context
- Malformed JSON retry tested (first attempt fails, second succeeds)
- Co-founder voice verified in system prompts
- Tier differentiation tested (different question counts)
- All tests run without ANTHROPIC_API_KEY (mocked)
</success_criteria>

<output>
After completion, create `.planning/phases/13-llm-activation-and-hardening/13-07-SUMMARY.md`
</output>
