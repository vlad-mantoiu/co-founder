---
phase: 13-llm-activation-and-hardening
plan: 03
type: execute
wave: 2
depends_on:
  - 13-01
requirements:
  - LLM-01
  - LLM-02
  - LLM-03
  - LLM-04
  - LLM-05
  - LLM-06
  - LLM-14
files_modified:
  - backend/app/agent/runner_real.py
autonomous: true

must_haves:
  truths:
    - "RunnerReal.generate_understanding_questions() calls Claude via create_tracked_llm and returns dynamically generated questions"
    - "RunnerReal.generate_idea_brief() calls Claude and returns a Rationalised Idea Brief with per-section confidence_scores"
    - "RunnerReal.check_question_relevance() calls Claude to assess if remaining questions need regeneration after an answer edit"
    - "RunnerReal.assess_section_confidence() calls Claude to classify section as strong/moderate/needs_depth"
    - "RunnerReal.generate_execution_options() calls Claude to generate 2-3 execution plan options with engineering impact"
    - "RunnerReal.generate_artifacts() calls Claude with co-founder voice to generate 5 artifact types"
    - "All prompts use co-founder 'we' voice — no third-party analyst tone"
    - "All methods use _invoke_with_retry for OverloadedError retry"
    - "All methods use _parse_json_response for fence-stripping before json.loads"
    - "Malformed JSON triggers one silent retry with stricter prompt hint"
    - "All methods accept context dict with user_id, session_id, tier keys"
  artifacts:
    - path: "backend/app/agent/runner_real.py"
      provides: "Complete RunnerReal with all 10 protocol methods implemented"
      contains: "_invoke_with_retry"
    - path: "backend/app/agent/runner_real.py"
      provides: "Co-founder voice system prompts"
      contains: "AI co-founder"
    - path: "backend/app/agent/runner_real.py"
      provides: "All 6 previously missing methods"
      contains: "generate_understanding_questions"
  key_links:
    - from: "backend/app/agent/runner_real.py"
      to: "backend/app/agent/llm_helpers.py"
      via: "Import retry and parsing utilities"
      pattern: "from app.agent.llm_helpers import"
    - from: "backend/app/agent/runner_real.py"
      to: "backend/app/core/llm_config.py"
      via: "create_tracked_llm for model resolution and usage tracking"
      pattern: "create_tracked_llm"
---

<objective>
Implement all 6 missing RunnerReal methods and rewrite existing methods with co-founder voice prompts, retry logic, and fence-stripping.

Purpose: This is the core deliverable of Phase 13. Founders currently receive fake inventory-tracker stubs from RunnerFake. After this plan, RunnerReal generates dynamically tailored interview questions, Rationalised Idea Briefs with confidence scores, relevance checks, execution plan options, and full artifact cascades — all from real Claude calls with proper co-founder tone.

Output: RunnerReal with all 10 protocol methods fully implemented, using co-founder voice, retry on 529, and fence-stripping.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/13-llm-activation-and-hardening/13-CONTEXT.md
@.planning/phases/13-llm-activation-and-hardening/13-RESEARCH.md

@backend/app/agent/runner.py
@backend/app/agent/runner_real.py
@backend/app/agent/runner_fake.py
@backend/app/agent/llm_helpers.py
@backend/app/core/llm_config.py
@backend/app/schemas/understanding.py
@backend/app/schemas/artifacts.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite RunnerReal with complete protocol implementation</name>
  <files>
    backend/app/agent/runner_real.py
  </files>
  <action>
Completely rewrite `backend/app/agent/runner_real.py` to implement all 10 Runner protocol methods. The file should have:

**Module-level setup:**
```python
"""RunnerReal: Production implementation of the Runner protocol wrapping LangGraph.

Implements all 10 Runner protocol methods with:
- Real Claude LLM calls via create_tracked_llm()
- Tenacity retry on 529 OverloadedError
- Markdown fence stripping before JSON parsing
- Co-founder "we" voice in all prompts
- Silent JSON retry with stricter prompt on first parse failure
"""

import json
import logging

from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.checkpoint.memory import MemorySaver

from app.agent.graph import create_cofounder_graph
from app.agent.llm_helpers import _invoke_with_retry, _parse_json_response
from app.agent.nodes import (
    architect_node,
    coder_node,
    debugger_node,
    executor_node,
    git_manager_node,
    reviewer_node,
)
from app.agent.state import CoFounderState
from app.core.llm_config import create_tracked_llm

logger = logging.getLogger(__name__)
```

**Co-founder system prompt constant:**
```python
COFOUNDER_SYSTEM = """You are the founder's AI co-founder — a senior technical partner invested in their success.

Your voice:
- Use "we" for shared decisions ("We should consider...", "Our biggest risk here is...")
- Use "your" for the founder's vision ("Your target customer...", "Your core insight...")
- Use "I'd suggest" for technical recommendations
- Validate first, then guide: "That's a solid instinct. One thing we should stress-test is..."
- Plain English only — no jargon. The founder should never need to Google a term.
- Never condescending. Smart, warm, direct.

{task_instructions}"""
```

**RunnerReal class** — preserve existing `__init__`, `run`, and `step` methods. Replace `generate_questions`, `generate_brief`, and `generate_artifacts` with improved versions. Add all 6 missing methods.

**Each method follows this pattern:**
1. Extract `user_id`, `session_id`, `tier` from context/arguments
2. Call `create_tracked_llm(user_id=user_id, role="architect", session_id=session_id)`
3. Build system prompt with co-founder voice + task-specific instructions
4. Build human message with founder's context
5. Call `await _invoke_with_retry(llm, [system_msg, human_msg])`
6. Parse with `_parse_json_response(response.content)`
7. On `json.JSONDecodeError`, retry once with stricter prompt
8. On second failure, raise `RuntimeError` with descriptive message

**Method implementations:**

### generate_questions(context: dict) -> list[dict]
System prompt: Co-founder generating onboarding questions. "Generate 5-7 questions that help us understand the founder's idea. Use 'we' language throughout."
JSON return format: array of `{"id": "q1", "text": "...", "required": true}`

### generate_brief(answers: dict) -> dict
System prompt: Co-founder converting answers into a structured product brief.
JSON return format: object with problem_statement, target_user, value_prop, differentiation, monetization_hypothesis, assumptions (array), risks (array), smallest_viable_experiment.

### generate_understanding_questions(context: dict) -> list[dict]
Extract tier from context (default "bootstrapper").
Tier-based question count: bootstrapper=6-8, partner=10-12, cto_scale=14-16.
System prompt:
```
Generate {question_count} understanding interview questions about the founder's idea.
These go deeper than initial onboarding — probe market validation, competitive landscape,
monetization details, risk awareness, and smallest viable experiment.

Use "we" language: "Who have we talked to...", "What's our biggest risk...", "How will we make money..."

Return ONLY a JSON array of objects:
[
  {
    "id": "uq1",
    "text": "...",
    "input_type": "textarea",
    "required": true,
    "options": null,
    "follow_up_hint": "..."
  }
]

End the interview with a closing question like: "I have enough to build your brief. Want to add anything else before I do?"
```
Human message: `"Idea: {idea_text}\n\nOnboarding answers: {onboarding_answers}"`

### generate_idea_brief(idea: str, questions: list[dict], answers: dict) -> dict
System prompt:
```
Generate a Rationalised Idea Brief from the founder's understanding interview.

Use "we" voice throughout: "We've identified...", "Our target user is...", "The risk here is..."
Plain English — no jargon. A non-technical founder should read this without Googling anything.

Return ONLY a JSON object with these fields:
{
  "problem_statement": "...",
  "target_user": "...",
  "value_prop": "...",
  "differentiation": "...",
  "monetization_hypothesis": "...",
  "market_context": "...",
  "key_constraints": ["..."],
  "assumptions": ["..."],
  "risks": ["..."],
  "smallest_viable_experiment": "...",
  "confidence_scores": {
    "problem_statement": "strong|moderate|needs_depth",
    "target_user": "strong|moderate|needs_depth",
    ...
  },
  "_schema_version": 1
}

For confidence_scores, assess each section as:
- "strong": Backed by specific evidence, customer interviews, or data from the founder's answers
- "moderate": Reasonable hypothesis but not yet validated with real data
- "needs_depth": Vague or missing — the founder should revisit this section
```
Human message: Idea text + formatted Q&A pairs.

### check_question_relevance(idea, answered, answers, remaining) -> dict
System prompt:
```
The founder has edited an answer in their understanding interview.
Review the remaining questions and determine if any are now irrelevant or if new questions are needed.

Return ONLY a JSON object:
{
  "needs_regeneration": true/false,
  "preserve_indices": [0, 2],  // indices of remaining questions to keep
  "new_questions": [...]  // optional: 1-2 new questions based on the changed answer
}
```
Human message: Idea, answered questions with answers, remaining questions.

### assess_section_confidence(section_key: str, content: str) -> str
System prompt:
```
Assess the confidence level of this Idea Brief section.

Return ONLY one of: "strong", "moderate", "needs_depth"

- "strong": Specific evidence, data points, named customers, or quantified metrics
- "moderate": Reasonable hypothesis, some supporting logic, but unvalidated
- "needs_depth": Vague, generic, or missing critical detail
```
Human message: `"Section: {section_key}\n\nContent: {content}"`
Parse response: strip whitespace, extract the keyword. Handle cases where Claude returns extra text by checking if response contains the keyword.

### generate_execution_options(brief: dict, feedback: str | None = None) -> dict
Extract tier from brief `_context` or default to "bootstrapper".
System prompt:
```
Generate 2-3 execution plan options based on the Rationalised Idea Brief.
Each option represents a different approach to building the MVP.

Use "we" voice: "We'll focus on...", "Our approach here is..."

For each option include:
- id, name, is_recommended (exactly one), time_to_ship, engineering_cost
- risk_level (low/medium/high), scope_coverage (0-100 percent)
- pros (array), cons (array)
- technical_approach, tradeoffs (array), engineering_impact

Return ONLY a JSON object:
{
  "options": [...],
  "recommended_id": "..."
}
```
If feedback is provided, add it as context: "The founder gave this feedback on previous options: {feedback}"

### generate_artifacts(brief: dict) -> dict
System prompt:
```
Generate a complete set of project artifacts from the product brief.
These artifacts are the founder's project documentation — they'll share these with advisors,
investors, and team members.

Use "we" voice throughout. Plain English — no jargon.

Return ONLY a JSON object with these 5 keys:
{
  "brief": { ... full product brief with _schema_version: 1 ... },
  "mvp_scope": { ... core_features, out_of_scope, success_metrics ... },
  "milestones": { ... milestones array with title, description, success_criteria, estimated_weeks ... },
  "risk_log": { ... technical_risks, market_risks, execution_risks ... },
  "how_it_works": { ... user_journey, architecture, data_flow ... }
}

Each artifact should cross-reference others (e.g., milestones reference MVP features,
risk log references brief assumptions).
```

**JSON retry pattern for ALL methods:**
```python
try:
    response = await _invoke_with_retry(llm, [system_msg, human_msg])
    return _parse_json_response(response.content)
except json.JSONDecodeError:
    logger.warning("RunnerReal.%s: JSON parse failed, retrying with strict prompt", method_name)
    strict_system = SystemMessage(
        content="IMPORTANT: Your response MUST be valid JSON only. "
        "Do not include any explanation, markdown, or code fences. "
        "Start your response with { or [.\n\n" + system_msg.content
    )
    response = await _invoke_with_retry(llm, [strict_system, human_msg])
    return _parse_json_response(response.content)
```

For `assess_section_confidence`, the return is a plain string, not JSON:
```python
response = await _invoke_with_retry(llm, [system_msg, human_msg])
text = response.content.strip().lower()
for level in ("strong", "moderate", "needs_depth"):
    if level in text:
        return level
return "moderate"  # safe default
```
  </action>
  <verify>
Run `cd /Users/vladcortex/co-founder/backend && python -c "
from app.agent.runner_real import RunnerReal
r = RunnerReal()
# Verify all 10 protocol methods exist
methods = ['run', 'step', 'generate_questions', 'generate_brief', 'generate_artifacts',
           'generate_understanding_questions', 'generate_idea_brief', 'check_question_relevance',
           'assess_section_confidence', 'generate_execution_options']
for m in methods:
    assert hasattr(r, m), f'Missing method: {m}'
    assert callable(getattr(r, m)), f'Not callable: {m}'
print(f'All {len(methods)} methods present')
"` — prints "All 10 methods present".

Run `cd /Users/vladcortex/co-founder/backend && python -c "from app.agent.runner import Runner; from app.agent.runner_real import RunnerReal; assert isinstance(RunnerReal(), Runner); print('Protocol satisfied')"` — prints "Protocol satisfied".

Grep for `except: pass` in runner_real.py — should return zero results (no silent swallowing).
Grep for `_invoke_with_retry` in runner_real.py — should find it in every LLM-calling method.
Grep for `expert product strategist` in runner_real.py — should return zero results (old third-party tone removed).
  </verify>
  <done>
RunnerReal implements all 10 Runner protocol methods with real Claude calls. All prompts use co-founder "we" voice. All LLM calls use _invoke_with_retry for 529 retry. All JSON parsing uses _parse_json_response with fence stripping. Malformed JSON triggers one silent retry with stricter prompt. No bare except:pass blocks. No third-party analyst tone.
  </done>
</task>

</tasks>

<verification>
1. RunnerReal has all 10 protocol methods (run, step, generate_questions, generate_brief, generate_artifacts, generate_understanding_questions, generate_idea_brief, check_question_relevance, assess_section_confidence, generate_execution_options)
2. `isinstance(RunnerReal(), Runner)` returns True
3. All methods use `_invoke_with_retry` for LLM calls
4. All methods use `_parse_json_response` for JSON output
5. No `except: pass` in the file
6. No "expert product strategist" or third-party analyst language
7. All prompts contain "co-founder" or "we" voice markers
</verification>

<success_criteria>
- A founder submitting a real idea receives dynamically tailored interview questions from Claude, not inventory-tracker boilerplate
- The generated Idea Brief contains confidence scores per section derived from real Claude analysis
- All RunnerReal methods use co-founder "we" voice consistently
- 529 overload triggers silent retry with exponential backoff
- Malformed JSON triggers one silent retry with stricter prompt
- All methods accept tier/user_id/session_id context for tracking
</success_criteria>

<output>
After completion, create `.planning/phases/13-llm-activation-and-hardening/13-03-SUMMARY.md`
</output>
