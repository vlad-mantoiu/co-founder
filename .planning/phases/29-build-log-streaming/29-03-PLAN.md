---
phase: 29-build-log-streaming
plan: 03
type: execute
wave: 2
depends_on:
  - 29-01
files_modified:
  - backend/app/sandbox/e2b_runtime.py
  - backend/app/services/generation_service.py
  - backend/app/queue/worker.py
  - backend/app/core/config.py
  - backend/tests/services/test_log_streaming_integration.py
autonomous: true
requirements:
  - BUILD-01

must_haves:
  truths:
    - "run_command() and run_background() accept optional on_stdout and on_stderr callback params and forward them to E2B commands"
    - "GenerationService.execute_build() creates LogStreamer(job_id, phase) and passes streamer.on_stdout/on_stderr as callbacks to sandbox run_command() and start_dev_server() calls"
    - "Stage-change events (system log lines) are emitted at each build stage transition"
    - "streamer.flush() is called in finally block to capture last buffered lines even on failure"
    - "S3 archival runs after build completion (terminal state) and is non-fatal"
    - "LOG_ARCHIVE_BUCKET config setting controls S3 archival (empty = skip)"
  artifacts:
    - path: "backend/app/sandbox/e2b_runtime.py"
      provides: "run_command and run_background with optional log streaming"
      contains: "on_stdout"
    - path: "backend/app/services/generation_service.py"
      provides: "LogStreamer integration in execute_build pipeline"
      contains: "LogStreamer"
    - path: "backend/app/queue/worker.py"
      provides: "S3 archival after terminal state"
      contains: "_archive_logs_to_s3"
    - path: "backend/app/core/config.py"
      provides: "log_archive_bucket setting"
      contains: "log_archive_bucket"
    - path: "backend/tests/services/test_log_streaming_integration.py"
      provides: "Integration tests for log streaming through build pipeline"
      min_lines: 60
  key_links:
    - from: "backend/app/services/generation_service.py"
      to: "backend/app/services/log_streamer.py"
      via: "LogStreamer instantiation and callback passing"
      pattern: "LogStreamer"
    - from: "backend/app/sandbox/e2b_runtime.py"
      to: "backend/app/services/log_streamer.py"
      via: "on_stdout/on_stderr callback assignment"
      pattern: "on_stdout.*on_stderr"
    - from: "backend/app/queue/worker.py"
      to: "boto3 S3 put_object"
      via: "_archive_logs_to_s3 function"
      pattern: "s3.*put_object"
---

<objective>
Wire LogStreamer into the existing build pipeline: E2BSandboxRuntime commands get log streaming callbacks, GenerationService orchestrates phase-tagged streaming with stage-change events, and the worker archives logs to S3 after build completion.

Purpose: This plan connects the LogStreamer (Plan 01) and endpoints (Plan 02) to the actual build pipeline. Without this wiring, logs are never captured. This is the integration layer that makes BUILD-01 real.

Output: Modified `e2b_runtime.py`, `generation_service.py`, `worker.py`, `config.py` + integration tests.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-build-log-streaming/29-RESEARCH.md
@.planning/phases/29-build-log-streaming/29-01-SUMMARY.md
@backend/app/sandbox/e2b_runtime.py
@backend/app/services/generation_service.py
@backend/app/queue/worker.py
@backend/app/core/config.py
@backend/app/db/redis.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add log streaming to E2BSandboxRuntime and GenerationService</name>
  <files>backend/app/sandbox/e2b_runtime.py, backend/app/services/generation_service.py, backend/app/core/config.py</files>
  <action>
    **1. Modify `e2b_runtime.py` — Add optional log streaming to run_command() and run_background():**

    Update `run_command()` signature to accept optional `on_stdout` and `on_stderr` callback parameters (type: `Callable | None = None`). When provided, pass them to `self._sandbox.commands.run()`. This keeps the runtime agnostic to LogStreamer — the caller (GenerationService) creates the streamer and passes its callbacks. Do NOT import LogStreamer in e2b_runtime.py.

    ```python
    async def run_command(
        self,
        command: str,
        timeout: int = 120,
        cwd: str | None = None,
        on_stdout=None,  # Optional[Callable[[str], Awaitable[None]]]
        on_stderr=None,  # Optional[Callable[[str], Awaitable[None]]]
    ) -> dict:
    ```

    Pass `on_stdout=on_stdout, on_stderr=on_stderr` to `self._sandbox.commands.run()`. These are None-safe — E2B ignores None callbacks.

    Similarly update `run_background()` to accept and forward `on_stdout` and `on_stderr`.

    Similarly update `start_dev_server()` — it calls `self.run_command("npm install", ...)` and `self.run_background(start_cmd, ...)` internally. Add `on_stdout` and `on_stderr` params and forward them to both internal calls. This allows GenerationService to pass streamer callbacks through the entire dev server startup flow.

    **2. Modify `generation_service.py` — Integrate LogStreamer into execute_build():**

    Add LogStreamer creation and wiring in `execute_build()`:
    - After sandbox.start(), import and create LogStreamer: `from app.services.log_streamer import LogStreamer` and `from app.db.redis import get_redis`
    - `streamer = LogStreamer(redis=get_redis(), job_id=job_id, phase="scaffold")`
    - Before each stage transition, emit a stage-change system event: `await streamer.write_event(f"--- {stage_label} ---")`
    - Update phase on the streamer before each stage: `streamer._phase = "code"`, `streamer._phase = "install"`, etc.
    - Pass `on_stdout=streamer.on_stdout, on_stderr=streamer.on_stderr` to:
      - `sandbox.run_command("echo 'health-check-ok'", ...)`
      - `sandbox.start_dev_server(workspace_path=..., working_files=..., on_stdout=..., on_stderr=...)`
    - In the exception handler (except block), call `await streamer.flush()` and `await streamer.write_event(f"BUILD FAILED: {_friendly_message(exc)}", source="system")` (non-fatal, wrap in try/except)
    - In a finally block after all operations, call `await streamer.flush()` to drain any remaining buffered content

    Apply the same pattern to `execute_iteration_build()` — same streamer creation, same stage events, same callback passing, same flush in finally.

    **3. Modify `config.py` — Add LOG_ARCHIVE_BUCKET setting:**

    Add to `Settings` class:
    ```python
    # Build log archival
    log_archive_bucket: str = ""
    ```

    Empty string = skip archival (opt-in). Set via `LOG_ARCHIVE_BUCKET` env var.
  </action>
  <verify>
    ```bash
    cd /Users/vladcortex/co-founder/backend && python -c "
    from app.sandbox.e2b_runtime import E2BSandboxRuntime
    import inspect
    sig = inspect.signature(E2BSandboxRuntime.run_command)
    print('run_command params:', list(sig.parameters.keys()))
    sig2 = inspect.signature(E2BSandboxRuntime.run_background)
    print('run_background params:', list(sig2.parameters.keys()))
    sig3 = inspect.signature(E2BSandboxRuntime.start_dev_server)
    print('start_dev_server params:', list(sig3.parameters.keys()))
    from app.core.config import Settings
    s = Settings()
    print('log_archive_bucket:', repr(s.log_archive_bucket))
    "
    ```
    All signatures include on_stdout, on_stderr. log_archive_bucket defaults to empty string.
  </verify>
  <done>
    run_command(), run_background(), and start_dev_server() accept on_stdout/on_stderr callbacks.
    GenerationService creates LogStreamer, emits stage events, passes callbacks, flushes in finally.
    config.py has log_archive_bucket setting.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add S3 archival to worker and integration tests</name>
  <files>backend/app/queue/worker.py, backend/tests/services/test_log_streaming_integration.py</files>
  <action>
    **1. Add `_archive_logs_to_s3()` to `worker.py`:**

    Add an async function `_archive_logs_to_s3(job_id: str, redis)` after `_persist_job_to_postgres()`:
    - Read `log_archive_bucket` from settings — if empty, return immediately (skip archival)
    - Read all entries from Redis Stream `job:{job_id}:logs` via `redis.xrange(stream_key)`
    - If no entries, return
    - Format as newline-delimited JSON (one JSON object per line, fields: id, ts, source, text, phase)
    - Upload to S3 via `boto3.client("s3", region_name="us-east-1").put_object(Bucket=..., Key=f"build-logs/{job_id}/build.jsonl", Body=..., ContentType="application/x-ndjson")`
    - Wrap entire function in try/except — on any failure, `logger.warning("build_log_archive_failed", ...)` and return (non-fatal)

    **2. Call `_archive_logs_to_s3()` from `process_next_job()`:**

    In the try block, after `_persist_job_to_postgres(...)` for READY path, add:
    ```python
    await _archive_logs_to_s3(job_id, redis)
    ```

    In the except block, after `_persist_job_to_postgres(...)` for FAILED path, add:
    ```python
    await _archive_logs_to_s3(job_id, redis)
    ```

    Both calls are after Postgres persistence so a failure in S3 archival doesn't affect job completion.

    **3. Create integration tests at `backend/tests/services/test_log_streaming_integration.py`:**

    Tests that verify the full pipeline without real E2B or S3:

    1. `test_run_command_with_callbacks` — Create fakeredis, create LogStreamer, call streamer.on_stdout("line1\nline2\n"), flush, verify 2 entries in Redis Stream via xrange
    2. `test_generation_service_creates_stage_events` — Mock sandbox + runner, run execute_build with real fakeredis, verify Redis Stream contains system stage-change entries like "--- Starting generation pipeline ---"
    3. `test_archive_logs_to_s3_success` — Seed Redis stream entries, mock boto3.client, call _archive_logs_to_s3, verify put_object called with correct bucket/key/body
    4. `test_archive_logs_to_s3_skip_when_no_bucket` — Set log_archive_bucket="" in settings, call _archive_logs_to_s3, verify no boto3 calls
    5. `test_archive_logs_to_s3_nonfatal_on_error` — Mock boto3 to raise, call _archive_logs_to_s3, verify no exception propagated

    Use the `FakeSandboxRuntime` pattern from `tests/services/test_generation_service.py`. Extend it to accept and call on_stdout/on_stderr callbacks in its run_command/start_dev_server methods so that the LogStreamer receives data.
  </action>
  <verify>
    ```bash
    cd /Users/vladcortex/co-founder/backend && python -m pytest tests/services/test_log_streaming_integration.py -v
    ```
    All 5 tests pass.
  </verify>
  <done>
    _archive_logs_to_s3 exists in worker.py, called after terminal state persistence.
    Integration tests verify LogStreamer → Redis Stream pipeline, stage events in execute_build, and S3 archival behavior.
    All tests pass.
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/vladcortex/co-founder/backend && python -m pytest tests/services/test_log_streamer.py tests/api/test_logs_api.py tests/services/test_log_streaming_integration.py tests/services/test_generation_service.py -v
```
All tests pass (including existing generation_service tests — no regressions).
</verification>

<success_criteria>
- E2BSandboxRuntime.run_command(), run_background(), start_dev_server() accept optional on_stdout/on_stderr
- GenerationService.execute_build() creates LogStreamer, emits stage events, passes callbacks, flushes in finally
- worker.py archives logs to S3 after terminal state (non-fatal, skips if no bucket configured)
- config.py has log_archive_bucket setting
- All new and existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/29-build-log-streaming/29-03-SUMMARY.md`
</output>
