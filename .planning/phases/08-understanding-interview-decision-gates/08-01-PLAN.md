---
phase: 08-understanding-interview-decision-gates
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/agent/runner.py
  - backend/app/agent/runner_fake.py
  - backend/app/schemas/understanding.py
  - backend/app/services/understanding_service.py
  - backend/app/api/routes/understanding.py
  - backend/app/api/routes/__init__.py
  - backend/tests/api/test_understanding_api.py
autonomous: true

must_haves:
  truths:
    - "Start understanding session returns 5-7 adaptive questions one at a time"
    - "Submitting an answer returns the next adaptive question"
    - "Editing a previous answer triggers selective question re-adaptation"
    - "Completing all questions generates a Rationalised Idea Brief with per-section confidence"
    - "Brief is stored as an Artifact with JSONB versioning"
    - "Inline editing updates brief section and recalculates confidence"
    - "LLM failures return friendly error with debug_id"
    - "User isolation enforced on all endpoints"
  artifacts:
    - path: "backend/app/schemas/understanding.py"
      provides: "RationalisedIdeaBrief Pydantic schema, question/answer schemas, confidence types"
      exports: ["RationalisedIdeaBrief", "UnderstandingQuestion", "UnderstandingAnswer", "StartUnderstandingRequest", "StartUnderstandingResponse", "SubmitAnswerResponse", "EditAnswerRequest"]
    - path: "backend/app/agent/runner.py"
      provides: "Extended Runner protocol with understanding interview methods"
      exports: ["Runner"]
    - path: "backend/app/agent/runner_fake.py"
      provides: "RunnerFake with adaptive question and idea brief generation"
      exports: ["RunnerFake"]
    - path: "backend/app/services/understanding_service.py"
      provides: "UnderstandingService with session management, answer submission, brief generation"
      exports: ["UnderstandingService"]
    - path: "backend/app/api/routes/understanding.py"
      provides: "Understanding interview REST endpoints"
      exports: ["router"]
    - path: "backend/tests/api/test_understanding_api.py"
      provides: "Integration tests for understanding API"
  key_links:
    - from: "backend/app/api/routes/understanding.py"
      to: "backend/app/services/understanding_service.py"
      via: "UnderstandingService instantiation"
      pattern: "UnderstandingService\\(.*runner.*session_factory"
    - from: "backend/app/services/understanding_service.py"
      to: "backend/app/agent/runner.py"
      via: "Runner protocol calls"
      pattern: "runner\\.generate_understanding_questions|runner\\.generate_idea_brief|runner\\.assess_section_confidence"
    - from: "backend/app/services/understanding_service.py"
      to: "backend/app/db/models/artifact.py"
      via: "Artifact model for brief storage"
      pattern: "Artifact\\("
---

<objective>
Build the complete Understanding Interview backend: adaptive questioning, Rationalised Idea Brief generation, inline editing with confidence recalculation, and comprehensive API.

Purpose: The understanding interview deepens the founder's idea exploration beyond initial onboarding, producing an investor-quality Idea Brief that feeds into Decision Gate 1. This is the critical data-gathering phase that determines whether the idea is ready to build.

Output: Runner protocol extensions, Pydantic schemas, UnderstandingService, 8 REST endpoints, integration tests
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-understanding-interview-decision-gates/08-CONTEXT.md
@.planning/phases/08-understanding-interview-decision-gates/08-RESEARCH.md

@backend/app/agent/runner.py
@backend/app/agent/runner_fake.py
@backend/app/services/onboarding_service.py
@backend/app/db/models/onboarding_session.py
@backend/app/db/models/artifact.py
@backend/app/schemas/artifacts.py
@backend/app/api/routes/onboarding.py
@backend/app/api/routes/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Runner Protocol Extension + Pydantic Schemas + RunnerFake</name>
  <files>
    backend/app/agent/runner.py
    backend/app/agent/runner_fake.py
    backend/app/schemas/understanding.py
  </files>
  <action>
1. Extend Runner protocol in `runner.py` with 4 new methods:
   - `generate_understanding_questions(context: dict) -> list[dict]` — Generate adaptive understanding questions (5-7) based on idea + prior answers. Context includes idea_text, answered_questions, answers. Returns list of question dicts with id, text, input_type, required, options, follow_up_hint.
   - `generate_idea_brief(idea: str, questions: list[dict], answers: dict) -> dict` — Generate Rationalised Idea Brief from interview answers. Returns dict matching RationalisedIdeaBrief schema.
   - `check_question_relevance(idea: str, answered: list[dict], answers: dict, remaining: list[dict]) -> dict` — Check if remaining questions are still relevant after an answer edit. Returns {"needs_regeneration": bool, "preserve_indices": list[int]}.
   - `assess_section_confidence(section_key: str, content: str) -> str` — Assess confidence level for a brief section. Returns "strong" | "moderate" | "needs_depth".

2. Create `schemas/understanding.py` with Pydantic models:
   - `UnderstandingQuestion` — id: str, text: str, input_type: str ("text"|"textarea"|"multiple_choice"), required: bool, options: list[str] | None, follow_up_hint: str | None
   - `RationalisedIdeaBrief` — problem_statement: str, target_user: str, value_prop: str, differentiation: str, monetization_hypothesis: str, market_context: str, key_constraints: list[str], assumptions: list[str], risks: list[str], smallest_viable_experiment: str, confidence_scores: dict[str, str] (mapping section_key to "strong"|"moderate"|"needs_depth"), generated_at: str, _schema_version: int = 1
   - `StartUnderstandingRequest` — session_id: str (onboarding session ID to continue from)
   - `StartUnderstandingResponse` — understanding_session_id: str, question: UnderstandingQuestion, question_number: int, total_questions: int
   - `SubmitAnswerRequest` — question_id: str, answer: str (min_length=1)
   - `SubmitAnswerResponse` — next_question: UnderstandingQuestion | None, question_number: int, total_questions: int, is_complete: bool
   - `EditAnswerRequest` — question_id: str, new_answer: str (min_length=1)
   - `EditAnswerResponse` — updated_questions: list[UnderstandingQuestion], current_question_number: int, total_questions: int, regenerated: bool
   - `IdeaBriefResponse` — brief: RationalisedIdeaBrief, artifact_id: str, version: int
   - `EditBriefSectionRequest` — section_key: str, new_content: str
   - `EditBriefSectionResponse` — updated_section: str, new_confidence: str, version: int

3. Extend RunnerFake in `runner_fake.py`:
   - `generate_understanding_questions`: Return 6 adaptive understanding questions (deeper than onboarding). Questions focus on market validation, competitive analysis, monetization depth, risk awareness, smallest experiment. Use "we" co-founder language. Return one question at a time (but fake returns all 6, service layer handles one-at-a-time delivery).
   - `generate_idea_brief`: Return a complete RationalisedIdeaBrief dict with all fields populated, realistic confidence_scores (mix of "strong", "moderate", "needs_depth"), investor-facing tone.
   - `check_question_relevance`: Return {"needs_regeneration": False, "preserve_indices": []} (in fake, no regeneration needed).
   - `assess_section_confidence`: Return "strong" for sections with > 100 chars, "moderate" for 50-100, "needs_depth" for < 50.
   - All methods respect scenario (llm_failure raises RuntimeError, rate_limited raises RuntimeError).
  </action>
  <verify>
    Run `cd /Users/vladcortex/co-founder/backend && python -c "from app.agent.runner import Runner; print('Protocol OK')"` to verify Runner imports.
    Run `cd /Users/vladcortex/co-founder/backend && python -c "from app.schemas.understanding import RationalisedIdeaBrief, StartUnderstandingResponse, SubmitAnswerResponse; print('Schemas OK')"` to verify schemas.
    Run `cd /Users/vladcortex/co-founder/backend && python -c "from app.agent.runner_fake import RunnerFake; f = RunnerFake(); import asyncio; qs = asyncio.run(f.generate_understanding_questions({})); print(f'{len(qs)} questions'); brief = asyncio.run(f.generate_idea_brief('test', [], {})); print(f'Brief keys: {list(brief.keys())}')"` to verify RunnerFake.
  </verify>
  <done>
    Runner protocol has 4 new methods (generate_understanding_questions, generate_idea_brief, check_question_relevance, assess_section_confidence). RationalisedIdeaBrief Pydantic schema validates with all required fields. RunnerFake returns 6 understanding questions and complete idea brief. All imports succeed without errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: UnderstandingService + API Routes + Integration Tests</name>
  <files>
    backend/app/services/understanding_service.py
    backend/app/api/routes/understanding.py
    backend/app/api/routes/__init__.py
    backend/tests/api/test_understanding_api.py
  </files>
  <action>
1. Create `services/understanding_service.py` following OnboardingService patterns (DI for runner + session_factory):
   - `start_session(clerk_user_id, onboarding_session_id)` — Load the completed onboarding session (verify ownership, verify status=completed). Store understanding state in a new JSONB column on OnboardingSession (understanding_questions, understanding_answers, understanding_status: "in_progress"|"completed", current_understanding_index). Generate first batch of understanding questions via runner.generate_understanding_questions(context with idea_text + onboarding answers). Return first question.

   NOTE: Rather than creating a new DB model, extend OnboardingSession usage. Add understanding-specific JSONB fields to the existing session. The session already has project_id, idea_text, answers. Understanding interview is a continuation of the same session.

   ALTERNATIVE (simpler): Store understanding state in the existing `thesis_edits` field or add understanding_state JSONB column to OnboardingSession. Since this is a migration concern, use the simpler approach: store understanding state as a separate JSON blob on the session with keys: understanding_questions, understanding_answers, understanding_status, understanding_index.

   SIMPLEST APPROACH: Create a separate UnderstandingSession record that references the OnboardingSession. Use a new table `understanding_sessions` with columns: id, clerk_user_id, onboarding_session_id (FK), project_id (FK), status, questions (JSONB), answers (JSONB), current_question_index, total_questions, created_at, updated_at, completed_at. Create Alembic migration for this table.

   - `submit_answer(clerk_user_id, session_id, question_id, answer)` — Store answer, generate next adaptive question via runner, increment index. If all questions answered, return is_complete=True.
   - `edit_answer(clerk_user_id, session_id, question_id, new_answer)` — Update answer, check question relevance via runner.check_question_relevance. If needs_regeneration, regenerate subsequent questions (preserve non-redundant ones). Return updated question list.
   - `finalize(clerk_user_id, session_id)` — Generate Rationalised Idea Brief via runner.generate_idea_brief. Store as Artifact (artifact_type="idea_brief"). Use ArtifactType enum — add IDEA_BRIEF value to the existing ArtifactType enum in schemas/artifacts.py. Set confidence_scores. Return IdeaBriefResponse.
   - `get_brief(clerk_user_id, project_id)` — Load the idea_brief Artifact for the project. Return brief content + metadata.
   - `edit_brief_section(clerk_user_id, project_id, section_key, new_content)` — Update section in Artifact's current_content JSONB. Recalculate confidence for that section via runner.assess_section_confidence. Increment version. Mark has_user_edits=True. Return updated section + new confidence.
   - `re_interview(clerk_user_id, session_id)` — Reset understanding session to start fresh (new questions based on existing brief). Existing brief is preserved until new one overwrites.
   - User isolation: All queries filter by clerk_user_id (404 pattern).

2. Create `api/routes/understanding.py` with 8 endpoints:
   - `POST /api/understanding/start` — Start understanding session (body: StartUnderstandingRequest)
   - `POST /api/understanding/{session_id}/answer` — Submit answer (body: SubmitAnswerRequest)
   - `PATCH /api/understanding/{session_id}/answer` — Edit previous answer (body: EditAnswerRequest)
   - `POST /api/understanding/{session_id}/finalize` — Generate Idea Brief
   - `GET /api/understanding/{project_id}/brief` — Get Idea Brief for project
   - `PATCH /api/understanding/{project_id}/brief` — Edit brief section (body: EditBriefSectionRequest)
   - `POST /api/understanding/{session_id}/re-interview` — Restart interview for major changes
   - `GET /api/understanding/{session_id}` — Get current session state (for resumption)
   - All routes use require_auth dependency.
   - All routes use get_runner() dependency (overrideable in tests).
   - LLM failures caught and returned as 500 with debug_id (UNDR-03).

3. Register router in `api/routes/__init__.py`:
   - `from app.api.routes import understanding`
   - `api_router.include_router(understanding.router, prefix="/understanding", tags=["understanding"])`

4. Create DB model for UnderstandingSession in `backend/app/db/models/understanding_session.py`:
   - id (UUID), clerk_user_id, onboarding_session_id (FK), project_id (FK), status, questions (JSONB), answers (JSONB), current_question_index, total_questions, created_at, updated_at, completed_at
   - Register in `backend/app/db/models/__init__.py`
   - Create Alembic migration

5. Add IDEA_BRIEF to ArtifactType enum in `backend/app/schemas/artifacts.py`.

6. Create integration tests in `tests/api/test_understanding_api.py` (following test_onboarding_api.py patterns):
   - test_start_understanding_returns_first_question (UNDR-01)
   - test_start_understanding_requires_completed_onboarding
   - test_submit_answer_returns_next_question
   - test_submit_all_answers_marks_complete
   - test_finalize_returns_idea_brief (UNDR-02)
   - test_finalize_brief_has_confidence_scores
   - test_edit_answer_preserves_progress
   - test_user_isolation_returns_404 (UNDR-05)
   - test_edit_brief_section_updates_confidence
   - test_get_brief_returns_artifact (UNDR-04)
   - test_llm_failure_returns_debug_id (UNDR-03)
   - test_re_interview_resets_session
   - All tests use dependency_overrides for require_auth and get_runner (RunnerFake).
  </action>
  <verify>
    Run `cd /Users/vladcortex/co-founder/backend && python -m pytest tests/api/test_understanding_api.py -v` — all tests pass.
    Run `cd /Users/vladcortex/co-founder/backend && python -c "from app.api.routes import api_router; routes = [r.path for r in api_router.routes if 'understanding' in str(r.path)]; print(f'{len(routes)} routes:', routes)"` — 8 routes registered.
    Run `cd /Users/vladcortex/co-founder/backend && python -c "from app.services.understanding_service import UnderstandingService; print('Service OK')"` — imports successfully.
  </verify>
  <done>
    UnderstandingService handles full interview lifecycle (start, answer, edit, finalize, brief CRUD, re-interview). 8 API endpoints registered and accessible. UnderstandingSession model with migration exists. IDEA_BRIEF added to ArtifactType. Integration tests cover UNDR-01 through UNDR-06. All tests pass. User isolation enforced via 404 pattern.
  </done>
</task>

</tasks>

<verification>
- `python -c "from app.agent.runner import Runner"` succeeds (protocol has 4 new methods)
- `python -c "from app.schemas.understanding import RationalisedIdeaBrief"` succeeds
- `python -c "from app.services.understanding_service import UnderstandingService"` succeeds
- `python -m pytest tests/api/test_understanding_api.py -v` — all tests pass
- 8 routes registered under /api/understanding/*
- RunnerFake generates understanding questions and idea brief
- LLM failure scenario returns debug_id (not secrets)
</verification>

<success_criteria>
- Founder can start an understanding interview and receive adaptive questions one at a time
- Submitting answers advances through the interview
- Editing a previous answer checks for question relevance changes
- Completing the interview produces a Rationalised Idea Brief stored as Artifact
- Brief has per-section confidence scores ("strong"/"moderate"/"needs_depth")
- Inline editing recalculates confidence for the edited section
- Re-interview option resets the session for major changes
- LLM failures return friendly error with debug_id (UNDR-03)
- User isolation enforced on all endpoints (UNDR-05)
</success_criteria>

<output>
After completion, create `.planning/phases/08-understanding-interview-decision-gates/08-01-SUMMARY.md`
</output>
