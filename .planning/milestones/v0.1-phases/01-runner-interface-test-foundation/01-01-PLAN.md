---
phase: 01-runner-interface-test-foundation
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - backend/app/agent/runner.py
  - backend/app/agent/runner_real.py
  - backend/tests/domain/test_runner_protocol.py
autonomous: true

must_haves:
  truths:
    - "Runner protocol defines all LLM operations: run, step, generate_questions, generate_brief, generate_artifacts"
    - "RunnerReal wraps existing LangGraph graph without modifying it"
    - "RunnerReal.run() invokes the full 6-node pipeline (Architect -> Coder -> Executor -> Debugger -> Reviewer -> GitManager)"
    - "RunnerReal.step() invokes a single named stage from the pipeline"
    - "isinstance(RunnerReal(...), Runner) returns True at runtime"
  artifacts:
    - path: "backend/app/agent/runner.py"
      provides: "Runner Protocol definition"
      contains: "class Runner(Protocol)"
    - path: "backend/app/agent/runner_real.py"
      provides: "Production Runner wrapping LangGraph"
      contains: "class RunnerReal"
    - path: "backend/tests/domain/test_runner_protocol.py"
      provides: "Protocol compliance and structural tests"
      min_lines: 50
  key_links:
    - from: "backend/app/agent/runner_real.py"
      to: "backend/app/agent/graph.py"
      via: "create_cofounder_graph import"
      pattern: "from app\\.agent\\.graph import create_cofounder_graph"
    - from: "backend/app/agent/runner.py"
      to: "backend/app/agent/state.py"
      via: "CoFounderState type reference"
      pattern: "from app\\.agent\\.state import CoFounderState"
---

<objective>
Define the Runner protocol and implement RunnerReal wrapping the existing LangGraph pipeline.

Purpose: Establish the testable abstraction layer that decouples all LLM operations from direct LangGraph calls, enabling TDD throughout the project. This is the foundation every other phase builds on.

Output: `runner.py` (Protocol), `runner_real.py` (production impl), `test_runner_protocol.py` (protocol compliance tests)
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-runner-interface-test-foundation/01-RESEARCH.md

# Source files to understand existing pipeline
@backend/app/agent/graph.py
@backend/app/agent/state.py
@backend/app/agent/nodes/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define Runner protocol and write compliance tests (RED)</name>
  <files>backend/app/agent/runner.py, backend/tests/domain/__init__.py, backend/tests/domain/test_runner_protocol.py</files>
  <action>
Create the Runner protocol at `backend/app/agent/runner.py` using `typing.Protocol` with `@runtime_checkable`.

The protocol MUST define these methods (per locked decisions):

```python
@runtime_checkable
class Runner(Protocol):
    async def run(self, state: CoFounderState) -> CoFounderState: ...
    async def step(self, state: CoFounderState, stage: str) -> CoFounderState: ...
    async def generate_questions(self, context: dict) -> list[dict]: ...
    async def generate_brief(self, answers: dict) -> dict: ...
    async def generate_artifacts(self, brief: dict) -> dict: ...
```

Import `CoFounderState` from `app.agent.state`. All methods are async.

Then create `backend/tests/domain/__init__.py` (empty) and write `test_runner_protocol.py` with tests that:

1. Verify `Runner` is runtime_checkable (`isinstance` check works)
2. Verify `Runner` has all 5 required methods with correct signatures
3. Verify `RunnerReal` satisfies the Runner protocol (`isinstance(RunnerReal(...), Runner)` is True) -- this test MUST FAIL (RED) because RunnerReal doesn't exist yet
4. Verify a class missing a method does NOT satisfy the protocol

Use descriptive test names. Example: `test_runner_real_satisfies_protocol`, `test_incomplete_class_does_not_satisfy_protocol`.

Do NOT use `@pytest.mark.asyncio` for these tests — they are structural/type checks, not async execution tests.
  </action>
  <verify>Run `cd /Users/vladcortex/co-founder/backend && python -m pytest tests/domain/test_runner_protocol.py -v`. The protocol definition tests (runtime_checkable, method signatures, incomplete class) MUST PASS. The RunnerReal protocol satisfaction test MUST FAIL (ImportError or assertion error because RunnerReal doesn't exist yet).</verify>
  <done>Runner protocol defined with 5 methods. Tests confirm protocol is checkable. RunnerReal test fails (RED state confirmed).</done>
</task>

<task type="auto">
  <name>Task 2: Implement RunnerReal wrapping LangGraph and pass tests (GREEN)</name>
  <files>backend/app/agent/runner_real.py, backend/tests/domain/test_runner_protocol.py</files>
  <action>
Create `backend/app/agent/runner_real.py` implementing all 5 Runner protocol methods:

**`__init__(self, checkpointer=None)`**: Accept optional checkpointer. Store a compiled graph via `create_cofounder_graph(checkpointer)`. Default to MemorySaver if none provided.

**`async def run(self, state: CoFounderState) -> CoFounderState`**: Invoke the compiled graph with the full state. Use `graph.ainvoke(state, config={"configurable": {"thread_id": state["session_id"] or "default"}})`. Return the resulting state. Do NOT modify the existing graph — wrap it as-is.

**`async def step(self, state: CoFounderState, stage: str) -> CoFounderState`**: Execute a single node from the graph. Valid stages: `architect`, `coder`, `executor`, `debugger`, `reviewer`, `git_manager`. Import the node function directly from `app.agent.nodes` and call it with the state. Merge the returned partial state dict into the input state and return. Raise `ValueError` if stage name is invalid.

**`async def generate_questions(self, context: dict) -> list[dict]`**: Placeholder for Phase 4 (Onboarding). For now, use the LLM to generate 5-7 questions based on `context.get("idea_keywords", "")`. Use `create_tracked_llm` with role="architect". Return list of dicts with keys: `id`, `text`, `required`.

**`async def generate_brief(self, answers: dict) -> dict`**: Placeholder for Phase 8 (Understanding Interview). Use LLM to generate a brief from answers. Return dict with keys: `problem_statement`, `target_user`, `value_prop`, `differentiation`, `monetization_hypothesis`, `assumptions`, `risks`, `smallest_viable_experiment`.

**`async def generate_artifacts(self, brief: dict) -> dict`**: Placeholder for Phase 6 (Artifact Generation). Use LLM to generate artifacts from brief. Return dict with keys: `product_brief`, `mvp_scope`, `milestones`, `risk_log`, `how_it_works`.

For `generate_questions`, `generate_brief`, and `generate_artifacts`: wrap the LLM call in try/except. On failure, raise `RuntimeError` with a descriptive message (no silent swallowing).

Update the import in `test_runner_protocol.py` so the RunnerReal test imports from `app.agent.runner_real`.
  </action>
  <verify>Run `cd /Users/vladcortex/co-founder/backend && python -m pytest tests/domain/test_runner_protocol.py -v`. ALL tests MUST PASS including the RunnerReal protocol satisfaction test.</verify>
  <done>RunnerReal implements all 5 Runner protocol methods. isinstance(RunnerReal(), Runner) returns True. All protocol compliance tests pass (GREEN).</done>
</task>

</tasks>

<verification>
1. `python -c "from app.agent.runner import Runner; print(Runner)"` succeeds
2. `python -c "from app.agent.runner_real import RunnerReal; from app.agent.runner import Runner; assert isinstance(RunnerReal(), Runner)"` succeeds
3. `cd backend && python -m pytest tests/domain/test_runner_protocol.py -v` — all tests pass
4. `python -c "from app.agent.graph import create_cofounder_graph"` still works (graph unchanged)
</verification>

<success_criteria>
- Runner protocol defines run, step, generate_questions, generate_brief, generate_artifacts
- RunnerReal wraps create_cofounder_graph without modifying graph.py
- RunnerReal satisfies Runner protocol at runtime (isinstance check)
- All domain tests pass
- Existing graph.py and nodes/* unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/01-runner-interface-test-foundation/01-01-SUMMARY.md`
</output>
