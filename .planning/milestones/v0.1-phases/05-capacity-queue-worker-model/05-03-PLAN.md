---
phase: 05-capacity-queue-worker-model
plan: 03
type: tdd
wave: 1
depends_on: []
files_modified:
  - backend/app/queue/state_machine.py
  - backend/app/queue/usage.py
  - backend/tests/domain/test_job_state_machine.py
  - backend/tests/domain/test_usage_counters.py
autonomous: true

must_haves:
  truths:
    - "Job status transitions follow valid state machine paths (queued->starting->scaffold->...->ready)"
    - "Invalid transitions rejected (e.g., queued->ready)"
    - "CHECKS can retry from SCAFFOLD (retry loop)"
    - "Iteration tracking counts build cycles, needs confirmation at tier depth (2/3/5)"
    - "Hard cap at 3x tier depth prevents infinite iteration"
    - "Daily job counter increments on enqueue with midnight UTC expiry"
    - "When daily limit hit, job gets SCHEDULED status instead of QUEUED"
    - "Usage counters include jobs_used, jobs_remaining, iterations_used, iterations_remaining"
  artifacts:
    - path: "backend/app/queue/state_machine.py"
      provides: "JobStateMachine with transition validation and iteration tracking"
      contains: "class JobStateMachine"
    - path: "backend/app/queue/usage.py"
      provides: "UsageTracker for daily limits, counters, and midnight reset"
      contains: "class UsageTracker"
  key_links:
    - from: "backend/app/queue/state_machine.py"
      to: "redis"
      via: "Redis HSET/HGET for job state, PUBLISH for status events"
      pattern: "redis\\.hset|redis\\.hget"
    - from: "backend/app/queue/usage.py"
      to: "redis"
      via: "Redis INCR with EXPIREAT for daily counters"
      pattern: "redis\\.incr|redis\\.expireat"
---

<objective>
Build the job state machine (status transitions + iteration tracking) and usage counter system (daily limits, midnight reset, counter responses).

Purpose: The state machine ensures jobs progress through well-defined states with valid transitions only, preventing invalid state corruption. Iteration tracking prevents runaway LLM costs. Usage counters give users visibility into their consumption and enable tier enforcement.
Output: Working JobStateMachine and UsageTracker with full TDD coverage.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-capacity-queue-worker-model/05-RESEARCH.md

Reference existing patterns:
@backend/app/domain/stages.py (existing state machine pattern with enums)
@backend/app/db/redis.py (Redis client access)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create job state machine with transition validation and iteration tracking (TDD)</name>
  <files>
    backend/app/queue/state_machine.py
    backend/tests/domain/test_job_state_machine.py
  </files>
  <action>
    **RED phase first — write failing tests:**

    Create `backend/tests/domain/test_job_state_machine.py` using fakeredis async:

    **Transition tests:**
    - Test QUEUED -> STARTING succeeds
    - Test QUEUED -> SCHEDULED succeeds (daily limit path)
    - Test QUEUED -> FAILED succeeds
    - Test STARTING -> SCAFFOLD succeeds
    - Test full happy path: QUEUED -> STARTING -> SCAFFOLD -> CODE -> DEPS -> CHECKS -> READY
    - Test CHECKS -> SCAFFOLD succeeds (retry loop)
    - Test QUEUED -> READY rejected (invalid skip)
    - Test READY -> anything rejected (terminal state)
    - Test FAILED -> anything rejected (terminal state)
    - Test SCHEDULED -> QUEUED succeeds (limit reset path)
    - Test transition on non-existent job returns False
    - Test transition publishes to Redis pub/sub channel `job:{id}:events`

    **Iteration tests:**
    - Test increment_iteration increases count
    - Test needs_confirmation returns True at tier depth boundary (depth=2, iteration 2 -> True)
    - Test needs_confirmation returns False before boundary (depth=2, iteration 1 -> False)
    - Test check_iteration_allowed returns True when under hard cap (3x depth)
    - Test check_iteration_allowed returns False at hard cap
    - Test confirm_continuation grants another batch (resets batch counter)

    **GREEN phase — implement:**

    Create `backend/app/queue/state_machine.py`:
    ```python
    from app.queue.schemas import JobStatus, TIER_ITERATION_DEPTH

    class JobStateMachine:
        TRANSITIONS = {
            JobStatus.QUEUED: [JobStatus.STARTING, JobStatus.SCHEDULED, JobStatus.FAILED],
            JobStatus.STARTING: [JobStatus.SCAFFOLD, JobStatus.FAILED],
            JobStatus.SCAFFOLD: [JobStatus.CODE, JobStatus.FAILED],
            JobStatus.CODE: [JobStatus.DEPS, JobStatus.FAILED],
            JobStatus.DEPS: [JobStatus.CHECKS, JobStatus.FAILED],
            JobStatus.CHECKS: [JobStatus.READY, JobStatus.SCAFFOLD, JobStatus.FAILED],
            JobStatus.READY: [],
            JobStatus.FAILED: [],
            JobStatus.SCHEDULED: [JobStatus.QUEUED],
        }

        def __init__(self, redis: Redis):
            self.redis = redis

        async def create_job(self, job_id: str, metadata: dict) -> None:
            """Initialize job hash in Redis with QUEUED status."""
            await self.redis.hset(f"job:{job_id}", mapping={
                "status": JobStatus.QUEUED.value,
                "created_at": datetime.now(timezone.utc).isoformat(),
                **metadata,
            })

        async def transition(self, job_id: str, new_status: JobStatus, message: str = "") -> bool:
            """Transition job to new status if valid. Publishes event on success."""
            current = await self.redis.hget(f"job:{job_id}", "status")
            if current is None:
                return False

            current_status = JobStatus(current)
            if new_status not in self.TRANSITIONS.get(current_status, []):
                return False

            # Atomic update
            async with self.redis.pipeline(transaction=True) as pipe:
                pipe.hset(f"job:{job_id}", "status", new_status.value)
                pipe.hset(f"job:{job_id}", "status_message", message)
                pipe.hset(f"job:{job_id}", "updated_at", datetime.now(timezone.utc).isoformat())
                await pipe.execute()

            # Publish status change for SSE
            await self.redis.publish(
                f"job:{job_id}:events",
                json.dumps({
                    "job_id": job_id,
                    "status": new_status.value,
                    "message": message,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                })
            )
            return True

        async def get_status(self, job_id: str) -> JobStatus | None:
            status = await self.redis.hget(f"job:{job_id}", "status")
            return JobStatus(status) if status else None

        async def get_job(self, job_id: str) -> dict | None:
            data = await self.redis.hgetall(f"job:{job_id}")
            return data if data else None
    ```

    **Iteration tracking** (in same file or as methods on JobStateMachine):
    ```python
    class IterationTracker:
        def __init__(self, redis: Redis):
            self.redis = redis

        async def increment(self, job_id: str) -> int:
            return await self.redis.incr(f"job:{job_id}:iterations")

        async def get_count(self, job_id: str) -> int:
            return int(await self.redis.get(f"job:{job_id}:iterations") or 0)

        async def needs_confirmation(self, job_id: str, tier: str) -> bool:
            depth = TIER_ITERATION_DEPTH.get(tier, 2)
            current = await self.get_count(job_id)
            return current > 0 and current % depth == 0

        async def check_allowed(self, job_id: str, tier: str) -> tuple[bool, int, int]:
            """Returns (allowed, current, remaining)."""
            depth = TIER_ITERATION_DEPTH.get(tier, 2)
            hard_cap = depth * 3
            current = await self.get_count(job_id)
            remaining = max(0, hard_cap - current)
            return (current < hard_cap, current, remaining)
    ```

    Use injectable `now` parameter where time is involved (per Phase 02 pattern for deterministic testing).
  </action>
  <verify>
    Run: `cd /Users/vladcortex/co-founder && python -m pytest backend/tests/domain/test_job_state_machine.py -v`
    All 18+ tests pass (12 transition, 6 iteration).
  </verify>
  <done>
    JobStateMachine validates all transitions per state diagram. Terminal states (READY, FAILED) reject all transitions. CHECKS->SCAFFOLD retry loop works. Iteration tracking enforces tier-based depth with 3x hard cap. Pub/sub events published on every transition. All tests green.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create usage counter system with daily limits and midnight UTC reset (TDD)</name>
  <files>
    backend/app/queue/usage.py
    backend/tests/domain/test_usage_counters.py
  </files>
  <action>
    **RED phase first — write failing tests:**

    Create `backend/tests/domain/test_usage_counters.py` using fakeredis async:
    - Test increment_daily_usage increases counter
    - Test daily counter has TTL set (auto-expires at midnight UTC)
    - Test get_daily_usage returns 0 for new user
    - Test check_daily_limit: bootstrapper at 5 returns exceeded=True
    - Test check_daily_limit: bootstrapper at 4 returns exceeded=False
    - Test get_usage_counters returns complete UsageCounters with all fields
    - Test get_usage_counters with job_id includes iterations_used and iterations_remaining
    - Test get_next_reset returns tomorrow midnight UTC
    - Test daily limit tiers: bootstrapper=5, partner=50, cto_scale=200

    **GREEN phase — implement:**

    Create `backend/app/queue/usage.py`:
    ```python
    from datetime import date, datetime, timedelta, timezone
    from app.queue.schemas import TIER_DAILY_LIMIT, TIER_ITERATION_DEPTH, UsageCounters

    class UsageTracker:
        def __init__(self, redis: Redis):
            self.redis = redis

        async def increment_daily_usage(self, user_id: str, now: datetime | None = None) -> int:
            """Increment daily job counter. Sets expiry at midnight UTC if not already set."""
            now = now or datetime.now(timezone.utc)
            today = now.date().isoformat()
            key = f"usage:{user_id}:jobs:{today}"

            count = await self.redis.incr(key)

            # Set expiry to next midnight if not already set
            ttl = await self.redis.ttl(key)
            if ttl == -1:  # No expiry
                reset_time = self._get_next_reset(now)
                await self.redis.expireat(key, int(reset_time.timestamp()))

            return count

        async def get_daily_usage(self, user_id: str, now: datetime | None = None) -> int:
            now = now or datetime.now(timezone.utc)
            today = now.date().isoformat()
            key = f"usage:{user_id}:jobs:{today}"
            return int(await self.redis.get(key) or 0)

        async def check_daily_limit(self, user_id: str, tier: str, now: datetime | None = None) -> tuple[bool, int, int]:
            """Check if daily limit exceeded. Returns (exceeded, used, limit)."""
            limit = TIER_DAILY_LIMIT.get(tier, 5)
            used = await self.get_daily_usage(user_id, now)
            return (used >= limit, used, limit)

        async def get_usage_counters(
            self, user_id: str, tier: str, job_id: str | None = None, now: datetime | None = None
        ) -> UsageCounters:
            """Build complete usage counters for API response."""
            now = now or datetime.now(timezone.utc)
            daily_limit = TIER_DAILY_LIMIT.get(tier, 5)
            jobs_used = await self.get_daily_usage(user_id, now)
            jobs_remaining = max(0, daily_limit - jobs_used)

            # Iteration counters (per-job)
            iteration_depth = TIER_ITERATION_DEPTH.get(tier, 2)
            hard_cap = iteration_depth * 3

            if job_id:
                iterations_used = int(await self.redis.get(f"job:{job_id}:iterations") or 0)
            else:
                iterations_used = 0

            iterations_remaining = max(0, hard_cap - iterations_used)

            reset_time = self._get_next_reset(now)

            return UsageCounters(
                jobs_used=jobs_used,
                jobs_remaining=jobs_remaining,
                iterations_used=iterations_used,
                iterations_remaining=iterations_remaining,
                daily_limit_resets_at=reset_time.isoformat(),
            )

        @staticmethod
        def _get_next_reset(now: datetime | None = None) -> datetime:
            """Get next midnight UTC."""
            now = now or datetime.now(timezone.utc)
            tomorrow = now.date() + timedelta(days=1)
            return datetime.combine(tomorrow, datetime.min.time(), tzinfo=timezone.utc)
    ```

    Use injectable `now` parameter throughout for deterministic testing (per Phase 02 pattern).
  </action>
  <verify>
    Run: `cd /Users/vladcortex/co-founder && python -m pytest backend/tests/domain/test_usage_counters.py -v`
    All 9+ tests pass.
  </verify>
  <done>
    UsageTracker increments daily counters with midnight UTC expiry. Daily limits enforced per tier (5/50/200). get_usage_counters returns complete UsageCounters with all 4 fields. Injectable `now` for deterministic testing. All tests green.
  </done>
</task>

</tasks>

<verification>
- `python -m pytest backend/tests/domain/test_job_state_machine.py backend/tests/domain/test_usage_counters.py -v` — all tests pass
- Verify TIER_ITERATION_DEPTH matches: {bootstrapper: 2, partner: 3, cto_scale: 5}
- Verify hard cap is 3x tier depth
- Verify TIER_DAILY_LIMIT matches: {bootstrapper: 5, partner: 50, cto_scale: 200}
- Verify midnight UTC reset
</verification>

<success_criteria>
- JobStateMachine validates all 9 status transitions per locked state diagram
- IterationTracker enforces tier-based depth with batch confirmation and 3x hard cap
- UsageTracker tracks daily jobs with auto-expiring Redis keys
- Usage counters include all 4 required fields per locked decision
- Injectable `now` for deterministic testing
- All tests green with fakeredis
</success_criteria>

<output>
After completion, create `.planning/phases/05-capacity-queue-worker-model/05-03-SUMMARY.md`
</output>
