---
phase: 05-capacity-queue-worker-model
plan: 04
type: tdd
wave: 2
depends_on: ["05-01", "05-02", "05-03"]
files_modified:
  - backend/app/api/routes/jobs.py
  - backend/app/api/routes/__init__.py
  - backend/app/queue/worker.py
  - backend/tests/api/test_jobs_api.py
autonomous: true

must_haves:
  truths:
    - "POST /api/jobs creates job, enqueues in Redis, returns job_id + position + usage counters"
    - "GET /api/jobs/{id} returns current job status, position, and usage counters"
    - "GET /api/jobs/{id}/stream returns SSE event stream with real-time status updates"
    - "POST /api/jobs/{id}/confirm grants another iteration batch when at depth limit"
    - "Jobs at daily limit get SCHEDULED status with 'Scheduled for tomorrow' message"
    - "Jobs beyond global cap (100) rejected with 503 and retry_after_minutes"
    - "Worker pulls from queue, acquires semaphore, runs job via Runner, releases on completion"
    - "SSE stream closes on terminal states (ready, failed)"
  artifacts:
    - path: "backend/app/api/routes/jobs.py"
      provides: "Job submission, status, streaming, and confirmation API endpoints"
      exports: ["router"]
    - path: "backend/app/queue/worker.py"
      provides: "JobWorker that processes jobs from queue with concurrency control"
      contains: "class JobWorker"
  key_links:
    - from: "backend/app/api/routes/jobs.py"
      to: "backend/app/queue/manager.py"
      via: "QueueManager for enqueue/position"
      pattern: "from app\\.queue\\.manager import"
    - from: "backend/app/api/routes/jobs.py"
      to: "backend/app/queue/state_machine.py"
      via: "JobStateMachine for status transitions"
      pattern: "from app\\.queue\\.state_machine import"
    - from: "backend/app/api/routes/jobs.py"
      to: "backend/app/queue/usage.py"
      via: "UsageTracker for daily limits and counters"
      pattern: "from app\\.queue\\.usage import"
    - from: "backend/app/queue/worker.py"
      to: "backend/app/queue/semaphore.py"
      via: "RedisSemaphore for concurrency control"
      pattern: "from app\\.queue\\.semaphore import"
    - from: "backend/app/queue/worker.py"
      to: "backend/app/agent/runner.py"
      via: "Runner protocol for job execution"
      pattern: "from app\\.agent\\.runner import"
---

<objective>
Wire all queue primitives (QueueManager, Semaphore, StateMachine, UsageTracker, Estimator) into FastAPI API routes and a background worker that processes jobs. This is the integration layer that makes the queue system accessible to users.

Purpose: Exposes the queue system via REST API with SSE streaming for real-time updates. The worker pulls jobs and executes them with concurrency control, completing the end-to-end flow.
Output: Working API routes (POST /jobs, GET /jobs/{id}, GET /jobs/{id}/stream, POST /jobs/{id}/confirm) and JobWorker with integration tests.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-capacity-queue-worker-model/05-RESEARCH.md

Reference prior plan SUMMARYs (these plans create the primitives used here):
@.planning/phases/05-capacity-queue-worker-model/05-01-SUMMARY.md
@.planning/phases/05-capacity-queue-worker-model/05-02-SUMMARY.md
@.planning/phases/05-capacity-queue-worker-model/05-03-SUMMARY.md

Reference existing patterns:
@backend/app/api/routes/onboarding.py (route pattern, dependency injection, response models)
@backend/app/api/routes/__init__.py (route registration)
@backend/app/core/auth.py (require_auth, require_subscription dependencies)
@backend/app/db/redis.py (get_redis())
@backend/tests/api/conftest.py (API test client fixture)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create job API routes with SSE streaming and confirmation endpoint (TDD)</name>
  <files>
    backend/app/api/routes/jobs.py
    backend/app/api/routes/__init__.py
    backend/tests/api/test_jobs_api.py
  </files>
  <action>
    **RED phase first — write failing tests:**

    Create `backend/tests/api/test_jobs_api.py`:
    Use existing api_client fixture pattern from `tests/api/conftest.py`. Override require_auth and require_subscription via app.dependency_overrides (per Phase 03 pattern). Use fakeredis for Redis mock (override get_redis dependency or use monkeypatch).

    Tests:
    - POST /api/jobs with valid body returns 201 with job_id, position, usage counters
    - POST /api/jobs requires auth (401 without token)
    - POST /api/jobs with empty goal returns 422
    - GET /api/jobs/{id} returns current status and usage counters
    - GET /api/jobs/{id} with wrong user returns 404 (user isolation)
    - GET /api/jobs/nonexistent returns 404
    - POST /api/jobs/{id}/confirm returns updated iteration counters
    - POST /api/jobs/{id}/confirm when not at limit returns 400
    - Test daily limit exceeded: 6th job for bootstrapper returns SCHEDULED status
    - Test global cap: when queue at 100, returns 503 with retry_after_minutes

    **GREEN phase — implement:**

    Create `backend/app/api/routes/jobs.py`:
    ```python
    from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
    from fastapi.responses import StreamingResponse
    from pydantic import BaseModel, Field

    from app.core.auth import ClerkUser, require_auth, require_subscription
    from app.core.llm_config import get_or_create_user_settings
    from app.db.redis import get_redis
    from app.queue.manager import QueueManager
    from app.queue.state_machine import JobStateMachine, IterationTracker
    from app.queue.usage import UsageTracker
    from app.queue.estimator import WaitTimeEstimator
    from app.queue.schemas import JobStatus, UsageCounters

    router = APIRouter()

    class SubmitJobRequest(BaseModel):
        project_id: str
        goal: str = Field(..., min_length=1)

    class SubmitJobResponse(BaseModel):
        job_id: str
        status: str
        position: int
        estimated_wait: dict | None = None
        usage: UsageCounters
        message: str

    class JobStatusResponse(BaseModel):
        job_id: str
        status: str
        position: int
        message: str
        usage: UsageCounters

    class ConfirmResponse(BaseModel):
        job_id: str
        iterations_granted: int
        usage: UsageCounters

    @router.post("", status_code=201, response_model=SubmitJobResponse)
    async def submit_job(
        request: SubmitJobRequest,
        background_tasks: BackgroundTasks,
        user: ClerkUser = Depends(require_subscription),
    ):
        """Submit a new job to the queue."""
        redis = get_redis()
        user_settings = await get_or_create_user_settings(user.user_id)
        tier = user_settings.plan_tier.slug

        # Check daily limit
        usage_tracker = UsageTracker(redis)
        exceeded, used, limit = await usage_tracker.check_daily_limit(user.user_id, tier)

        # Generate job_id
        import uuid
        job_id = str(uuid.uuid4())

        if exceeded:
            # Schedule for tomorrow (per locked decision: accept but schedule)
            state_machine = JobStateMachine(redis)
            await state_machine.create_job(job_id, {
                "project_id": request.project_id,
                "user_id": user.user_id,
                "tier": tier,
                "goal": request.goal,
            })
            # Transition to SCHEDULED
            await state_machine.transition(job_id, JobStatus.SCHEDULED, "Scheduled for tomorrow")

            counters = await usage_tracker.get_usage_counters(user.user_id, tier, job_id)
            return SubmitJobResponse(
                job_id=job_id,
                status="scheduled",
                position=0,
                usage=counters,
                message="Daily limit reached. Scheduled for tomorrow.",
            )

        # Check global cap
        queue_manager = QueueManager(redis)
        queue_length = await queue_manager.get_length()
        if queue_length >= 100:
            estimator = WaitTimeEstimator(redis)
            wait = await estimator.estimate_wait_time(tier, queue_length - 100 + 1)
            retry_minutes = max(1, wait // 60)
            raise HTTPException(
                status_code=503,
                detail=f"System busy. Try again in {retry_minutes} minutes.",
            )

        # Create job in state machine
        state_machine = JobStateMachine(redis)
        await state_machine.create_job(job_id, {
            "project_id": request.project_id,
            "user_id": user.user_id,
            "tier": tier,
            "goal": request.goal,
        })

        # Enqueue
        result = await queue_manager.enqueue(job_id, tier)

        # Increment daily usage
        await usage_tracker.increment_daily_usage(user.user_id)

        # Estimate wait time
        estimator = WaitTimeEstimator(redis)
        wait_estimate = await estimator.estimate_with_confidence(tier, result["position"])

        # Get usage counters
        counters = await usage_tracker.get_usage_counters(user.user_id, tier, job_id)

        # Trigger background worker to process queue
        from app.queue.worker import process_next_job
        background_tasks.add_task(process_next_job)

        return SubmitJobResponse(
            job_id=job_id,
            status="queued",
            position=result["position"],
            estimated_wait=wait_estimate,
            usage=counters,
            message=f"Queued at position {result['position']}",
        )

    @router.get("/{job_id}", response_model=JobStatusResponse)
    async def get_job_status(
        job_id: str,
        user: ClerkUser = Depends(require_auth),
    ):
        """Get current job status with usage counters."""
        redis = get_redis()
        state_machine = JobStateMachine(redis)

        job_data = await state_machine.get_job(job_id)
        if not job_data or job_data.get("user_id") != user.user_id:
            raise HTTPException(status_code=404, detail="Job not found")

        queue_manager = QueueManager(redis)
        position = await queue_manager.get_position(job_id)

        user_settings = await get_or_create_user_settings(user.user_id)
        tier = user_settings.plan_tier.slug

        usage_tracker = UsageTracker(redis)
        counters = await usage_tracker.get_usage_counters(user.user_id, tier, job_id)

        return JobStatusResponse(
            job_id=job_id,
            status=job_data.get("status", "unknown"),
            position=position,
            message=job_data.get("status_message", ""),
            usage=counters,
        )

    @router.get("/{job_id}/stream")
    async def stream_job_status(
        job_id: str,
        user: ClerkUser = Depends(require_auth),
    ):
        """Stream real-time job status updates via SSE."""
        redis = get_redis()
        state_machine = JobStateMachine(redis)

        job_data = await state_machine.get_job(job_id)
        if not job_data or job_data.get("user_id") != user.user_id:
            raise HTTPException(status_code=404, detail="Job not found")

        async def event_generator():
            # Send initial status
            import json
            yield f"data: {json.dumps({'job_id': job_id, 'status': job_data.get('status'), 'message': job_data.get('status_message', '')})}\n\n"

            # Check if already terminal
            status = job_data.get("status")
            if status in ["ready", "failed"]:
                return

            # Subscribe to events
            pubsub = redis.pubsub()
            channel = f"job:{job_id}:events"
            await pubsub.subscribe(channel)

            try:
                async for message in pubsub.listen():
                    if message["type"] == "message":
                        yield f"data: {message['data']}\n\n"
                        data = json.loads(message["data"])
                        if data.get("status") in ["ready", "failed"]:
                            break
            finally:
                await pubsub.unsubscribe(channel)
                await pubsub.close()

        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )

    @router.post("/{job_id}/confirm", response_model=ConfirmResponse)
    async def confirm_iteration(
        job_id: str,
        user: ClerkUser = Depends(require_auth),
    ):
        """Confirm continuation for another iteration batch."""
        redis = get_redis()
        state_machine = JobStateMachine(redis)

        job_data = await state_machine.get_job(job_id)
        if not job_data or job_data.get("user_id") != user.user_id:
            raise HTTPException(status_code=404, detail="Job not found")

        tier = job_data.get("tier", "bootstrapper")
        from app.queue.schemas import TIER_ITERATION_DEPTH
        iteration_tracker = IterationTracker(redis)

        needs_confirm = await iteration_tracker.needs_confirmation(job_id, tier)
        if not needs_confirm:
            raise HTTPException(status_code=400, detail="Job is not awaiting confirmation")

        # Check hard cap
        allowed, current, remaining = await iteration_tracker.check_allowed(job_id, tier)
        if not allowed:
            raise HTTPException(status_code=400, detail="Hard iteration cap reached. Job cannot continue.")

        depth = TIER_ITERATION_DEPTH.get(tier, 2)

        user_settings = await get_or_create_user_settings(user.user_id)
        usage_tracker = UsageTracker(redis)
        counters = await usage_tracker.get_usage_counters(user.user_id, tier, job_id)

        return ConfirmResponse(
            job_id=job_id,
            iterations_granted=depth,
            usage=counters,
        )
    ```

    Register route in `backend/app/api/routes/__init__.py`:
    Add `from app.api.routes import jobs` to imports.
    Add `api_router.include_router(jobs.router, prefix="/jobs", tags=["jobs"])`.
  </action>
  <verify>
    Run: `cd /Users/vladcortex/co-founder && python -m pytest backend/tests/api/test_jobs_api.py -v`
    All 10+ tests pass.
  </verify>
  <done>
    POST /api/jobs creates queued job with position and usage counters. GET /api/jobs/{id} returns status. GET /api/jobs/{id}/stream returns SSE. POST /api/jobs/{id}/confirm grants iteration batch. Daily limit produces SCHEDULED status. Global cap produces 503. User isolation enforced. All tests green.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create background worker that processes jobs from queue with concurrency control</name>
  <files>
    backend/app/queue/worker.py
  </files>
  <action>
    Create `backend/app/queue/worker.py`:

    ```python
    """JobWorker: pulls jobs from queue, enforces concurrency, executes via Runner."""

    import logging
    import time
    from datetime import datetime, timezone

    from app.agent.runner import Runner
    from app.db.redis import get_redis
    from app.queue.manager import QueueManager
    from app.queue.semaphore import user_semaphore, project_semaphore
    from app.queue.state_machine import JobStateMachine, IterationTracker
    from app.queue.estimator import WaitTimeEstimator
    from app.queue.schemas import JobStatus

    logger = logging.getLogger(__name__)

    async def process_next_job(runner: Runner | None = None) -> bool:
        """Pull next job from queue and process it.

        Called by FastAPI BackgroundTasks. Returns True if job processed, False if queue empty.

        Steps:
        1. Dequeue highest priority job
        2. Check per-user and per-project concurrency semaphores
        3. If semaphore acquired: transition STARTING->SCAFFOLD->...->READY
        4. If semaphore unavailable: re-enqueue job
        5. Record duration for wait time estimation
        6. Release semaphore on completion (or crash via TTL)
        """
        redis = get_redis()
        queue = QueueManager(redis)
        state_machine = JobStateMachine(redis)

        # Dequeue
        job_id = await queue.dequeue()
        if job_id is None:
            return False

        job_data = await state_machine.get_job(job_id)
        if job_data is None:
            logger.error(f"Dequeued job {job_id} but no metadata found")
            return False

        user_id = job_data.get("user_id")
        project_id = job_data.get("project_id")
        tier = job_data.get("tier", "bootstrapper")

        # Acquire concurrency semaphores
        user_sem = user_semaphore(redis, user_id, tier)
        project_sem = project_semaphore(redis, project_id, tier)

        user_acquired = await user_sem.acquire(job_id)
        if not user_acquired:
            # Re-enqueue with same priority
            await queue.enqueue(job_id, tier)
            logger.info(f"Job {job_id}: user concurrency limit, re-enqueued")
            return False

        project_acquired = await project_sem.acquire(job_id)
        if not project_acquired:
            await user_sem.release(job_id)
            await queue.enqueue(job_id, tier)
            logger.info(f"Job {job_id}: project concurrency limit, re-enqueued")
            return False

        start_time = time.time()

        try:
            # Transition to STARTING
            await state_machine.transition(job_id, JobStatus.STARTING, "Starting job execution")

            # Execute phases: SCAFFOLD -> CODE -> DEPS -> CHECKS
            # For MVP: simulate with Runner.run() wrapping the full pipeline
            # In production, each status corresponds to a LangGraph node
            for status in [JobStatus.SCAFFOLD, JobStatus.CODE, JobStatus.DEPS, JobStatus.CHECKS]:
                await state_machine.transition(job_id, status, f"Phase: {status.value}")

                # Heartbeat to extend semaphore TTL during long operations
                await user_sem.heartbeat(job_id)
                await project_sem.heartbeat(job_id)

            # If we have a runner, execute the actual pipeline
            if runner:
                from app.agent.state import create_initial_state
                state = create_initial_state(
                    user_id=user_id,
                    project_id=project_id,
                    project_path=f"/tmp/jobs/{job_id}",
                    goal=job_data.get("goal", ""),
                    session_id=job_id,
                )
                await runner.run(state)

            # Mark as READY
            await state_machine.transition(job_id, JobStatus.READY, "Job completed successfully")

            # Record duration for wait time estimation
            duration = time.time() - start_time
            estimator = WaitTimeEstimator(redis)
            await estimator.record_completion(tier, duration)

            # Persist to Postgres (terminal state)
            await _persist_job_to_postgres(job_id, job_data, JobStatus.READY, duration)

        except Exception as exc:
            logger.error(f"Job {job_id} failed: {exc}", exc_info=True)
            await state_machine.transition(
                job_id, JobStatus.FAILED,
                f"Job failed: {str(exc)[:200]}"
            )
            duration = time.time() - start_time
            await _persist_job_to_postgres(job_id, job_data, JobStatus.FAILED, duration, str(exc))

        finally:
            # Release semaphores
            await user_sem.release(job_id)
            await project_sem.release(job_id)

        return True


    async def _persist_job_to_postgres(
        job_id: str,
        job_data: dict,
        status: JobStatus,
        duration: float,
        error_message: str | None = None,
    ) -> None:
        """Write job record to Postgres for audit trail (terminal states only)."""
        import uuid
        from app.db.base import get_session_factory
        from app.db.models.job import Job

        try:
            factory = get_session_factory()
            async with factory() as session:
                job = Job(
                    id=uuid.UUID(job_id),
                    project_id=uuid.UUID(job_data.get("project_id", job_id)),
                    clerk_user_id=job_data.get("user_id", ""),
                    tier=job_data.get("tier", "bootstrapper"),
                    status=status.value,
                    goal=job_data.get("goal", ""),
                    duration_seconds=int(duration),
                    error_message=error_message,
                    debug_id=str(uuid.uuid4()),
                )
                session.add(job)
                await session.commit()
        except Exception as exc:
            logger.error(f"Failed to persist job {job_id}: {exc}", exc_info=True)
    ```

    This uses FastAPI BackgroundTasks for MVP (per research recommendation). Worker acquires both user and project semaphores before execution. TTL on semaphores prevents deadlock. Duration recorded for EMA estimation.
  </action>
  <verify>
    Import check: `cd /Users/vladcortex/co-founder && python -c "from app.queue.worker import process_next_job; print('OK')"`
    Run all API tests: `python -m pytest backend/tests/api/test_jobs_api.py -v`
  </verify>
  <done>
    JobWorker dequeues highest priority job, acquires per-user + per-project semaphores, transitions through status pipeline, executes via Runner, records duration, persists to Postgres on terminal states, and releases semaphores in finally block. BackgroundTasks integration for MVP.
  </done>
</task>

</tasks>

<verification>
- `python -m pytest backend/tests/api/test_jobs_api.py -v` — all API tests pass
- Verify POST /api/jobs returns 201 with job_id, position, usage counters
- Verify GET /api/jobs/{id}/stream has media_type text/event-stream
- Verify jobs router registered at /api/jobs in routes/__init__.py
- Verify user isolation: wrong user gets 404
</verification>

<success_criteria>
- Job submission flow: validate -> check daily limit -> check global cap -> enqueue -> return position
- Job status with usage counters returned on every response
- SSE streaming with Redis pub/sub for real-time updates
- Iteration confirmation endpoint grants tier-based batch
- Worker processes jobs with concurrency control and Postgres persistence
- All tests green
</success_criteria>

<output>
After completion, create `.planning/phases/05-capacity-queue-worker-model/05-04-SUMMARY.md`
</output>
