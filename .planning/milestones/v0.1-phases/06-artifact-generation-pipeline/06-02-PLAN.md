---
phase: 06-artifact-generation-pipeline
plan: 02
type: tdd
wave: 2
depends_on: ["06-01"]
files_modified:
  - backend/app/artifacts/__init__.py
  - backend/app/artifacts/generator.py
  - backend/app/artifacts/prompts.py
  - backend/app/services/artifact_service.py
  - backend/tests/domain/test_artifact_generator.py
  - backend/tests/api/test_artifact_service.py
autonomous: true

must_haves:
  truths:
    - "ArtifactGenerator produces structured content for each artifact type using Claude structured outputs"
    - "Generation cascade follows linear chain: Brief -> MVP Scope -> Milestones -> Risk Log -> How It Works"
    - "Each downstream artifact receives upstream artifacts as context"
    - "Failure in one artifact does not re-generate already-completed artifacts"
    - "ArtifactService orchestrates generation, versioning, and persistence"
    - "Regeneration moves current_content to previous_content and increments version_number"
    - "Tier filtering adds/removes sections before persistence"
  artifacts:
    - path: "backend/app/artifacts/generator.py"
      provides: "ArtifactGenerator with per-type generation methods"
      contains: "class ArtifactGenerator"
    - path: "backend/app/artifacts/prompts.py"
      provides: "System prompts per artifact type with co-founder voice"
      contains: "BRIEF_SYSTEM_PROMPT"
    - path: "backend/app/services/artifact_service.py"
      provides: "ArtifactService orchestrating generation, versioning, persistence"
      exports: ["ArtifactService"]
  key_links:
    - from: "backend/app/services/artifact_service.py"
      to: "backend/app/artifacts/generator.py"
      via: "ArtifactService calls generator.generate_artifact(type, context)"
      pattern: "generator\\.generate"
    - from: "backend/app/artifacts/generator.py"
      to: "backend/app/schemas/artifacts.py"
      via: "Structured output schemas for Claude API"
      pattern: "ProductBriefContent|MvpScopeContent"
    - from: "backend/app/services/artifact_service.py"
      to: "backend/app/db/models/artifact.py"
      via: "SQLAlchemy persistence of generated content"
      pattern: "Artifact\\("
---

<objective>
Artifact generator with Claude structured outputs and cascade orchestration service.

Purpose: Core generation engine that produces all 5 artifacts in linear chain order, with each downstream artifact receiving upstream content as context. Service layer handles versioning, tier filtering, and persistence.
Output: ArtifactGenerator, system prompts, ArtifactService with cascade logic, tests.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-artifact-generation-pipeline/06-CONTEXT.md
@.planning/phases/06-artifact-generation-pipeline/06-RESEARCH.md
@.planning/phases/06-artifact-generation-pipeline/06-01-SUMMARY.md
@backend/app/schemas/artifacts.py
@backend/app/db/models/artifact.py
@backend/app/agent/runner.py
@backend/app/agent/runner_fake.py
@backend/app/services/onboarding_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: ArtifactGenerator and system prompts (RED then GREEN)</name>
  <files>
    backend/app/artifacts/__init__.py
    backend/app/artifacts/generator.py
    backend/app/artifacts/prompts.py
    backend/tests/domain/test_artifact_generator.py
  </files>
  <action>
**RED: Write tests first in backend/tests/domain/test_artifact_generator.py:**

Create the `backend/app/artifacts/` package (mkdir, create __init__.py).

Tests using RunnerFake as the LLM backend (ArtifactGenerator takes a Runner dependency):

1. `test_generate_brief_returns_product_brief_content` — generate_artifact(ArtifactType.BRIEF, onboarding_data) returns dict that validates as ProductBriefContent
2. `test_generate_mvp_scope_uses_brief_as_context` — generate_artifact(ArtifactType.MVP_SCOPE, onboarding_data, prior_artifacts={"brief": brief_data}) receives brief in context
3. `test_generate_milestones_uses_brief_and_mvp_as_context` — prior_artifacts includes brief + mvp_scope
4. `test_generate_all_cascade_returns_five_artifacts` — generate_cascade(onboarding_data) returns dict with all 5 ArtifactType keys populated
5. `test_generate_cascade_partial_failure_keeps_completed` — If runner raises on milestones, result has brief + mvp_scope populated but milestones/risk_log/how_it_works in failed_artifacts list
6. `test_generate_cascade_respects_order` — Brief generated before MVP Scope, etc. (verify by checking context passed to each call)
7. `test_tier_filter_bootstrapper_strips_business_and_strategic` — filter_by_tier("bootstrapper", brief_content) returns content with market_analysis=None and competitive_strategy=None
8. `test_tier_filter_partner_keeps_business_strips_strategic` — filter_by_tier("partner", brief_content) keeps market_analysis but strips competitive_strategy
9. `test_tier_filter_cto_keeps_all` — filter_by_tier("cto_scale", brief_content) keeps everything

Run tests: MUST fail (RED).

**GREEN: Implement generator and prompts:**

**backend/app/artifacts/prompts.py** — System prompts for each artifact type:

Define 5 system prompts (BRIEF_SYSTEM_PROMPT, MVP_SCOPE_SYSTEM_PROMPT, etc.) that:
- Use "we" co-founder voice throughout per locked decision ("We identified...", "Our MVP should...")
- Include instructions for cross-referencing prior artifacts per locked decision
- Instruct Claude to use specific section IDs from the Pydantic schemas (research pitfall 1)
- Include tier-awareness instructions: "Include all sections. Tier filtering happens after generation."
- Feel like a "$500/hr strategy consultant" per CONTEXT.md specifics

Each prompt should include:
- Role: "You are a technical co-founder creating a {document_type} for a startup."
- Voice: "Use 'we' throughout — you and the founder are building this together."
- Context template: "Here is what we know about the startup: {onboarding_data_placeholder}"
- Prior artifact reference: "Here are the documents we've already created: {prior_artifacts_placeholder}"
- Quality bar: "This should read like a strategy deliverable from a senior consultant, not generic AI output."

**backend/app/artifacts/generator.py** — ArtifactGenerator class:

```python
class ArtifactGenerator:
    """Generates structured artifacts using Claude structured outputs.

    Takes a Runner dependency for testability (RunnerFake in tests, real Anthropic in production).
    For MVP: delegates to runner.generate_artifacts() and restructures output.
    Production path: direct Anthropic structured output calls.
    """

    def __init__(self, runner: Runner):
        self.runner = runner

    async def generate_artifact(
        self,
        artifact_type: ArtifactType,
        onboarding_data: dict,
        prior_artifacts: dict[str, dict] | None = None,
    ) -> dict:
        """Generate a single artifact with optional context from prior artifacts."""
        # Build context including prior artifacts
        # Call runner with assembled context
        # Return structured content dict

    async def generate_cascade(
        self,
        onboarding_data: dict,
        existing_artifacts: dict[str, dict] | None = None,
    ) -> tuple[dict[str, dict], list[str]]:
        """Generate all artifacts in cascade order.

        Args:
            onboarding_data: From onboarding session (thesis_snapshot, answers)
            existing_artifacts: Already-generated artifacts to skip (for retry)

        Returns:
            Tuple of (completed_artifacts dict, failed_artifact_types list)

        Per locked decision:
        - Linear chain: Brief -> MVP Scope -> Milestones -> Risk Log -> How It Works
        - Each artifact gets all prior artifacts as context
        - On failure: keep completed, add to failed list, continue only if downstream doesn't depend on failed
        - Skip artifacts that already exist in existing_artifacts (for retry)
        """

    @staticmethod
    def filter_by_tier(tier: str, artifact_type: ArtifactType, content: dict) -> dict:
        """Filter artifact content by subscription tier.

        Core fields always present. Business fields for partner+. Strategic for cto_scale only.
        Per locked decision: same base structure, higher tiers get more sections.
        """
```

The generate_cascade method implements the retry strategy from research open question 4:
- If Brief fails: all downstream fail (can't generate without Brief)
- If MVP Scope fails: skip Milestones, Risk Log, How It Works (they depend on MVP)
- If Milestones fails: attempt Risk Log (can use Brief + MVP), attempt How It Works
- Track which failed for UI retry button

For MVP: ArtifactGenerator delegates to `runner.generate_artifacts(brief)` and restructures the flat dict output into per-type structured content. This allows RunnerFake to work in tests. Production will use direct Anthropic calls with structured outputs.

Run tests: ALL 9 tests MUST pass (GREEN).

Commit: `test(06-02): add artifact generator tests` then `feat(06-02): implement ArtifactGenerator with cascade and tier filtering`
  </action>
  <verify>
    cd /Users/vladcortex/co-founder && python -m pytest backend/tests/domain/test_artifact_generator.py -v
    All 9 tests pass.
    python -c "from app.artifacts.generator import ArtifactGenerator; from app.artifacts.prompts import BRIEF_SYSTEM_PROMPT; print('OK')"
  </verify>
  <done>
    ArtifactGenerator produces structured content for all 5 types. Cascade follows linear chain order. Partial failure preserves completed artifacts. Tier filtering strips business/strategic fields appropriately. All 9 generator tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: ArtifactService with versioning and persistence (RED then GREEN)</name>
  <files>
    backend/app/services/artifact_service.py
    backend/tests/api/test_artifact_service.py
  </files>
  <action>
**RED: Write tests first in backend/tests/api/test_artifact_service.py:**

Integration tests using RunnerFake and test database (follow patterns from test_onboarding_api.py):

1. `test_generate_artifacts_creates_five_records` — ArtifactService.generate_all(project_id, onboarding_data, tier) creates 5 Artifact rows
2. `test_generate_artifacts_sets_version_1` — All created artifacts have version_number=1
3. `test_generate_artifacts_sets_generation_status_idle_after_completion` — generation_status is "idle" after successful generation
4. `test_get_artifact_by_id_returns_content` — get_artifact(artifact_id, user_id) returns ArtifactResponse with current_content populated
5. `test_get_artifact_user_isolation` — get_artifact with wrong user_id returns None (404 pattern)
6. `test_get_project_artifacts_returns_all_types` — get_project_artifacts(project_id, user_id) returns list of 5 artifacts
7. `test_regenerate_artifact_bumps_version` — After regenerate, version_number=2, previous_content has v1 content
8. `test_regenerate_artifact_preserves_previous_content` — previous_content matches what was current_content before regeneration
9. `test_regenerate_clears_user_edits` — After regenerate, has_user_edits=False, edited_sections=None
10. `test_edit_artifact_section` — edit_section(artifact_id, section_id, new_value) sets has_user_edits=True and updates content
11. `test_add_annotation` — add_annotation(artifact_id, section_id, note) appends to annotations JSONB array
12. `test_check_edits_before_regenerate` — check_has_edits(artifact_id) returns edited section names for the UI warning

Run tests: MUST fail (RED).

**GREEN: Implement ArtifactService:**

**backend/app/services/artifact_service.py:**

```python
class ArtifactService:
    """Orchestrates artifact generation, versioning, editing, and retrieval.

    Follows existing patterns from OnboardingService:
    - Constructor dependency injection (generator, session_factory)
    - User isolation via clerk_user_id filtering on project
    - JSONB state management with flag_modified
    """

    def __init__(self, generator: ArtifactGenerator, session_factory):
        self.generator = generator
        self.session_factory = session_factory

    async def generate_all(
        self,
        project_id: UUID,
        user_id: str,
        onboarding_data: dict,
        tier: str,
    ) -> tuple[list[UUID], list[str]]:
        """Generate all artifacts for a project via cascade.

        Returns: (artifact_ids, failed_types)

        Steps:
        1. Verify project belongs to user (404 pattern)
        2. Check if artifacts already exist (for retry: pass existing to generator)
        3. Set generation_status="generating" on all artifacts
        4. Call generator.generate_cascade()
        5. For each completed artifact: upsert with ON CONFLICT (project_id, artifact_type)
        6. Filter content by tier before persisting (locked decision)
        7. Set generation_status="idle" on completed, "failed" on failed
        8. Return artifact IDs and failed types
        """

    async def get_artifact(self, artifact_id: UUID, user_id: str) -> Artifact | None:
        """Get artifact by ID with user isolation via project ownership."""

    async def get_project_artifacts(self, project_id: UUID, user_id: str) -> list[Artifact]:
        """Get all artifacts for a project. Returns empty list if project not found or unauthorized."""

    async def regenerate_artifact(
        self,
        artifact_id: UUID,
        user_id: str,
        onboarding_data: dict,
        tier: str,
        force: bool = False,
    ) -> tuple[Artifact, bool]:
        """Regenerate a single artifact.

        Per locked decisions:
        - Move current_content -> previous_content
        - Increment version_number
        - Clear has_user_edits and edited_sections
        - Use row-level lock (SELECT FOR UPDATE) per research pitfall 6
        - If force=False and has_user_edits, return (artifact, True) to signal UI warning needed

        Returns: (updated_artifact, had_edits)
        """

    async def edit_section(
        self,
        artifact_id: UUID,
        user_id: str,
        section_path: str,
        new_value: str | dict,
    ) -> Artifact:
        """Edit a section of artifact content inline.

        Per locked decision: founders can inline-edit artifact content.
        Updates current_content JSONB at section_path.
        Sets has_user_edits=True, adds section_path to edited_sections.
        Uses flag_modified for SQLAlchemy JSONB tracking.
        """

    async def add_annotation(
        self,
        artifact_id: UUID,
        user_id: str,
        section_id: str,
        note: str,
    ) -> Artifact:
        """Add annotation to artifact (separate from content per research).

        Appends to annotations JSONB array.
        """

    async def check_has_edits(self, artifact_id: UUID, user_id: str) -> list[str] | None:
        """Check if artifact has user edits (for regeneration warning).

        Returns list of edited section names, or None if no edits.
        Per locked decision: warn before overwriting edits.
        """
```

Key patterns:
- User isolation: join Artifact with Project, filter by clerk_user_id (follow existing 404 pattern from onboarding)
- JSONB mutations: use `flag_modified(artifact, 'current_content')` after dict updates
- Version rotation: `previous_content = current_content; current_content = new; version_number += 1`
- Generation status: set "generating" before LLM call, "idle" after success, "failed" after error

Run tests: ALL 12 tests MUST pass (GREEN).

Commit: `test(06-02): add artifact service integration tests` then `feat(06-02): implement ArtifactService with versioning and cascade orchestration`
  </action>
  <verify>
    cd /Users/vladcortex/co-founder && python -m pytest backend/tests/api/test_artifact_service.py -v
    All 12 tests pass.
    python -c "from app.services.artifact_service import ArtifactService; print('OK')"
  </verify>
  <done>
    ArtifactService generates all 5 artifacts via cascade. Versioning rotates current -> previous with version_number increment. Tier filtering applied before persistence. User isolation enforced. Inline editing and annotations work. Edit detection for regeneration warning functional. All 12 service tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest backend/tests/domain/test_artifact_generator.py backend/tests/api/test_artifact_service.py -v` — all 21 tests pass
2. `python -c "from app.artifacts.generator import ArtifactGenerator; print('OK')"` — generator importable
3. `python -c "from app.services.artifact_service import ArtifactService; print('OK')"` — service importable
4. `python -c "from app.artifacts.prompts import BRIEF_SYSTEM_PROMPT, MVP_SCOPE_SYSTEM_PROMPT, MILESTONES_SYSTEM_PROMPT, RISK_LOG_SYSTEM_PROMPT, HOW_IT_WORKS_SYSTEM_PROMPT; print('All 5 prompts loaded')"` — all prompts defined
</verification>

<success_criteria>
- ArtifactGenerator generates structured content for all 5 artifact types
- Cascade follows exact order: Brief -> MVP Scope -> Milestones -> Risk Log -> How It Works
- Each downstream artifact receives all upstream artifacts as context
- Partial failure preserves completed artifacts, tracks failed ones
- Tier filtering correctly strips business/strategic fields
- ArtifactService handles versioning (current -> previous rotation)
- Regeneration with edit detection returns warning signal
- User isolation enforced via project ownership
- Generation status prevents concurrent writes
- All 21 tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-artifact-generation-pipeline/06-02-SUMMARY.md`
</output>
