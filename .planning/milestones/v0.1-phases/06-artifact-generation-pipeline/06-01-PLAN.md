---
phase: 06-artifact-generation-pipeline
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - backend/app/db/models/artifact.py
  - backend/app/db/models/__init__.py
  - backend/app/schemas/artifacts.py
  - backend/app/agent/runner_fake.py
  - backend/tests/domain/test_artifact_models.py
  - backend/alembic/versions/xxxx_add_artifacts_table.py
autonomous: true

must_haves:
  truths:
    - "Artifact model stores JSONB content with current_content and previous_content columns"
    - "Five artifact types defined as enum: brief, mvp_scope, milestones, risk_log, how_it_works"
    - "Versioning tracks version_number, has_user_edits, edited_sections, annotations"
    - "Generation status prevents concurrent writes (idle, generating, failed)"
    - "Pydantic schemas define section structure per artifact type with tier-gated sections"
    - "RunnerFake.generate_artifacts returns structured content matching Pydantic schemas"
  artifacts:
    - path: "backend/app/db/models/artifact.py"
      provides: "Artifact SQLAlchemy model with JSONB versioning"
      contains: "class Artifact"
    - path: "backend/app/schemas/artifacts.py"
      provides: "Pydantic schemas for all 5 artifact types with tier-gated sections"
      exports: ["ArtifactType", "ProductBriefContent", "MvpScopeContent", "MilestonesContent", "RiskLogContent", "HowItWorksContent", "ArtifactResponse", "ArtifactAnnotation"]
    - path: "backend/tests/domain/test_artifact_models.py"
      provides: "Domain tests for artifact model and schema validation"
      min_lines: 50
  key_links:
    - from: "backend/app/schemas/artifacts.py"
      to: "backend/app/db/models/artifact.py"
      via: "ArtifactType enum shared between schema and model"
      pattern: "ArtifactType"
---

<objective>
Artifact data model, Pydantic schemas with tier-gated sections, and RunnerFake extension for artifact generation testing.

Purpose: Establishes the foundation data layer for the entire artifact pipeline. Every subsequent plan depends on this model and these schemas.
Output: Artifact SQLAlchemy model, 5 Pydantic content schemas, Alembic migration, RunnerFake extension, domain tests.
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-artifact-generation-pipeline/06-CONTEXT.md
@.planning/phases/06-artifact-generation-pipeline/06-RESEARCH.md
@backend/app/db/models/__init__.py
@backend/app/db/models/project.py
@backend/app/db/models/onboarding_session.py
@backend/app/schemas/onboarding.py
@backend/app/agent/runner_fake.py
@backend/app/agent/runner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Artifact model, Pydantic schemas, and Alembic migration (RED then GREEN)</name>
  <files>
    backend/app/db/models/artifact.py
    backend/app/db/models/__init__.py
    backend/app/schemas/artifacts.py
    backend/tests/domain/test_artifact_models.py
    backend/alembic/versions/xxxx_add_artifacts_table.py
  </files>
  <action>
**RED: Write tests first in backend/tests/domain/test_artifact_models.py:**

Tests to write:
1. `test_artifact_type_enum_has_five_values` — ArtifactType enum has brief, mvp_scope, milestones, risk_log, how_it_works
2. `test_product_brief_content_core_fields_required` — ProductBriefContent requires problem_statement, target_user, value_proposition, key_constraint
3. `test_product_brief_content_business_fields_optional` — market_analysis is None by default (Partner+ tier)
4. `test_product_brief_content_strategic_fields_optional` — competitive_strategy is None by default (CTO tier)
5. `test_mvp_scope_content_validates` — MvpScopeContent requires core_features, out_of_scope, success_metrics
6. `test_milestones_content_validates` — MilestonesContent requires milestones list with title/description/success_criteria/estimated_weeks
7. `test_risk_log_content_validates` — RiskLogContent requires technical_risks, market_risks, execution_risks
8. `test_how_it_works_content_validates` — HowItWorksContent requires user_journey, architecture, data_flow
9. `test_artifact_response_schema` — ArtifactResponse includes id, project_id, artifact_type, version_number, current_content, has_user_edits, generation_status
10. `test_annotation_schema` — ArtifactAnnotation has section_id, note, created_at

Run tests: they MUST fail (RED).

**GREEN: Create the model and schemas:**

**backend/app/schemas/artifacts.py** — Pydantic v2 schemas:

```python
from enum import StrEnum

class ArtifactType(StrEnum):
    BRIEF = "brief"
    MVP_SCOPE = "mvp_scope"
    MILESTONES = "milestones"
    RISK_LOG = "risk_log"
    HOW_IT_WORKS = "how_it_works"

# Generation order (locked decision: linear chain)
GENERATION_ORDER = [
    ArtifactType.BRIEF,
    ArtifactType.MVP_SCOPE,
    ArtifactType.MILESTONES,
    ArtifactType.RISK_LOG,
    ArtifactType.HOW_IT_WORKS,
]
```

Define 5 content schemas with tier-gated sections per research recommendations:

- **ProductBriefContent**: Core: problem_statement, target_user, value_proposition, key_constraint, differentiation_points (list[str]). Business (Partner+): market_analysis (str|None). Strategic (CTO): competitive_strategy (str|None).
- **MvpScopeContent**: Core: core_features (list[FeatureItem]), out_of_scope (list[str]), success_metrics (list[str]). Business: technical_architecture (str|None). Strategic: scalability_plan (str|None). FeatureItem has name, description, priority.
- **MilestonesContent**: Core: milestones (list[Milestone]), critical_path (list[str]), total_duration_weeks (int). Business: resource_plan (str|None). Strategic: risk_mitigation_timeline (str|None). Milestone has title, description, success_criteria (list[str]), estimated_weeks (int, ge=1).
- **RiskLogContent**: Core: technical_risks, market_risks, execution_risks (each list[RiskItem]). Business: financial_risks (list[RiskItem]|None). Strategic: strategic_risks (list[RiskItem]|None). RiskItem has title, description, severity (high/medium/low), mitigation.
- **HowItWorksContent**: Core: user_journey (list[JourneyStep]), architecture (str), data_flow (str). Business: integration_points (str|None). Strategic: security_compliance (str|None). JourneyStep has step_number, title, description.

Also define:
- **ArtifactAnnotation**: section_id (str), note (str), created_at (datetime)
- **ArtifactResponse**: id (UUID), project_id (UUID), artifact_type (ArtifactType), version_number (int), current_content (dict), previous_content (dict|None), has_user_edits (bool), edited_sections (list[str]|None), annotations (list[ArtifactAnnotation]|None), generation_status (str), schema_version (int), created_at, updated_at
- **GenerateArtifactsRequest**: project_id (UUID), force (bool = False — if True, regenerate even with edits)
- **RegenerateArtifactRequest**: force (bool = False)

All content schemas must include `_schema_version: int = 1` field for future migration safety (research pitfall 3).

Use "we" voice in all schema field descriptions per locked decision.

**backend/app/db/models/artifact.py** — SQLAlchemy model:

```python
class Artifact(Base):
    __tablename__ = "artifacts"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    project_id = Column(UUID(as_uuid=True), ForeignKey("projects.id"), nullable=False, index=True)
    artifact_type = Column(String(50), nullable=False)  # ArtifactType enum value

    # Version tracking
    current_content = Column(JSONB, nullable=True)  # None while generating
    previous_content = Column(JSONB, nullable=True)  # None for v1
    version_number = Column(Integer, nullable=False, default=1)
    schema_version = Column(Integer, nullable=False, default=1)

    # Edit tracking
    has_user_edits = Column(Boolean, nullable=False, default=False)
    edited_sections = Column(JSONB, nullable=True)  # ["section_id1", ...]

    # Annotations (separate from content per research recommendation)
    annotations = Column(JSONB, nullable=True, default=list)  # [{section_id, note, created_at}]

    # Generation status (prevents concurrent writes per research pitfall 6)
    generation_status = Column(String(20), nullable=False, default="idle")  # idle, generating, failed

    # Timestamps
    created_at = Column(DateTime(timezone=True), ...)
    updated_at = Column(DateTime(timezone=True), ...)

    # Unique constraint: one artifact per type per project
    __table_args__ = (UniqueConstraint('project_id', 'artifact_type', name='uq_project_artifact_type'),)
```

Add `Artifact` import to `backend/app/db/models/__init__.py`.

**Alembic migration**: Generate with `alembic revision --autogenerate -m "add artifacts table"`. If autogenerate misses the table (known issue from Phase 4), manually write the migration with all columns, index on project_id, and unique constraint.

Run tests again: they MUST pass (GREEN).

Commit: `test(06-01): add failing artifact model and schema tests` then `feat(06-01): implement Artifact model, Pydantic schemas, and migration`
  </action>
  <verify>
    cd /Users/vladcortex/co-founder && python -m pytest backend/tests/domain/test_artifact_models.py -v
    All 10 tests pass.
    python -c "from app.db.models.artifact import Artifact; from app.schemas.artifacts import ArtifactType, ProductBriefContent, MvpScopeContent, MilestonesContent, RiskLogContent, HowItWorksContent, ArtifactResponse, ArtifactAnnotation, GENERATION_ORDER; print('All imports OK')"
  </verify>
  <done>
    Artifact model importable with all columns. Five Pydantic content schemas validate correctly with tier-gated optional fields. ArtifactType enum has 5 values. GENERATION_ORDER defines the linear chain. Alembic migration exists. All 10 domain tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend RunnerFake with structured artifact generation (RED then GREEN)</name>
  <files>
    backend/app/agent/runner_fake.py
    backend/tests/domain/test_artifact_models.py
  </files>
  <action>
**RED: Add tests to backend/tests/domain/test_artifact_models.py:**

11. `test_runner_fake_generate_artifacts_returns_all_five_types` — RunnerFake.generate_artifacts returns dict with keys matching all 5 ArtifactType values
12. `test_runner_fake_generate_artifacts_brief_matches_schema` — ProductBriefContent.model_validate(result["brief"]) succeeds
13. `test_runner_fake_generate_artifacts_mvp_matches_schema` — MvpScopeContent.model_validate(result["mvp_scope"]) succeeds
14. `test_runner_fake_generate_artifacts_milestones_matches_schema` — MilestonesContent.model_validate(result["milestones"]) succeeds
15. `test_runner_fake_generate_artifacts_risks_matches_schema` — RiskLogContent.model_validate(result["risk_log"]) succeeds
16. `test_runner_fake_generate_artifacts_how_it_works_matches_schema` — HowItWorksContent.model_validate(result["how_it_works"]) succeeds
17. `test_runner_fake_generate_artifacts_llm_failure` — Raises RuntimeError for llm_failure scenario

Run tests: new tests MUST fail (RED) while Task 1 tests still pass.

**GREEN: Update RunnerFake.generate_artifacts():**

Replace the existing plain-text `generate_artifacts` method in `backend/app/agent/runner_fake.py` with structured data matching the new Pydantic schemas. Each artifact must:

- Include `_schema_version: 1`
- Use "we" language throughout (per locked decision)
- Include all core fields populated with realistic inventory tracker data
- Include business and strategic tier fields populated (RunnerFake returns all fields; tier filtering happens in service layer)
- ProductBriefContent: problem_statement about inventory tracking, target_user, value_proposition, key_constraint, differentiation_points (3 items), market_analysis, competitive_strategy
- MvpScopeContent: 4-5 core_features as FeatureItem dicts, 4 out_of_scope items, 3 success_metrics, technical_architecture, scalability_plan
- MilestonesContent: 4 milestones with week estimates summing to ~4 weeks, critical_path of 3 items, total_duration_weeks=4, resource_plan, risk_mitigation_timeline
- RiskLogContent: 2 technical_risks, 2 market_risks, 2 execution_risks, 1 financial_risk, 1 strategic_risk — each with title, description, severity, mitigation
- HowItWorksContent: 5 user_journey steps, architecture string, data_flow string, integration_points, security_compliance

Cross-references between artifacts per locked decision:
- MVP Scope references Brief's value_proposition by name
- Milestones reference MVP Scope features by name
- Risk Log references specific Milestones
- How It Works references MVP Scope features and Milestones

Run tests: ALL 17 tests MUST pass (GREEN).

Commit: `test(06-01): add RunnerFake artifact schema compliance tests` then `feat(06-01): extend RunnerFake with structured artifact generation`
  </action>
  <verify>
    cd /Users/vladcortex/co-founder && python -m pytest backend/tests/domain/test_artifact_models.py -v
    All 17 tests pass.
    python -c "from app.agent.runner_fake import RunnerFake; import asyncio; r = RunnerFake(); arts = asyncio.run(r.generate_artifacts({'problem': 'test'})); print(list(arts.keys())); assert len(arts) == 5"
  </verify>
  <done>
    RunnerFake.generate_artifacts returns structured dicts for all 5 artifact types. Each dict validates against its Pydantic schema. All 17 domain tests pass. Cross-references exist between artifacts.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest backend/tests/domain/test_artifact_models.py -v` — all 17 tests pass
2. `python -c "from app.db.models.artifact import Artifact; print('OK')"` — model importable
3. `python -c "from app.schemas.artifacts import ArtifactType, GENERATION_ORDER; print(len(GENERATION_ORDER))"` — prints 5
4. `python -c "from app.db.models import Artifact; print('OK')"` — registered in __init__.py
5. Alembic migration file exists in backend/alembic/versions/
</verification>

<success_criteria>
- Artifact SQLAlchemy model with JSONB current_content/previous_content columns
- Five ArtifactType enum values with GENERATION_ORDER
- Five Pydantic content schemas with tier-gated optional sections
- ArtifactResponse, ArtifactAnnotation, GenerateArtifactsRequest schemas
- generation_status column prevents concurrent writes
- schema_version field for future migration safety
- UniqueConstraint on (project_id, artifact_type)
- RunnerFake returns structured artifacts matching all 5 schemas
- All 17 tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-artifact-generation-pipeline/06-01-SUMMARY.md`
</output>
