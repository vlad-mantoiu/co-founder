---
phase: 04-onboarding-idea-capture
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - backend/app/db/models/onboarding_session.py
  - backend/app/db/models/__init__.py
  - backend/app/schemas/onboarding.py
  - backend/app/agent/runner.py
  - backend/app/agent/runner_fake.py
  - backend/tests/domain/test_onboarding_models.py
  - backend/alembic/versions/xxx_add_onboarding_sessions_table.py
autonomous: true

must_haves:
  truths:
    - "OnboardingSession model stores idea, questions, answers as JSONB with clerk_user_id isolation"
    - "Pydantic schemas enforce question structure (id, text, input_type, required, options)"
    - "ThesisSnapshot schema has tier-dependent sections (core, business, strategic)"
    - "RunnerFake.generate_questions returns 6 questions with mixed input_type (text, textarea, multiple_choice)"
    - "RunnerFake.generate_brief returns full ThesisSnapshot with all 9 fields populated"
  artifacts:
    - path: "backend/app/db/models/onboarding_session.py"
      provides: "OnboardingSession SQLAlchemy model with JSONB columns"
      contains: "onboarding_sessions"
    - path: "backend/app/schemas/onboarding.py"
      provides: "OnboardingQuestion, QuestionSet, ThesisSnapshot Pydantic models"
      exports: ["OnboardingQuestion", "QuestionSet", "ThesisSnapshot", "StartOnboardingRequest", "AnswerRequest"]
    - path: "backend/tests/domain/test_onboarding_models.py"
      provides: "Model and schema validation tests"
      min_lines: 80
  key_links:
    - from: "backend/app/schemas/onboarding.py"
      to: "backend/app/agent/runner_fake.py"
      via: "RunnerFake returns data matching Pydantic schemas"
      pattern: "QuestionSet|ThesisSnapshot"
---

<objective>
Onboarding domain models, Pydantic schemas, and RunnerFake extensions for deterministic testing

Purpose: Establish the data layer and test doubles that all subsequent onboarding plans depend on. The OnboardingSession model stores JSONB state for infinite resumption. Pydantic schemas define the API contract. RunnerFake provides deterministic responses matching the new schemas.

Output: OnboardingSession table, Pydantic request/response schemas, extended RunnerFake, passing model tests
</objective>

<execution_context>
@/Users/vladcortex/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vladcortex/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-onboarding-idea-capture/04-RESEARCH.md
@backend/app/db/models/user_settings.py
@backend/app/db/models/project.py
@backend/app/db/models/__init__.py
@backend/app/db/base.py
@backend/app/agent/runner.py
@backend/app/agent/runner_fake.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: OnboardingSession model + Pydantic schemas + Alembic migration</name>
  <files>
    backend/app/db/models/onboarding_session.py
    backend/app/db/models/__init__.py
    backend/app/schemas/onboarding.py
    backend/alembic/versions/xxx_add_onboarding_sessions_table.py
  </files>
  <action>
    **OnboardingSession model** (`backend/app/db/models/onboarding_session.py`):
    Create SQLAlchemy model with these columns:
    - `id`: UUID primary key (default uuid4)
    - `clerk_user_id`: String(255), NOT NULL, indexed
    - `status`: String(20), NOT NULL, default "in_progress" — values: "in_progress", "completed", "abandoned"
    - `current_question_index`: Integer, NOT NULL, default 0
    - `total_questions`: Integer, NOT NULL
    - `idea_text`: Text, NOT NULL
    - `questions`: JSON (JSONB), NOT NULL — stores QuestionSet as dict
    - `answers`: JSON (JSONB), NOT NULL, default dict — stores {question_id: answer_text}
    - `thesis_snapshot`: JSON (JSONB), nullable — generated Thesis Snapshot
    - `thesis_edits`: JSON (JSONB), nullable — inline edits that override LLM output (canonical)
    - `project_id`: UUID, nullable, ForeignKey("projects.id") — linked project after creation
    - `created_at`: DateTime(timezone=True), NOT NULL, lambda default
    - `updated_at`: DateTime(timezone=True), NOT NULL, lambda default + onupdate
    - `completed_at`: DateTime(timezone=True), nullable

    Follow existing patterns from user_settings.py and project.py for datetime lambda defaults.

    **Update `__init__.py`**: Add OnboardingSession import to `backend/app/db/models/__init__.py`.

    **Pydantic schemas** (`backend/app/schemas/onboarding.py`):
    Create these Pydantic v2 BaseModel classes:

    1. `OnboardingQuestion`: id (str), text (str), input_type (Literal["text", "textarea", "multiple_choice"]), required (bool), options (list[str] | None = None), follow_up_hint (str | None = None)
    2. `QuestionSet`: questions (list[OnboardingQuestion], min_length=5, max_length=7), total_count (int)
    3. `ThesisSnapshot`:
       - Core (always present): problem (str), target_user (str), value_prop (str), key_constraint (str)
       - Business (Partner+): differentiation (str | None = None), monetization_hypothesis (str | None = None)
       - Strategic (CTO): assumptions (list[str] | None = None), risks (list[str] | None = None), smallest_viable_experiment (str | None = None)
    4. `StartOnboardingRequest`: idea (str, min_length=1) — with validator that strips whitespace and rejects empty
    5. `AnswerRequest`: question_id (str), answer (str)
    6. `OnboardingSessionResponse`: id (str), status (str), current_question_index (int), total_questions (int), idea_text (str), questions (list[OnboardingQuestion]), answers (dict[str, str]), thesis_snapshot (ThesisSnapshot | None), progress_percent (int) — computed property: int(current_question_index / total_questions * 100)
    7. `ThesisSnapshotEditRequest`: field_name (str), new_value (str) — for inline editing

    **Alembic migration**: Generate migration to create `onboarding_sessions` table. Use `alembic revision --autogenerate -m "add onboarding_sessions table"`. Review the generated migration to ensure UUID, JSONB types, and index on clerk_user_id are correct. The migration should also add a partial unique index: `CREATE UNIQUE INDEX IF NOT EXISTS idx_unique_active_session_per_user ON onboarding_sessions(clerk_user_id) WHERE status = 'in_progress'` — but ONLY for bootstrapper tier enforcement. Actually, skip the partial unique index since tier limits vary and will be enforced in application logic. Just ensure there's an index on (clerk_user_id, status) for efficient session queries.
  </action>
  <verify>
    - `python -c "from app.db.models.onboarding_session import OnboardingSession; print(OnboardingSession.__tablename__)"` prints "onboarding_sessions"
    - `python -c "from app.schemas.onboarding import QuestionSet, ThesisSnapshot, StartOnboardingRequest; print('OK')"` prints OK
    - Alembic migration file exists in backend/alembic/versions/
    - `python -c "from app.schemas.onboarding import StartOnboardingRequest; StartOnboardingRequest(idea='  ')"` raises ValidationError (whitespace-only rejected)
  </verify>
  <done>
    OnboardingSession model importable with all columns. Pydantic schemas validate correctly (QuestionSet enforces 5-7 questions, StartOnboardingRequest rejects empty/whitespace ideas). Alembic migration created. Models re-exported from __init__.py.
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend RunnerFake with onboarding scenarios + write model tests</name>
  <files>
    backend/app/agent/runner_fake.py
    backend/tests/domain/test_onboarding_models.py
  </files>
  <action>
    **Extend RunnerFake** (`backend/app/agent/runner_fake.py`):

    Update `generate_questions` to return data that matches the new `OnboardingQuestion` schema:
    - Return 6 questions (within 5-7 range)
    - Include mixed input_type: at least one "text", one "textarea", one "multiple_choice"
    - Use "we" language in question text (per user decision)
    - Include options list for multiple_choice questions
    - Include follow_up_hint for at least one question
    - Return as list[dict] matching OnboardingQuestion schema fields

    Update `generate_brief` to return data matching `ThesisSnapshot` schema:
    - Return all 9 fields (problem, target_user, value_prop, key_constraint, differentiation, monetization_hypothesis, assumptions as list, risks as list, smallest_viable_experiment)
    - Use the same inventory tracker scenario for consistency
    - Ensure `assumptions` and `risks` are lists of strings (not single strings)

    Do NOT change the Runner protocol signatures — generate_questions still returns list[dict] and generate_brief still returns dict. The Pydantic validation happens at the API layer.

    **Write tests** (`backend/tests/domain/test_onboarding_models.py`):

    RED then GREEN for each:

    1. `test_question_set_validates_min_max` — QuestionSet rejects < 5 questions and > 7 questions
    2. `test_question_set_accepts_valid_range` — QuestionSet accepts 5, 6, 7 questions
    3. `test_thesis_snapshot_core_fields_required` — ThesisSnapshot requires problem, target_user, value_prop, key_constraint
    4. `test_thesis_snapshot_optional_fields` — ThesisSnapshot allows None for business/strategic fields
    5. `test_start_onboarding_rejects_empty` — StartOnboardingRequest rejects "", "   ", None
    6. `test_start_onboarding_accepts_valid` — StartOnboardingRequest accepts "Food delivery app"
    7. `test_runner_fake_questions_match_schema` — RunnerFake.generate_questions output validates as list of OnboardingQuestion
    8. `test_runner_fake_brief_matches_schema` — RunnerFake.generate_brief output validates as ThesisSnapshot
    9. `test_onboarding_session_response_progress` — OnboardingSessionResponse computes progress_percent correctly (3/6 = 50, 0/5 = 0, 5/5 = 100)

    Use pytest with standard `async` markers for RunnerFake tests. Import directly from schemas and models.
  </action>
  <verify>
    - `cd backend && python -m pytest tests/domain/test_onboarding_models.py -v` — all 9 tests pass
    - `python -c "from app.agent.runner_fake import RunnerFake; import asyncio; r = RunnerFake(); qs = asyncio.run(r.generate_questions({})); print(len(qs), [q['input_type'] for q in qs])"` shows 6 questions with mixed input types
  </verify>
  <done>
    RunnerFake returns onboarding data matching new Pydantic schemas. 9 tests cover schema validation (min/max questions, required fields, empty rejection, progress computation) and RunnerFake compliance. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. OnboardingSession model importable and has correct tablename
2. All Pydantic schemas importable: QuestionSet, ThesisSnapshot, StartOnboardingRequest, AnswerRequest, OnboardingSessionResponse
3. RunnerFake.generate_questions returns 6 questions with mixed input_type
4. RunnerFake.generate_brief returns all 9 ThesisSnapshot fields including list types
5. All 9 domain tests pass
6. Alembic migration file exists
7. StartOnboardingRequest rejects empty/whitespace ideas (PROJ-03 validation)
</verification>

<success_criteria>
- OnboardingSession JSONB model stores idea, questions, answers, thesis_snapshot, thesis_edits
- Pydantic schemas enforce API contracts (5-7 questions, required fields, input types)
- RunnerFake provides deterministic data matching schemas
- 9+ tests pass covering model validation and RunnerFake compliance
- Alembic migration ready to run
</success_criteria>

<output>
After completion, create `.planning/phases/04-onboarding-idea-capture/04-01-SUMMARY.md`
</output>
